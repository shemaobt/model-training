{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5de8e895c109485b9713c91301c7e94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad90f4f503ac439d819cf0a80a7c5ebc",
              "IPY_MODEL_69c18ec873ed4a419b45e8703449cf04",
              "IPY_MODEL_8ca52e36eed04269bdeeee59f9c73059"
            ],
            "layout": "IPY_MODEL_1586b6f88c074046bf5ff317c2c91fcb"
          }
        },
        "ad90f4f503ac439d819cf0a80a7c5ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d14f32486bc14827b87eeda604916ce9",
            "placeholder": "​",
            "style": "IPY_MODEL_8a4bdb3e31b4406ebadc6b0e0930764d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "69c18ec873ed4a419b45e8703449cf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cacc2e1a77f14d0ea62c3d6f6b703ccc",
            "max": 1269737156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_976473d43def42988a0fafb41e9f438f",
            "value": 1269737156
          }
        },
        "8ca52e36eed04269bdeeee59f9c73059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aa4a120cea6458aa3dc40158adac393",
            "placeholder": "​",
            "style": "IPY_MODEL_c8d88cdb33fc4975acdb9a1d11b2b8c6",
            "value": " 1.27G/1.27G [00:03&lt;00:00, 387MB/s]"
          }
        },
        "1586b6f88c074046bf5ff317c2c91fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d14f32486bc14827b87eeda604916ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4bdb3e31b4406ebadc6b0e0930764d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cacc2e1a77f14d0ea62c3d6f6b703ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "976473d43def42988a0fafb41e9f438f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aa4a120cea6458aa3dc40158adac393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d88cdb33fc4975acdb9a1d11b2b8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6cfe941e264470fbb5ce0d99429991a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3595607027740ddb1b9374a9e6a4359",
              "IPY_MODEL_0a8ab65049a4459c9a832ba883580fde",
              "IPY_MODEL_50b57b23f2af4eba959c0641d4b09e47"
            ],
            "layout": "IPY_MODEL_dc6140892e574f8983ec1a864974d3e9"
          }
        },
        "e3595607027740ddb1b9374a9e6a4359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df93b241028e4facb305d8b66ca4ce85",
            "placeholder": "​",
            "style": "IPY_MODEL_156a0a39a8e949e58df2cca35d7a606f",
            "value": "model.safetensors: 100%"
          }
        },
        "0a8ab65049a4459c9a832ba883580fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea57092f5a14a4d8c7d43df089fc487",
            "max": 1269615400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36969647bf914fc0957e6beb0a65b922",
            "value": 1269615400
          }
        },
        "50b57b23f2af4eba959c0641d4b09e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7380e5c30283418caeac510b9f549aed",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7f0e15d4614c15ac95f70d11a9e3a1",
            "value": " 1.27G/1.27G [00:05&lt;00:00, 245MB/s]"
          }
        },
        "dc6140892e574f8983ec1a864974d3e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df93b241028e4facb305d8b66ca4ce85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "156a0a39a8e949e58df2cca35d7a606f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ea57092f5a14a4d8c7d43df089fc487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36969647bf914fc0957e6beb0a65b922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7380e5c30283418caeac510b9f549aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7f0e15d4614c15ac95f70d11a9e3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CucfiAyDdkw6",
        "outputId": "a1df2cb1-9028-4315-fe03-6972ba411ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Install dependencies and setup\n",
        "!pip install transformers torch torchaudio sentencepiece librosa soundfile -q\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Configuration\n",
        "# =============================================================\n",
        "# EDIT THIS PATH to point to your audio file in Google Drive\n",
        "# =============================================================\n",
        "AUDIO_PATH = \"/content/drive/MyDrive/satere_project/acts12.mp3\"\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/satere_project/phase2_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Model settings\n",
        "MODEL_NAME = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "SAMPLE_RATE = 16000  # XLSR-53 expects 16kHz\n",
        "\n",
        "# BPE settings\n",
        "VOCAB_SIZE = 500     # Number of motifs to discover\n",
        "MIN_FREQUENCY = 5    # Minimum times a pattern must appear\n",
        "\n",
        "print(f\"Audio file: {AUDIO_PATH}\")\n",
        "print(f\"Output dir: {OUTPUT_DIR}\")\n",
        "print(f\"File exists: {os.path.exists(AUDIO_PATH)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IGWBpFSge5c",
        "outputId": "25ba50dc-e7dd-4257-99f5-2e081c5d3ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio file: /content/drive/MyDrive/satere_project/acts12.mp3\n",
            "Output dir: /content/drive/MyDrive/satere_project/phase2_output\n",
            "File exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Load and preprocess audio\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "print(\"Loading audio file...\")\n",
        "\n",
        "# Load audio and resample to 16kHz\n",
        "audio, sr = librosa.load(AUDIO_PATH, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "duration_seconds = len(audio) / SAMPLE_RATE\n",
        "duration_minutes = duration_seconds / 60\n",
        "\n",
        "print(f\"Audio loaded successfully!\")\n",
        "print(f\"  Duration: {duration_minutes:.1f} minutes ({duration_seconds:.0f} seconds)\")\n",
        "print(f\"  Sample rate: {sr} Hz\")\n",
        "print(f\"  Samples: {len(audio):,}\")\n",
        "\n",
        "# Save preprocessed audio\n",
        "preprocessed_path = os.path.join(OUTPUT_DIR, \"audio_16khz.wav\")\n",
        "sf.write(preprocessed_path, audio, SAMPLE_RATE)\n",
        "print(f\"  Saved preprocessed audio to: {preprocessed_path}\")\n",
        "```\n",
        "\n",
        "Press **Shift+Enter**.\n",
        "\n",
        "You should see something like:\n",
        "```\n",
        "Audio loaded successfully!\n",
        "  Duration: 9.3 minutes (557 seconds)\n",
        "  Sample rate: 16000 Hz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "R_j6dJIGg7PO",
        "outputId": "ed938012-349f-4dd9-b329-58c83a4175af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1005178797.py, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1005178797.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Load and preprocess audio\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "print(\"Loading audio file...\")\n",
        "\n",
        "audio, sr = librosa.load(AUDIO_PATH, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "duration_seconds = len(audio) / SAMPLE_RATE\n",
        "duration_minutes = duration_seconds / 60\n",
        "\n",
        "print(f\"Audio loaded successfully!\")\n",
        "print(f\"  Duration: {duration_minutes:.1f} minutes ({duration_seconds:.0f} seconds)\")\n",
        "print(f\"  Sample rate: {sr} Hz\")\n",
        "print(f\"  Samples: {len(audio):,}\")\n",
        "\n",
        "preprocessed_path = os.path.join(OUTPUT_DIR, \"audio_16khz.wav\")\n",
        "sf.write(preprocessed_path, audio, SAMPLE_RATE)\n",
        "print(f\"  Saved preprocessed audio to: {preprocessed_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a-ZYMTlhIiE",
        "outputId": "215cfef4-495b-478b-863b-00858d115ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio file...\n",
            "Audio loaded successfully!\n",
            "  Duration: 9.3 minutes (557 seconds)\n",
            "  Sample rate: 16000 Hz\n",
            "  Samples: 8,912,000\n",
            "  Saved preprocessed audio to: /content/drive/MyDrive/satere_project/phase2_output/audio_16khz.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Load XLSR-53 model\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "\n",
        "print(\"Loading XLSR-53 model (this may take a minute)...\")\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "model = Wav2Vec2Model.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "5de8e895c109485b9713c91301c7e94f",
            "ad90f4f503ac439d819cf0a80a7c5ebc",
            "69c18ec873ed4a419b45e8703449cf04",
            "8ca52e36eed04269bdeeee59f9c73059",
            "1586b6f88c074046bf5ff317c2c91fcb",
            "d14f32486bc14827b87eeda604916ce9",
            "8a4bdb3e31b4406ebadc6b0e0930764d",
            "cacc2e1a77f14d0ea62c3d6f6b703ccc",
            "976473d43def42988a0fafb41e9f438f",
            "2aa4a120cea6458aa3dc40158adac393",
            "c8d88cdb33fc4975acdb9a1d11b2b8c6",
            "e6cfe941e264470fbb5ce0d99429991a",
            "e3595607027740ddb1b9374a9e6a4359",
            "0a8ab65049a4459c9a832ba883580fde",
            "50b57b23f2af4eba959c0641d4b09e47",
            "dc6140892e574f8983ec1a864974d3e9",
            "df93b241028e4facb305d8b66ca4ce85",
            "156a0a39a8e949e58df2cca35d7a606f",
            "7ea57092f5a14a4d8c7d43df089fc487",
            "36969647bf914fc0957e6beb0a65b922",
            "7380e5c30283418caeac510b9f549aed",
            "1d7f0e15d4614c15ac95f70d11a9e3a1"
          ]
        },
        "id": "qixr8xwehw86",
        "outputId": "9c0e1056-a2cb-43da-ff1a-5f2e79368477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XLSR-53 model (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de8e895c109485b9713c91301c7e94f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6cfe941e264470fbb5ce0d99429991a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "  Model: facebook/wav2vec2-large-xlsr-53\n",
            "  Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Extract acoustic features\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features_chunked(audio, model, feature_extractor, chunk_duration=30, overlap=1):\n",
        "    chunk_samples = int(chunk_duration * SAMPLE_RATE)\n",
        "    overlap_samples = int(overlap * SAMPLE_RATE)\n",
        "    step = chunk_samples - overlap_samples\n",
        "\n",
        "    all_features = []\n",
        "    n_chunks = max(1, int(np.ceil((len(audio) - overlap_samples) / step)))\n",
        "\n",
        "    print(f\"Processing {n_chunks} chunks...\")\n",
        "\n",
        "    for i in tqdm(range(n_chunks)):\n",
        "        start = i * step\n",
        "        end = min(start + chunk_samples, len(audio))\n",
        "        chunk = audio[start:end]\n",
        "\n",
        "        inputs = feature_extractor(chunk, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_values)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        if i > 0 and overlap_samples > 0:\n",
        "            frames_to_skip = int(overlap_samples / SAMPLE_RATE * 50)\n",
        "            hidden_states = hidden_states[:, frames_to_skip:, :]\n",
        "\n",
        "        all_features.append(hidden_states.cpu().numpy())\n",
        "\n",
        "    features = np.concatenate(all_features, axis=1)\n",
        "    return features[0]\n",
        "\n",
        "print(\"Extracting acoustic features...\")\n",
        "features = extract_features_chunked(audio, model, feature_extractor)\n",
        "\n",
        "print(f\"\\nFeature extraction complete!\")\n",
        "print(f\"  Feature shape: {features.shape}\")\n",
        "print(f\"  Time frames: {features.shape[0]}\")\n",
        "print(f\"  Feature dimension: {features.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzu_rIamigBR",
        "outputId": "a23d8733-5c5b-469a-fae2-aca57fe3f6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting acoustic features...\n",
            "Processing 20 chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:08<00:00,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature extraction complete!\n",
            "  Feature shape: (27830, 1024)\n",
            "  Time frames: 27830\n",
            "  Feature dimension: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Quantize to discrete units using K-means\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "N_CLUSTERS = 100  # Number of discrete acoustic units\n",
        "\n",
        "print(f\"Clustering features into {N_CLUSTERS} discrete units...\")\n",
        "\n",
        "kmeans = MiniBatchKMeans(\n",
        "    n_clusters=N_CLUSTERS,\n",
        "    batch_size=1000,\n",
        "    random_state=42,\n",
        "    n_init=3\n",
        ")\n",
        "\n",
        "acoustic_units = kmeans.fit_predict(features)\n",
        "\n",
        "print(f\"\\nQuantization complete!\")\n",
        "print(f\"  Total frames: {len(acoustic_units):,}\")\n",
        "print(f\"  Unique units: {len(np.unique(acoustic_units))}\")\n",
        "\n",
        "# Save acoustic units\n",
        "units_path = os.path.join(OUTPUT_DIR, \"acoustic_units.json\")\n",
        "with open(units_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"units\": acoustic_units.tolist(),\n",
        "        \"n_clusters\": N_CLUSTERS,\n",
        "        \"total_frames\": len(acoustic_units)\n",
        "    }, f)\n",
        "\n",
        "# Convert to pseudo-text\n",
        "unit_string = \" \".join([f\"U{u:02d}\" for u in acoustic_units])\n",
        "print(f\"\\nPseudo-text sample: {unit_string[:200]}...\")\n",
        "\n",
        "pseudotext_path = os.path.join(OUTPUT_DIR, \"pseudotext.txt\")\n",
        "with open(pseudotext_path, 'w') as f:\n",
        "    f.write(unit_string)\n",
        "print(f\"Saved pseudo-text to: {pseudotext_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4QcgizUjEaN",
        "outputId": "ab086208-ac0b-440f-dfe8-58b6e8e9077d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering features into 100 discrete units...\n",
            "\n",
            "Quantization complete!\n",
            "  Total frames: 27,830\n",
            "  Unique units: 100\n",
            "\n",
            "Pseudo-text sample: U35 U12 U55 U55 U25 U65 U65 U25 U25 U55 U55 U55 U55 U55 U55 U55 U55 U55 U55 U55 U25 U65 U65 U55 U65 U81 U82 U82 U53 U53 U83 U79 U79 U79 U44 U44 U44 U51 U66 U66 U66 U31 U66 U63 U38 U38 U59 U59 U61 U61 ...\n",
            "Saved pseudo-text to: /content/drive/MyDrive/satere_project/phase2_output/pseudotext.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Discover motifs using SentencePiece BPE\n",
        "import sentencepiece as spm\n",
        "\n",
        "print(\"Training BPE model to discover acoustic motifs...\")\n",
        "\n",
        "# Split into chunks (BPE needs multiple \"sentences\")\n",
        "units_list = unit_string.split(\" \")\n",
        "chunk_size = 500\n",
        "chunks = [\" \".join(units_list[i:i+chunk_size]) for i in range(0, len(units_list), chunk_size)]\n",
        "\n",
        "train_path = os.path.join(OUTPUT_DIR, \"bpe_train.txt\")\n",
        "with open(train_path, 'w') as f:\n",
        "    for chunk in chunks:\n",
        "        f.write(chunk + \"\\n\")\n",
        "\n",
        "model_prefix = os.path.join(OUTPUT_DIR, \"motifs\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=train_path,\n",
        "    model_prefix=model_prefix,\n",
        "    vocab_size=116,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    max_sentence_length=10000,\n",
        "    num_threads=4\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{model_prefix}.model\")\n",
        "\n",
        "print(f\"BPE training complete!\")\n",
        "print(f\"  Vocabulary size: {sp.get_piece_size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCiVaNQPkBfz",
        "outputId": "6471333d-9ff9-43b6-c6f8-6caee803ffde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BPE model to discover acoustic motifs...\n",
            "BPE training complete!\n",
            "  Vocabulary size: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Analyze discovered motifs\n",
        "from collections import Counter\n",
        "\n",
        "encoded = sp.encode_as_pieces(unit_string)\n",
        "motif_counts = Counter(encoded)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DISCOVERED ACOUSTIC MOTIFS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get multi-unit motifs (the interesting ones)\n",
        "multi_unit_motifs = [(m, c) for m, c in motif_counts.most_common()\n",
        "                      if len(m.replace(\"▁\", \"\").split(\"U\")) > 2]\n",
        "\n",
        "print(f\"\\nMulti-unit motifs (potential morphemes): {len(multi_unit_motifs)}\")\n",
        "\n",
        "print(\"\\nTop 30 multi-unit motifs by frequency:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (motif, count) in enumerate(multi_unit_motifs[:30]):\n",
        "    n_units = len(motif.replace(\"▁\", \"\").split(\"U\")) - 1\n",
        "    duration_ms = n_units * 20\n",
        "    print(f\"  {i+1:2d}. {motif:30s} count={count:4d}  ~{duration_ms}ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3H5zUQkkX0G",
        "outputId": "56b20cef-4bf6-4380-c3fd-c0f6c14f7086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DISCOVERED ACOUSTIC MOTIFS\n",
            "============================================================\n",
            "\n",
            "Multi-unit motifs (potential morphemes): 0\n",
            "\n",
            "Top 30 multi-unit motifs by frequency:\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8b: Analyze all motifs (adjusted)\n",
        "from collections import Counter\n",
        "\n",
        "encoded = sp.encode_as_pieces(unit_string)\n",
        "motif_counts = Counter(encoded)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DISCOVERED ACOUSTIC MOTIFS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal unique motifs: {len(motif_counts)}\")\n",
        "print(f\"Total tokens in sequence: {len(encoded)}\")\n",
        "\n",
        "print(\"\\nTop 30 motifs by frequency:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (motif, count) in enumerate(motif_counts.most_common(30)):\n",
        "    # Count units in this motif\n",
        "    units_in_motif = motif.replace(\"▁\", \"\").strip().split(\"U\")\n",
        "    units_in_motif = [u for u in units_in_motif if u]  # Remove empty strings\n",
        "    n_units = len(units_in_motif)\n",
        "    duration_ms = n_units * 20\n",
        "    print(f\"  {i+1:2d}. {motif:25s} units={n_units:2d}  count={count:4d}  ~{duration_ms}ms\")\n",
        "\n",
        "# Store all motifs for later use\n",
        "all_motifs = [(m, c) for m, c in motif_counts.most_common()]\n",
        "print(f\"\\nTotal motifs to analyze: {len(all_motifs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmdgAk64knhd",
        "outputId": "52f977e2-93fd-4238-ba88-07a1b0746b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DISCOVERED ACOUSTIC MOTIFS\n",
            "============================================================\n",
            "\n",
            "Total unique motifs: 101\n",
            "Total tokens in sequence: 55660\n",
            "\n",
            "Top 30 motifs by frequency:\n",
            "--------------------------------------------------\n",
            "   1. ▁U                        units= 0  count=27830  ~0ms\n",
            "   2. 79                        units= 1  count= 764  ~20ms\n",
            "   3. 58                        units= 1  count= 701  ~20ms\n",
            "   4. 33                        units= 1  count= 669  ~20ms\n",
            "   5. 68                        units= 1  count= 567  ~20ms\n",
            "   6. 55                        units= 1  count= 527  ~20ms\n",
            "   7. 32                        units= 1  count= 491  ~20ms\n",
            "   8. 35                        units= 1  count= 475  ~20ms\n",
            "   9. 12                        units= 1  count= 463  ~20ms\n",
            "  10. 59                        units= 1  count= 463  ~20ms\n",
            "  11. 74                        units= 1  count= 463  ~20ms\n",
            "  12. 41                        units= 1  count= 459  ~20ms\n",
            "  13. 81                        units= 1  count= 450  ~20ms\n",
            "  14. 97                        units= 1  count= 449  ~20ms\n",
            "  15. 96                        units= 1  count= 438  ~20ms\n",
            "  16. 87                        units= 1  count= 433  ~20ms\n",
            "  17. 36                        units= 1  count= 431  ~20ms\n",
            "  18. 04                        units= 1  count= 411  ~20ms\n",
            "  19. 75                        units= 1  count= 408  ~20ms\n",
            "  20. 53                        units= 1  count= 403  ~20ms\n",
            "  21. 82                        units= 1  count= 394  ~20ms\n",
            "  22. 43                        units= 1  count= 392  ~20ms\n",
            "  23. 80                        units= 1  count= 392  ~20ms\n",
            "  24. 91                        units= 1  count= 388  ~20ms\n",
            "  25. 34                        units= 1  count= 387  ~20ms\n",
            "  26. 67                        units= 1  count= 375  ~20ms\n",
            "  27. 62                        units= 1  count= 374  ~20ms\n",
            "  28. 19                        units= 1  count= 368  ~20ms\n",
            "  29. 90                        units= 1  count= 366  ~20ms\n",
            "  30. 25                        units= 1  count= 364  ~20ms\n",
            "\n",
            "Total motifs to analyze: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8c: Find patterns using n-grams instead of BPE\n",
        "from collections import Counter\n",
        "\n",
        "# Work directly with the acoustic units sequence\n",
        "units_list = [f\"U{u:02d}\" for u in acoustic_units]\n",
        "\n",
        "# Find bigrams (2-unit patterns)\n",
        "bigrams = [f\"{units_list[i]}_{units_list[i+1]}\" for i in range(len(units_list)-1)]\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Find trigrams (3-unit patterns)\n",
        "trigrams = [f\"{units_list[i]}_{units_list[i+1]}_{units_list[i+2]}\" for i in range(len(units_list)-2)]\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "# Find 4-grams\n",
        "fourgrams = [f\"{units_list[i]}_{units_list[i+1]}_{units_list[i+2]}_{units_list[i+3]}\" for i in range(len(units_list)-3)]\n",
        "fourgram_counts = Counter(fourgrams)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DISCOVERED ACOUSTIC MOTIFS (N-gram Analysis)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nTop 20 Bigrams (2-unit patterns, ~40ms):\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(bigram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:20s} count={count:4d}\")\n",
        "\n",
        "print(\"\\nTop 20 Trigrams (3-unit patterns, ~60ms):\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(trigram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:20s} count={count:4d}\")\n",
        "\n",
        "print(\"\\nTop 20 Four-grams (4-unit patterns, ~80ms):\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(fourgram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:25s} count={count:4d}\")\n",
        "\n",
        "# Store for next step\n",
        "motif_data = {\n",
        "    'bigrams': bigram_counts.most_common(50),\n",
        "    'trigrams': trigram_counts.most_common(50),\n",
        "    'fourgrams': fourgram_counts.most_common(50)\n",
        "}\n",
        "print(f\"\\nStored top 50 patterns of each type for audio extraction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXBdzTAvk473",
        "outputId": "78b6306c-40ee-4f47-ee45-27194c0015a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DISCOVERED ACOUSTIC MOTIFS (N-gram Analysis)\n",
            "============================================================\n",
            "\n",
            "Top 20 Bigrams (2-unit patterns, ~40ms):\n",
            "--------------------------------------------------\n",
            "   1. U58_U58              count= 574\n",
            "   2. U79_U79              count= 459\n",
            "   3. U68_U68              count= 345\n",
            "   4. U12_U12              count= 330\n",
            "   5. U32_U32              count= 330\n",
            "   6. U55_U55              count= 307\n",
            "   7. U59_U59              count= 277\n",
            "   8. U04_U04              count= 240\n",
            "   9. U33_U33              count= 239\n",
            "  10. U64_U64              count= 228\n",
            "  11. U41_U41              count= 224\n",
            "  12. U96_U96              count= 217\n",
            "  13. U82_U82              count= 202\n",
            "  14. U75_U75              count= 187\n",
            "  15. U25_U25              count= 185\n",
            "  16. U53_U53              count= 184\n",
            "  17. U87_U87              count= 183\n",
            "  18. U11_U11              count= 178\n",
            "  19. U90_U90              count= 178\n",
            "  20. U83_U83              count= 176\n",
            "\n",
            "Top 20 Trigrams (3-unit patterns, ~60ms):\n",
            "--------------------------------------------------\n",
            "   1. U58_U58_U58          count= 511\n",
            "   2. U79_U79_U79          count= 275\n",
            "   3. U12_U12_U12          count= 265\n",
            "   4. U68_U68_U68          count= 230\n",
            "   5. U32_U32_U32          count= 230\n",
            "   6. U55_U55_U55          count= 216\n",
            "   7. U59_U59_U59          count= 161\n",
            "   8. U64_U64_U64          count= 161\n",
            "   9. U04_U04_U04          count= 129\n",
            "  10. U25_U25_U25          count= 119\n",
            "  11. U82_U82_U82          count= 112\n",
            "  12. U41_U41_U41          count= 110\n",
            "  13. U96_U96_U96          count= 101\n",
            "  14. U11_U11_U11          count=  95\n",
            "  15. U83_U83_U83          count=  93\n",
            "  16. U90_U90_U90          count=  88\n",
            "  17. U75_U75_U75          count=  86\n",
            "  18. U01_U01_U01          count=  81\n",
            "  19. U84_U84_U84          count=  80\n",
            "  20. U52_U52_U52          count=  79\n",
            "\n",
            "Top 20 Four-grams (4-unit patterns, ~80ms):\n",
            "--------------------------------------------------\n",
            "   1. U58_U58_U58_U58           count= 465\n",
            "   2. U12_U12_U12_U12           count= 225\n",
            "   3. U55_U55_U55_U55           count= 164\n",
            "   4. U79_U79_U79_U79           count= 164\n",
            "   5. U32_U32_U32_U32           count= 161\n",
            "   6. U68_U68_U68_U68           count= 149\n",
            "   7. U64_U64_U64_U64           count= 111\n",
            "   8. U25_U25_U25_U25           count=  82\n",
            "   9. U59_U59_U59_U59           count=  80\n",
            "  10. U28_U28_U28_U28           count=  58\n",
            "  11. U83_U83_U83_U83           count=  57\n",
            "  12. U04_U04_U04_U04           count=  55\n",
            "  13. U05_U05_U05_U05           count=  53\n",
            "  14. U82_U82_U82_U82           count=  50\n",
            "  15. U41_U41_U41_U41           count=  50\n",
            "  16. U96_U96_U96_U96           count=  47\n",
            "  17. U01_U01_U01_U01           count=  45\n",
            "  18. U89_U89_U89_U89           count=  43\n",
            "  19. U69_U69_U69_U69           count=  42\n",
            "  20. U84_U84_U84_U84           count=  42\n",
            "\n",
            "Stored top 50 patterns of each type for audio extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Find heterogeneous patterns (more linguistically interesting)\n",
        "from collections import Counter\n",
        "\n",
        "units_list = [f\"U{u:02d}\" for u in acoustic_units]\n",
        "\n",
        "# Find bigrams where units are DIFFERENT\n",
        "hetero_bigrams = [f\"{units_list[i]}_{units_list[i+1]}\"\n",
        "                  for i in range(len(units_list)-1)\n",
        "                  if units_list[i] != units_list[i+1]]\n",
        "hetero_bigram_counts = Counter(hetero_bigrams)\n",
        "\n",
        "# Find trigrams with at least 2 different units\n",
        "hetero_trigrams = []\n",
        "for i in range(len(units_list)-2):\n",
        "    tri = (units_list[i], units_list[i+1], units_list[i+2])\n",
        "    if len(set(tri)) >= 2:  # At least 2 different units\n",
        "        hetero_trigrams.append(f\"{tri[0]}_{tri[1]}_{tri[2]}\")\n",
        "hetero_trigram_counts = Counter(hetero_trigrams)\n",
        "\n",
        "# Find 4-grams with at least 2 different units\n",
        "hetero_fourgrams = []\n",
        "for i in range(len(units_list)-3):\n",
        "    fg = (units_list[i], units_list[i+1], units_list[i+2], units_list[i+3])\n",
        "    if len(set(fg)) >= 2:\n",
        "        hetero_fourgrams.append(f\"{fg[0]}_{fg[1]}_{fg[2]}_{fg[3]}\")\n",
        "hetero_fourgram_counts = Counter(hetero_fourgrams)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HETEROGENEOUS MOTIFS (Different Units = More Interesting)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nTop 20 Transition Bigrams:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(hetero_bigram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:20s} count={count:4d}\")\n",
        "\n",
        "print(\"\\nTop 20 Heterogeneous Trigrams:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(hetero_trigram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:20s} count={count:4d}\")\n",
        "\n",
        "print(\"\\nTop 20 Heterogeneous Four-grams:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (pattern, count) in enumerate(hetero_fourgram_counts.most_common(20)):\n",
        "    print(f\"  {i+1:2d}. {pattern:25s} count={count:4d}\")\n",
        "\n",
        "# Store the interesting motifs\n",
        "interesting_motifs = {\n",
        "    'transitions': hetero_bigram_counts.most_common(30),\n",
        "    'trigrams': hetero_trigram_counts.most_common(30),\n",
        "    'fourgrams': hetero_fourgram_counts.most_common(30)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhP21EC5lNre",
        "outputId": "992cd20d-e4d3-4152-df03-946cb538b347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "HETEROGENEOUS MOTIFS (Different Units = More Interesting)\n",
            "============================================================\n",
            "\n",
            "Top 20 Transition Bigrams:\n",
            "--------------------------------------------------\n",
            "   1. U62_U33              count= 146\n",
            "   2. U48_U43              count= 142\n",
            "   3. U33_U48              count= 128\n",
            "   4. U35_U97              count= 124\n",
            "   5. U33_U81              count=  94\n",
            "   6. U55_U25              count=  89\n",
            "   7. U97_U36              count=  88\n",
            "   8. U13_U33              count=  86\n",
            "   9. U32_U47              count=  82\n",
            "  10. U25_U55              count=  79\n",
            "  11. U98_U35              count=  79\n",
            "  12. U48_U02              count=  79\n",
            "  13. U36_U74              count=  79\n",
            "  14. U91_U92              count=  78\n",
            "  15. U36_U33              count=  78\n",
            "  16. U00_U60              count=  74\n",
            "  17. U34_U40              count=  73\n",
            "  18. U68_U84              count=  73\n",
            "  19. U85_U13              count=  72\n",
            "  20. U81_U43              count=  72\n",
            "\n",
            "Top 20 Heterogeneous Trigrams:\n",
            "--------------------------------------------------\n",
            "   1. U62_U33_U33          count=  56\n",
            "   2. U33_U48_U43          count=  52\n",
            "   3. U32_U32_U47          count=  51\n",
            "   4. U81_U81_U82          count=  50\n",
            "   5. U13_U33_U33          count=  50\n",
            "   6. U33_U33_U48          count=  49\n",
            "   7. U79_U79_U83          count=  48\n",
            "   8. U11_U11_U91          count=  48\n",
            "   9. U81_U82_U82          count=  45\n",
            "  10. U84_U68_U68          count=  45\n",
            "  11. U91_U91_U92          count=  44\n",
            "  12. U33_U33_U81          count=  43\n",
            "  13. U11_U91_U91          count=  43\n",
            "  14. U41_U04_U04          count=  42\n",
            "  15. U55_U55_U25          count=  41\n",
            "  16. U59_U59_U77          count=  41\n",
            "  17. U12_U12_U19          count=  39\n",
            "  18. U33_U81_U43          count=  38\n",
            "  19. U32_U47_U47          count=  38\n",
            "  20. U25_U55_U55          count=  37\n",
            "\n",
            "Top 20 Heterogeneous Four-grams:\n",
            "--------------------------------------------------\n",
            "   1. U11_U11_U91_U91           count=  41\n",
            "   2. U81_U81_U82_U82           count=  38\n",
            "   3. U81_U82_U82_U82           count=  37\n",
            "   4. U59_U59_U59_U77           count=  35\n",
            "   5. U11_U11_U11_U91           count=  34\n",
            "   6. U84_U68_U68_U68           count=  34\n",
            "   7. U59_U59_U77_U77           count=  33\n",
            "   8. U32_U32_U32_U47           count=  33\n",
            "   9. U79_U79_U79_U83           count=  27\n",
            "  10. U11_U91_U91_U91           count=  27\n",
            "  11. U55_U55_U55_U25           count=  26\n",
            "  12. U91_U91_U91_U92           count=  26\n",
            "  13. U79_U79_U83_U83           count=  24\n",
            "  14. U12_U12_U12_U19           count=  24\n",
            "  15. U68_U68_U68_U26           count=  24\n",
            "  16. U25_U55_U55_U55           count=  23\n",
            "  17. U70_U70_U99_U99           count=  23\n",
            "  18. U68_U68_U68_U84           count=  23\n",
            "  19. U32_U32_U47_U47           count=  23\n",
            "  20. U79_U79_U79_U44           count=  22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Extract audio samples for motifs\n",
        "import soundfile as sf\n",
        "\n",
        "motif_audio_dir = os.path.join(OUTPUT_DIR, \"motif_samples\")\n",
        "os.makedirs(motif_audio_dir, exist_ok=True)\n",
        "\n",
        "def find_pattern_positions(units_list, pattern):\n",
        "    \"\"\"Find where a pattern occurs in the unit sequence.\"\"\"\n",
        "    pattern_units = pattern.split(\"_\")\n",
        "    positions = []\n",
        "    for i in range(len(units_list) - len(pattern_units) + 1):\n",
        "        if units_list[i:i+len(pattern_units)] == pattern_units:\n",
        "            positions.append(i)\n",
        "    return positions\n",
        "\n",
        "def frame_to_time(frame_idx, total_frames, total_duration):\n",
        "    \"\"\"Convert frame index to time in seconds.\"\"\"\n",
        "    return (frame_idx / total_frames) * total_duration\n",
        "\n",
        "# Collect top motifs from each category\n",
        "motifs_to_extract = []\n",
        "\n",
        "# Top 15 transition bigrams\n",
        "for pattern, count in hetero_bigram_counts.most_common(15):\n",
        "    motifs_to_extract.append(('bigram', pattern, count))\n",
        "\n",
        "# Top 15 heterogeneous trigrams\n",
        "for pattern, count in hetero_trigram_counts.most_common(15):\n",
        "    motifs_to_extract.append(('trigram', pattern, count))\n",
        "\n",
        "# Top 10 heterogeneous fourgrams\n",
        "for pattern, count in hetero_fourgram_counts.most_common(10):\n",
        "    motifs_to_extract.append(('fourgram', pattern, count))\n",
        "\n",
        "print(f\"Extracting audio for {len(motifs_to_extract)} motifs...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "avita_data = []\n",
        "units_list = [f\"U{u:02d}\" for u in acoustic_units]\n",
        "\n",
        "for idx, (mtype, pattern, count) in enumerate(motifs_to_extract):\n",
        "    positions = find_pattern_positions(units_list, pattern)\n",
        "\n",
        "    # Extract up to 3 samples per motif\n",
        "    samples_extracted = 0\n",
        "    for pos in positions[:3]:\n",
        "        # Convert frame position to time\n",
        "        start_time = frame_to_time(pos, len(units_list), duration_seconds)\n",
        "\n",
        "        # Duration based on pattern length (each unit ~20ms) plus padding\n",
        "        n_units = len(pattern.split(\"_\"))\n",
        "        duration = (n_units * 0.02) + 0.15  # Add 150ms padding\n",
        "\n",
        "        # Add context before\n",
        "        start_time = max(0, start_time - 0.075)\n",
        "\n",
        "        # Extract audio\n",
        "        start_sample = int(start_time * SAMPLE_RATE)\n",
        "        end_sample = int((start_time + duration) * SAMPLE_RATE)\n",
        "        end_sample = min(end_sample, len(audio))\n",
        "\n",
        "        sample_audio = audio[start_sample:end_sample]\n",
        "\n",
        "        # Save audio file\n",
        "        filename = f\"motif_{idx:03d}_{mtype}_{samples_extracted}.wav\"\n",
        "        filepath = os.path.join(motif_audio_dir, filename)\n",
        "        sf.write(filepath, sample_audio, SAMPLE_RATE)\n",
        "\n",
        "        avita_data.append({\n",
        "            \"id\": f\"motif_{idx:03d}_{samples_extracted}\",\n",
        "            \"motif_id\": f\"motif_{idx:03d}\",\n",
        "            \"type\": mtype,\n",
        "            \"pattern\": pattern,\n",
        "            \"frequency\": count,\n",
        "            \"audio_file\": f\"motif_samples/{filename}\",\n",
        "            \"start_time\": round(start_time, 3),\n",
        "            \"duration\": round(duration, 3),\n",
        "            \"tags\": {}\n",
        "        })\n",
        "        samples_extracted += 1\n",
        "\n",
        "    if idx < 10:\n",
        "        print(f\"  {idx+1}. {pattern:25s} ({mtype:8s}) - {samples_extracted} samples\")\n",
        "\n",
        "print(f\"\\nTotal audio samples extracted: {len(avita_data)}\")\n",
        "print(f\"Saved to: {motif_audio_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0UDJfQClcsl",
        "outputId": "89602707-ccdb-43a6-b2cb-655ea69039d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting audio for 40 motifs...\n",
            "--------------------------------------------------\n",
            "  1. U62_U33                   (bigram  ) - 3 samples\n",
            "  2. U48_U43                   (bigram  ) - 3 samples\n",
            "  3. U33_U48                   (bigram  ) - 3 samples\n",
            "  4. U35_U97                   (bigram  ) - 3 samples\n",
            "  5. U33_U81                   (bigram  ) - 3 samples\n",
            "  6. U55_U25                   (bigram  ) - 3 samples\n",
            "  7. U97_U36                   (bigram  ) - 3 samples\n",
            "  8. U13_U33                   (bigram  ) - 3 samples\n",
            "  9. U32_U47                   (bigram  ) - 3 samples\n",
            "  10. U25_U55                   (bigram  ) - 3 samples\n",
            "\n",
            "Total audio samples extracted: 120\n",
            "Saved to: /content/drive/MyDrive/satere_project/phase2_output/motif_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Generate AViTA data file\n",
        "\n",
        "# Save as JavaScript for AViTA\n",
        "avita_js_path = os.path.join(OUTPUT_DIR, \"avita_motifs.js\")\n",
        "with open(avita_js_path, 'w') as f:\n",
        "    f.write(\"// AViTA Motif Data - Sateré-Mawé Acts 12\\n\")\n",
        "    f.write(f\"// Total motifs: {len(motifs_to_extract)}\\n\")\n",
        "    f.write(f\"// Total samples: {len(avita_data)}\\n\\n\")\n",
        "    f.write(f\"const MOTIF_SAMPLES = {json.dumps(avita_data, indent=2)};\\n\")\n",
        "\n",
        "# Save as JSON\n",
        "avita_json_path = os.path.join(OUTPUT_DIR, \"avita_motifs.json\")\n",
        "with open(avita_json_path, 'w') as f:\n",
        "    json.dump(avita_data, f, indent=2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 2 COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "FILES CREATED IN GOOGLE DRIVE:\n",
        "{OUTPUT_DIR}/\n",
        "├── audio_16khz.wav          (preprocessed audio)\n",
        "├── acoustic_units.json      (raw unit sequence)\n",
        "├── pseudotext.txt           (units as text)\n",
        "├── avita_motifs.js          (for AViTA app)\n",
        "├── avita_motifs.json        (same data as JSON)\n",
        "└── motif_samples/           ({len(avita_data)} audio files)\n",
        "\n",
        "SUMMARY:\n",
        "- Input: {duration_minutes:.1f} minutes of Sateré-Mawé audio\n",
        "- Extracted: {len(acoustic_units):,} acoustic frames\n",
        "- Clustered into: {N_CLUSTERS} discrete units\n",
        "- Discovered: {len(motifs_to_extract)} recurring motifs\n",
        "- Created: {len(avita_data)} audio samples for tagging\n",
        "\n",
        "NEXT STEPS:\n",
        "1. Download the 'phase2_output' folder from Google Drive\n",
        "2. Copy avita_motifs.js and motif_samples/ to your AViTA folder\n",
        "3. Open AViTA and tag each motif with its grammatical meaning\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj7MH_-hlqSv",
        "outputId": "505982e9-a973-4feb-e1d6-73c15bbc3da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 2 COMPLETE!\n",
            "============================================================\n",
            "\n",
            "FILES CREATED IN GOOGLE DRIVE:\n",
            "/content/drive/MyDrive/satere_project/phase2_output/\n",
            "├── audio_16khz.wav          (preprocessed audio)\n",
            "├── acoustic_units.json      (raw unit sequence)\n",
            "├── pseudotext.txt           (units as text)\n",
            "├── avita_motifs.js          (for AViTA app)\n",
            "├── avita_motifs.json        (same data as JSON)\n",
            "└── motif_samples/           (120 audio files)\n",
            "\n",
            "SUMMARY:\n",
            "- Input: 9.3 minutes of Sateré-Mawé audio\n",
            "- Extracted: 27,830 acoustic frames\n",
            "- Clustered into: 100 discrete units\n",
            "- Discovered: 40 recurring motifs\n",
            "- Created: 120 audio samples for tagging\n",
            "\n",
            "NEXT STEPS:\n",
            "1. Download the 'phase2_output' folder from Google Drive\n",
            "2. Copy avita_motifs.js and motif_samples/ to your AViTA folder\n",
            "3. Open AViTA and tag each motif with its grammatical meaning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create avita_motifs.js file (JavaScript format)\n",
        "avita_js_path = os.path.join(OUTPUT_DIR, \"avita_motifs.js\")\n",
        "with open(avita_js_path, 'w') as f:\n",
        "    f.write(\"const MOTIF_SAMPLES = \")\n",
        "    f.write(json.dumps(avita_data, indent=2))\n",
        "    f.write(\";\")\n",
        "\n",
        "print(f\"Created: {avita_js_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCAYchPGmDjg",
        "outputId": "383a53a0-eec0-4c87-b241-6610c9183ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: /content/drive/MyDrive/satere_project/phase2_output/avita_motifs.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if avita_data has content\n",
        "print(f\"avita_data has {len(avita_data)} items\")\n",
        "\n",
        "# Recreate the JS file with the data\n",
        "avita_js_path = os.path.join(OUTPUT_DIR, \"avita_motifs.js\")\n",
        "with open(avita_js_path, 'w') as f:\n",
        "    f.write(\"const MOTIF_SAMPLES = \")\n",
        "    f.write(json.dumps(avita_data, indent=2))\n",
        "    f.write(\";\")\n",
        "\n",
        "print(f\"Created {avita_js_path} with {len(avita_data)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it1YeUa8yCgC",
        "outputId": "92481653-6057-43f9-91f2-97b51d2ad270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avita_data has 120 items\n",
            "Created /content/drive/MyDrive/satere_project/phase2_output/avita_motifs.js with 120 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the data so we can embed it\n",
        "import json\n",
        "print(json.dumps(avita_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSzorfbx2sFm",
        "outputId": "075bdf11-81cb-44b9-a11f-57e894c0c8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"id\": \"motif_000_0\", \"motif_id\": \"motif_000\", \"type\": \"bigram\", \"pattern\": \"U62_U33\", \"frequency\": 146, \"audio_file\": \"motif_samples/motif_000_bigram_0.wav\", \"start_time\": 1.026, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_000_1\", \"motif_id\": \"motif_000\", \"type\": \"bigram\", \"pattern\": \"U62_U33\", \"frequency\": 146, \"audio_file\": \"motif_samples/motif_000_bigram_1.wav\", \"start_time\": 2.527, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_000_2\", \"motif_id\": \"motif_000\", \"type\": \"bigram\", \"pattern\": \"U62_U33\", \"frequency\": 146, \"audio_file\": \"motif_samples/motif_000_bigram_2.wav\", \"start_time\": 3.207, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_001_0\", \"motif_id\": \"motif_001\", \"type\": \"bigram\", \"pattern\": \"U48_U43\", \"frequency\": 142, \"audio_file\": \"motif_samples/motif_001_bigram_0.wav\", \"start_time\": 6.169, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_001_1\", \"motif_id\": \"motif_001\", \"type\": \"bigram\", \"pattern\": \"U48_U43\", \"frequency\": 142, \"audio_file\": \"motif_samples/motif_001_bigram_1.wav\", \"start_time\": 9.572, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_001_2\", \"motif_id\": \"motif_001\", \"type\": \"bigram\", \"pattern\": \"U48_U43\", \"frequency\": 142, \"audio_file\": \"motif_samples/motif_001_bigram_2.wav\", \"start_time\": 15.516, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_002_0\", \"motif_id\": \"motif_002\", \"type\": \"bigram\", \"pattern\": \"U33_U48\", \"frequency\": 128, \"audio_file\": \"motif_samples/motif_002_bigram_0.wav\", \"start_time\": 9.032, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_002_1\", \"motif_id\": \"motif_002\", \"type\": \"bigram\", \"pattern\": \"U33_U48\", \"frequency\": 128, \"audio_file\": \"motif_samples/motif_002_bigram_1.wav\", \"start_time\": 30.647, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_002_2\", \"motif_id\": \"motif_002\", \"type\": \"bigram\", \"pattern\": \"U33_U48\", \"frequency\": 128, \"audio_file\": \"motif_samples/motif_002_bigram_2.wav\", \"start_time\": 35.33, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_003_0\", \"motif_id\": \"motif_003\", \"type\": \"bigram\", \"pattern\": \"U35_U97\", \"frequency\": 124, \"audio_file\": \"motif_samples/motif_003_bigram_0.wav\", \"start_time\": 23.922, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_003_1\", \"motif_id\": \"motif_003\", \"type\": \"bigram\", \"pattern\": \"U35_U97\", \"frequency\": 124, \"audio_file\": \"motif_samples/motif_003_bigram_1.wav\", \"start_time\": 31.668, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_003_2\", \"motif_id\": \"motif_003\", \"type\": \"bigram\", \"pattern\": \"U35_U97\", \"frequency\": 124, \"audio_file\": \"motif_samples/motif_003_bigram_2.wav\", \"start_time\": 36.011, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_004_0\", \"motif_id\": \"motif_004\", \"type\": \"bigram\", \"pattern\": \"U33_U81\", \"frequency\": 94, \"audio_file\": \"motif_samples/motif_004_bigram_0.wav\", \"start_time\": 1.046, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_004_1\", \"motif_id\": \"motif_004\", \"type\": \"bigram\", \"pattern\": \"U33_U81\", \"frequency\": 94, \"audio_file\": \"motif_samples/motif_004_bigram_1.wav\", \"start_time\": 8.511, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_004_2\", \"motif_id\": \"motif_004\", \"type\": \"bigram\", \"pattern\": \"U33_U81\", \"frequency\": 94, \"audio_file\": \"motif_samples/motif_004_bigram_2.wav\", \"start_time\": 9.752, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_005_0\", \"motif_id\": \"motif_005\", \"type\": \"bigram\", \"pattern\": \"U55_U25\", \"frequency\": 89, \"audio_file\": \"motif_samples/motif_005_bigram_0.wav\", \"start_time\": 0, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_005_1\", \"motif_id\": \"motif_005\", \"type\": \"bigram\", \"pattern\": \"U55_U25\", \"frequency\": 89, \"audio_file\": \"motif_samples/motif_005_bigram_1.wav\", \"start_time\": 0.305, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_005_2\", \"motif_id\": \"motif_005\", \"type\": \"bigram\", \"pattern\": \"U55_U25\", \"frequency\": 89, \"audio_file\": \"motif_samples/motif_005_bigram_2.wav\", \"start_time\": 3.387, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_006_0\", \"motif_id\": \"motif_006\", \"type\": \"bigram\", \"pattern\": \"U97_U36\", \"frequency\": 88, \"audio_file\": \"motif_samples/motif_006_bigram_0.wav\", \"start_time\": 30.927, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_006_1\", \"motif_id\": \"motif_006\", \"type\": \"bigram\", \"pattern\": \"U97_U36\", \"frequency\": 88, \"audio_file\": \"motif_samples/motif_006_bigram_1.wav\", \"start_time\": 36.872, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_006_2\", \"motif_id\": \"motif_006\", \"type\": \"bigram\", \"pattern\": \"U97_U36\", \"frequency\": 88, \"audio_file\": \"motif_samples/motif_006_bigram_2.wav\", \"start_time\": 37.752, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_007_0\", \"motif_id\": \"motif_007\", \"type\": \"bigram\", \"pattern\": \"U13_U33\", \"frequency\": 86, \"audio_file\": \"motif_samples/motif_007_bigram_0.wav\", \"start_time\": 8.051, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_007_1\", \"motif_id\": \"motif_007\", \"type\": \"bigram\", \"pattern\": \"U13_U33\", \"frequency\": 86, \"audio_file\": \"motif_samples/motif_007_bigram_1.wav\", \"start_time\": 10.232, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_007_2\", \"motif_id\": \"motif_007\", \"type\": \"bigram\", \"pattern\": \"U13_U33\", \"frequency\": 86, \"audio_file\": \"motif_samples/motif_007_bigram_2.wav\", \"start_time\": 20.76, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_008_0\", \"motif_id\": \"motif_008\", \"type\": \"bigram\", \"pattern\": \"U32_U47\", \"frequency\": 82, \"audio_file\": \"motif_samples/motif_008_bigram_0.wav\", \"start_time\": 176.051, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_008_1\", \"motif_id\": \"motif_008\", \"type\": \"bigram\", \"pattern\": \"U32_U47\", \"frequency\": 82, \"audio_file\": \"motif_samples/motif_008_bigram_1.wav\", \"start_time\": 179.634, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_008_2\", \"motif_id\": \"motif_008\", \"type\": \"bigram\", \"pattern\": \"U32_U47\", \"frequency\": 82, \"audio_file\": \"motif_samples/motif_008_bigram_2.wav\", \"start_time\": 181.175, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_009_0\", \"motif_id\": \"motif_009\", \"type\": \"bigram\", \"pattern\": \"U25_U55\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_009_bigram_0.wav\", \"start_time\": 0.085, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_009_1\", \"motif_id\": \"motif_009\", \"type\": \"bigram\", \"pattern\": \"U25_U55\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_009_bigram_1.wav\", \"start_time\": 3.408, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_009_2\", \"motif_id\": \"motif_009\", \"type\": \"bigram\", \"pattern\": \"U25_U55\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_009_bigram_2.wav\", \"start_time\": 3.728, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_010_0\", \"motif_id\": \"motif_010\", \"type\": \"bigram\", \"pattern\": \"U98_U35\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_010_bigram_0.wav\", \"start_time\": 14.115, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_010_1\", \"motif_id\": \"motif_010\", \"type\": \"bigram\", \"pattern\": \"U98_U35\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_010_bigram_1.wav\", \"start_time\": 27.405, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_010_2\", \"motif_id\": \"motif_010\", \"type\": \"bigram\", \"pattern\": \"U98_U35\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_010_bigram_2.wav\", \"start_time\": 29.386, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_011_0\", \"motif_id\": \"motif_011\", \"type\": \"bigram\", \"pattern\": \"U48_U02\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_011_bigram_0.wav\", \"start_time\": 30.967, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_011_1\", \"motif_id\": \"motif_011\", \"type\": \"bigram\", \"pattern\": \"U48_U02\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_011_bigram_1.wav\", \"start_time\": 43.937, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_011_2\", \"motif_id\": \"motif_011\", \"type\": \"bigram\", \"pattern\": \"U48_U02\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_011_bigram_2.wav\", \"start_time\": 55.225, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_012_0\", \"motif_id\": \"motif_012\", \"type\": \"bigram\", \"pattern\": \"U36_U74\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_012_bigram_0.wav\", \"start_time\": 40.154, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_012_1\", \"motif_id\": \"motif_012\", \"type\": \"bigram\", \"pattern\": \"U36_U74\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_012_bigram_1.wav\", \"start_time\": 44.677, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_012_2\", \"motif_id\": \"motif_012\", \"type\": \"bigram\", \"pattern\": \"U36_U74\", \"frequency\": 79, \"audio_file\": \"motif_samples/motif_012_bigram_2.wav\", \"start_time\": 61.99, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_013_0\", \"motif_id\": \"motif_013\", \"type\": \"bigram\", \"pattern\": \"U91_U92\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_013_bigram_0.wav\", \"start_time\": 7.23, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_013_1\", \"motif_id\": \"motif_013\", \"type\": \"bigram\", \"pattern\": \"U91_U92\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_013_bigram_1.wav\", \"start_time\": 31.628, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_013_2\", \"motif_id\": \"motif_013\", \"type\": \"bigram\", \"pattern\": \"U91_U92\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_013_bigram_2.wav\", \"start_time\": 33.489, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_014_0\", \"motif_id\": \"motif_014\", \"type\": \"bigram\", \"pattern\": \"U36_U33\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_014_bigram_0.wav\", \"start_time\": 15.977, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_014_1\", \"motif_id\": \"motif_014\", \"type\": \"bigram\", \"pattern\": \"U36_U33\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_014_bigram_1.wav\", \"start_time\": 36.091, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_014_2\", \"motif_id\": \"motif_014\", \"type\": \"bigram\", \"pattern\": \"U36_U33\", \"frequency\": 78, \"audio_file\": \"motif_samples/motif_014_bigram_2.wav\", \"start_time\": 37.772, \"duration\": 0.19, \"tags\": {}}, {\"id\": \"motif_015_0\", \"motif_id\": \"motif_015\", \"type\": \"trigram\", \"pattern\": \"U62_U33_U33\", \"frequency\": 56, \"audio_file\": \"motif_samples/motif_015_trigram_0.wav\", \"start_time\": 5.669, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_015_1\", \"motif_id\": \"motif_015\", \"type\": \"trigram\", \"pattern\": \"U62_U33_U33\", \"frequency\": 56, \"audio_file\": \"motif_samples/motif_015_trigram_1.wav\", \"start_time\": 6.97, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_015_2\", \"motif_id\": \"motif_015\", \"type\": \"trigram\", \"pattern\": \"U62_U33_U33\", \"frequency\": 56, \"audio_file\": \"motif_samples/motif_015_trigram_2.wav\", \"start_time\": 14.896, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_016_0\", \"motif_id\": \"motif_016\", \"type\": \"trigram\", \"pattern\": \"U33_U48_U43\", \"frequency\": 52, \"audio_file\": \"motif_samples/motif_016_trigram_0.wav\", \"start_time\": 30.647, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_016_1\", \"motif_id\": \"motif_016\", \"type\": \"trigram\", \"pattern\": \"U33_U48_U43\", \"frequency\": 52, \"audio_file\": \"motif_samples/motif_016_trigram_1.wav\", \"start_time\": 35.33, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_016_2\", \"motif_id\": \"motif_016\", \"type\": \"trigram\", \"pattern\": \"U33_U48_U43\", \"frequency\": 52, \"audio_file\": \"motif_samples/motif_016_trigram_2.wav\", \"start_time\": 38.613, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_017_0\", \"motif_id\": \"motif_017\", \"type\": \"trigram\", \"pattern\": \"U32_U32_U47\", \"frequency\": 51, \"audio_file\": \"motif_samples/motif_017_trigram_0.wav\", \"start_time\": 176.031, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_017_1\", \"motif_id\": \"motif_017\", \"type\": \"trigram\", \"pattern\": \"U32_U32_U47\", \"frequency\": 51, \"audio_file\": \"motif_samples/motif_017_trigram_1.wav\", \"start_time\": 181.155, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_017_2\", \"motif_id\": \"motif_017\", \"type\": \"trigram\", \"pattern\": \"U32_U32_U47\", \"frequency\": 51, \"audio_file\": \"motif_samples/motif_017_trigram_2.wav\", \"start_time\": 182.576, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_018_0\", \"motif_id\": \"motif_018\", \"type\": \"trigram\", \"pattern\": \"U81_U81_U82\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_018_trigram_0.wav\", \"start_time\": 4.728, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_018_1\", \"motif_id\": \"motif_018\", \"type\": \"trigram\", \"pattern\": \"U81_U81_U82\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_018_trigram_1.wav\", \"start_time\": 17.598, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_018_2\", \"motif_id\": \"motif_018\", \"type\": \"trigram\", \"pattern\": \"U81_U81_U82\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_018_trigram_2.wav\", \"start_time\": 34.29, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_019_0\", \"motif_id\": \"motif_019\", \"type\": \"trigram\", \"pattern\": \"U13_U33_U33\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_019_trigram_0.wav\", \"start_time\": 8.051, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_019_1\", \"motif_id\": \"motif_019\", \"type\": \"trigram\", \"pattern\": \"U13_U33_U33\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_019_trigram_1.wav\", \"start_time\": 10.232, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_019_2\", \"motif_id\": \"motif_019\", \"type\": \"trigram\", \"pattern\": \"U13_U33_U33\", \"frequency\": 50, \"audio_file\": \"motif_samples/motif_019_trigram_2.wav\", \"start_time\": 22.321, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_020_0\", \"motif_id\": \"motif_020\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U48\", \"frequency\": 49, \"audio_file\": \"motif_samples/motif_020_trigram_0.wav\", \"start_time\": 35.31, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_020_1\", \"motif_id\": \"motif_020\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U48\", \"frequency\": 49, \"audio_file\": \"motif_samples/motif_020_trigram_1.wav\", \"start_time\": 38.593, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_020_2\", \"motif_id\": \"motif_020\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U48\", \"frequency\": 49, \"audio_file\": \"motif_samples/motif_020_trigram_2.wav\", \"start_time\": 56.726, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_021_0\", \"motif_id\": \"motif_021\", \"type\": \"trigram\", \"pattern\": \"U79_U79_U83\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_021_trigram_0.wav\", \"start_time\": 4.889, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_021_1\", \"motif_id\": \"motif_021\", \"type\": \"trigram\", \"pattern\": \"U79_U79_U83\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_021_trigram_1.wav\", \"start_time\": 11.954, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_021_2\", \"motif_id\": \"motif_021\", \"type\": \"trigram\", \"pattern\": \"U79_U79_U83\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_021_trigram_2.wav\", \"start_time\": 16.297, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_022_0\", \"motif_id\": \"motif_022\", \"type\": \"trigram\", \"pattern\": \"U11_U11_U91\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_022_trigram_0.wav\", \"start_time\": 16.737, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_022_1\", \"motif_id\": \"motif_022\", \"type\": \"trigram\", \"pattern\": \"U11_U11_U91\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_022_trigram_1.wav\", \"start_time\": 39.193, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_022_2\", \"motif_id\": \"motif_022\", \"type\": \"trigram\", \"pattern\": \"U11_U11_U91\", \"frequency\": 48, \"audio_file\": \"motif_samples/motif_022_trigram_2.wav\", \"start_time\": 40.754, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_023_0\", \"motif_id\": \"motif_023\", \"type\": \"trigram\", \"pattern\": \"U81_U82_U82\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_023_trigram_0.wav\", \"start_time\": 0.425, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_023_1\", \"motif_id\": \"motif_023\", \"type\": \"trigram\", \"pattern\": \"U81_U82_U82\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_023_trigram_1.wav\", \"start_time\": 4.748, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_023_2\", \"motif_id\": \"motif_023\", \"type\": \"trigram\", \"pattern\": \"U81_U82_U82\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_023_trigram_2.wav\", \"start_time\": 17.618, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_024_0\", \"motif_id\": \"motif_024\", \"type\": \"trigram\", \"pattern\": \"U84_U68_U68\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_024_trigram_0.wav\", \"start_time\": 180.415, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_024_1\", \"motif_id\": \"motif_024\", \"type\": \"trigram\", \"pattern\": \"U84_U68_U68\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_024_trigram_1.wav\", \"start_time\": 180.495, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_024_2\", \"motif_id\": \"motif_024\", \"type\": \"trigram\", \"pattern\": \"U84_U68_U68\", \"frequency\": 45, \"audio_file\": \"motif_samples/motif_024_trigram_2.wav\", \"start_time\": 184.678, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_025_0\", \"motif_id\": \"motif_025\", \"type\": \"trigram\", \"pattern\": \"U91_U91_U92\", \"frequency\": 44, \"audio_file\": \"motif_samples/motif_025_trigram_0.wav\", \"start_time\": 31.608, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_025_1\", \"motif_id\": \"motif_025\", \"type\": \"trigram\", \"pattern\": \"U91_U91_U92\", \"frequency\": 44, \"audio_file\": \"motif_samples/motif_025_trigram_1.wav\", \"start_time\": 33.469, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_025_2\", \"motif_id\": \"motif_025\", \"type\": \"trigram\", \"pattern\": \"U91_U91_U92\", \"frequency\": 44, \"audio_file\": \"motif_samples/motif_025_trigram_2.wav\", \"start_time\": 40.794, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_026_0\", \"motif_id\": \"motif_026\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U81\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_026_trigram_0.wav\", \"start_time\": 8.491, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_026_1\", \"motif_id\": \"motif_026\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U81\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_026_trigram_1.wav\", \"start_time\": 14.916, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_026_2\", \"motif_id\": \"motif_026\", \"type\": \"trigram\", \"pattern\": \"U33_U33_U81\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_026_trigram_2.wav\", \"start_time\": 15.997, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_027_0\", \"motif_id\": \"motif_027\", \"type\": \"trigram\", \"pattern\": \"U11_U91_U91\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_027_trigram_0.wav\", \"start_time\": 39.213, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_027_1\", \"motif_id\": \"motif_027\", \"type\": \"trigram\", \"pattern\": \"U11_U91_U91\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_027_trigram_1.wav\", \"start_time\": 40.774, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_027_2\", \"motif_id\": \"motif_027\", \"type\": \"trigram\", \"pattern\": \"U11_U91_U91\", \"frequency\": 43, \"audio_file\": \"motif_samples/motif_027_trigram_2.wav\", \"start_time\": 45.198, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_028_0\", \"motif_id\": \"motif_028\", \"type\": \"trigram\", \"pattern\": \"U41_U04_U04\", \"frequency\": 42, \"audio_file\": \"motif_samples/motif_028_trigram_0.wav\", \"start_time\": 6.25, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_028_1\", \"motif_id\": \"motif_028\", \"type\": \"trigram\", \"pattern\": \"U41_U04_U04\", \"frequency\": 42, \"audio_file\": \"motif_samples/motif_028_trigram_1.wav\", \"start_time\": 26.624, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_028_2\", \"motif_id\": \"motif_028\", \"type\": \"trigram\", \"pattern\": \"U41_U04_U04\", \"frequency\": 42, \"audio_file\": \"motif_samples/motif_028_trigram_2.wav\", \"start_time\": 42.335, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_029_0\", \"motif_id\": \"motif_029\", \"type\": \"trigram\", \"pattern\": \"U55_U55_U25\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_029_trigram_0.wav\", \"start_time\": 0, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_029_1\", \"motif_id\": \"motif_029\", \"type\": \"trigram\", \"pattern\": \"U55_U55_U25\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_029_trigram_1.wav\", \"start_time\": 0.285, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_029_2\", \"motif_id\": \"motif_029\", \"type\": \"trigram\", \"pattern\": \"U55_U55_U25\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_029_trigram_2.wav\", \"start_time\": 3.448, \"duration\": 0.21, \"tags\": {}}, {\"id\": \"motif_030_0\", \"motif_id\": \"motif_030\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U91_U91\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_030_fourgram_0.wav\", \"start_time\": 39.193, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_030_1\", \"motif_id\": \"motif_030\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U91_U91\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_030_fourgram_1.wav\", \"start_time\": 40.754, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_030_2\", \"motif_id\": \"motif_030\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U91_U91\", \"frequency\": 41, \"audio_file\": \"motif_samples/motif_030_fourgram_2.wav\", \"start_time\": 45.177, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_031_0\", \"motif_id\": \"motif_031\", \"type\": \"fourgram\", \"pattern\": \"U81_U81_U82_U82\", \"frequency\": 38, \"audio_file\": \"motif_samples/motif_031_fourgram_0.wav\", \"start_time\": 4.728, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_031_1\", \"motif_id\": \"motif_031\", \"type\": \"fourgram\", \"pattern\": \"U81_U81_U82_U82\", \"frequency\": 38, \"audio_file\": \"motif_samples/motif_031_fourgram_1.wav\", \"start_time\": 17.598, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_031_2\", \"motif_id\": \"motif_031\", \"type\": \"fourgram\", \"pattern\": \"U81_U81_U82_U82\", \"frequency\": 38, \"audio_file\": \"motif_samples/motif_031_fourgram_2.wav\", \"start_time\": 34.29, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_032_0\", \"motif_id\": \"motif_032\", \"type\": \"fourgram\", \"pattern\": \"U81_U82_U82_U82\", \"frequency\": 37, \"audio_file\": \"motif_samples/motif_032_fourgram_0.wav\", \"start_time\": 4.748, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_032_1\", \"motif_id\": \"motif_032\", \"type\": \"fourgram\", \"pattern\": \"U81_U82_U82_U82\", \"frequency\": 37, \"audio_file\": \"motif_samples/motif_032_fourgram_1.wav\", \"start_time\": 17.618, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_032_2\", \"motif_id\": \"motif_032\", \"type\": \"fourgram\", \"pattern\": \"U81_U82_U82_U82\", \"frequency\": 37, \"audio_file\": \"motif_samples/motif_032_fourgram_2.wav\", \"start_time\": 34.31, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_033_0\", \"motif_id\": \"motif_033\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U59_U77\", \"frequency\": 35, \"audio_file\": \"motif_samples/motif_033_fourgram_0.wav\", \"start_time\": 34.85, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_033_1\", \"motif_id\": \"motif_033\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U59_U77\", \"frequency\": 35, \"audio_file\": \"motif_samples/motif_033_fourgram_1.wav\", \"start_time\": 46.659, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_033_2\", \"motif_id\": \"motif_033\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U59_U77\", \"frequency\": 35, \"audio_file\": \"motif_samples/motif_033_fourgram_2.wav\", \"start_time\": 57.386, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_034_0\", \"motif_id\": \"motif_034\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U11_U91\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_034_fourgram_0.wav\", \"start_time\": 16.717, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_034_1\", \"motif_id\": \"motif_034\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U11_U91\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_034_fourgram_1.wav\", \"start_time\": 39.173, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_034_2\", \"motif_id\": \"motif_034\", \"type\": \"fourgram\", \"pattern\": \"U11_U11_U11_U91\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_034_fourgram_2.wav\", \"start_time\": 40.734, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_035_0\", \"motif_id\": \"motif_035\", \"type\": \"fourgram\", \"pattern\": \"U84_U68_U68_U68\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_035_fourgram_0.wav\", \"start_time\": 180.495, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_035_1\", \"motif_id\": \"motif_035\", \"type\": \"fourgram\", \"pattern\": \"U84_U68_U68_U68\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_035_fourgram_1.wav\", \"start_time\": 184.678, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_035_2\", \"motif_id\": \"motif_035\", \"type\": \"fourgram\", \"pattern\": \"U84_U68_U68_U68\", \"frequency\": 34, \"audio_file\": \"motif_samples/motif_035_fourgram_2.wav\", \"start_time\": 184.778, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_036_0\", \"motif_id\": \"motif_036\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U77_U77\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_036_fourgram_0.wav\", \"start_time\": 5.429, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_036_1\", \"motif_id\": \"motif_036\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U77_U77\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_036_fourgram_1.wav\", \"start_time\": 18.338, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_036_2\", \"motif_id\": \"motif_036\", \"type\": \"fourgram\", \"pattern\": \"U59_U59_U77_U77\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_036_fourgram_2.wav\", \"start_time\": 34.87, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_037_0\", \"motif_id\": \"motif_037\", \"type\": \"fourgram\", \"pattern\": \"U32_U32_U32_U47\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_037_fourgram_0.wav\", \"start_time\": 176.011, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_037_1\", \"motif_id\": \"motif_037\", \"type\": \"fourgram\", \"pattern\": \"U32_U32_U32_U47\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_037_fourgram_1.wav\", \"start_time\": 185.118, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_037_2\", \"motif_id\": \"motif_037\", \"type\": \"fourgram\", \"pattern\": \"U32_U32_U32_U47\", \"frequency\": 33, \"audio_file\": \"motif_samples/motif_037_fourgram_2.wav\", \"start_time\": 193.484, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_038_0\", \"motif_id\": \"motif_038\", \"type\": \"fourgram\", \"pattern\": \"U79_U79_U79_U83\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_038_fourgram_0.wav\", \"start_time\": 11.934, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_038_1\", \"motif_id\": \"motif_038\", \"type\": \"fourgram\", \"pattern\": \"U79_U79_U79_U83\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_038_fourgram_1.wav\", \"start_time\": 16.277, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_038_2\", \"motif_id\": \"motif_038\", \"type\": \"fourgram\", \"pattern\": \"U79_U79_U79_U83\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_038_fourgram_2.wav\", \"start_time\": 17.798, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_039_0\", \"motif_id\": \"motif_039\", \"type\": \"fourgram\", \"pattern\": \"U11_U91_U91_U91\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_039_fourgram_0.wav\", \"start_time\": 39.213, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_039_1\", \"motif_id\": \"motif_039\", \"type\": \"fourgram\", \"pattern\": \"U11_U91_U91_U91\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_039_fourgram_1.wav\", \"start_time\": 45.198, \"duration\": 0.23, \"tags\": {}}, {\"id\": \"motif_039_2\", \"motif_id\": \"motif_039\", \"type\": \"fourgram\", \"pattern\": \"U11_U91_U91_U91\", \"frequency\": 27, \"audio_file\": \"motif_samples/motif_039_fourgram_2.wav\", \"start_time\": 60.148, \"duration\": 0.23, \"tags\": {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 3: Analyze discovered motifs\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nStep 3: Analyzing discovered motifs...\")\n",
        "\n",
        "# Encode the full corpus\n",
        "all_encoded = []\n",
        "for line in lines:\n",
        "    encoded = sp.encode_as_pieces(line.strip())\n",
        "    all_encoded.extend(encoded)\n",
        "\n",
        "motif_counts = Counter(all_encoded)\n",
        "\n",
        "print(f\"  Total unique motifs: {len(motif_counts)}\")\n",
        "print(f\"  Total tokens: {len(all_encoded):,}\")\n",
        "\n",
        "# Categorize motifs\n",
        "single_u# =============================================================================\n",
        "# SOUNDSCRIPT GENERATION: Full NT Corpus (45 hours)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 1: Combine all unit sequences from the full corpus\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Step 1: Loading all unit sequences...\")\n",
        "\n",
        "all_units_file = f\"{UNITS_DIR}/all_units_for_bpe.txt\"\n",
        "\n",
        "# Check file exists and count lines\n",
        "with open(all_units_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(f\"  Found {len(lines)} files in corpus\")\n",
        "\n",
        "# Count total units\n",
        "total_units = sum(len(line.strip().split()) for line in lines)\n",
        "print(f\"  Total acoustic units: {total_units:,}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 2: Train BPE with larger vocabulary for full corpus\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nStep 2: Training BPE model (this takes 10-30 minutes)...\")\n",
        "\n",
        "VOCAB_SIZE = 500  # Larger vocabulary for 45 hours of audio\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=all_units_file,\n",
        "    model_prefix=f\"{OUTPUT_DIR}/soundscript_satere\",\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    num_threads=4,\n",
        "    max_sentence_length=500000,  # Allow longer sequences\n",
        "    train_extremely_large_corpus=True\n",
        ")\n",
        "\n",
        "print(\"  BPE model trained!\")\n",
        "\n",
        "# Load the trained model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Vocabulary size: {sp.get_piece_size()}\")\n",
        "nit = []\n",
        "multi_unit = []\n",
        "special = []\n",
        "\n",
        "for motif, count in motif_counts.items():\n",
        "    clean = motif.replace(\"▁\", \"\").strip()\n",
        "    if not clean or clean in [\"U\", \"\"]:\n",
        "        special.append((motif, count))\n",
        "    elif \"_\" in clean or len(clean) > 2:\n",
        "        multi_unit.append((motif, count))\n",
        "    else:\n",
        "        single_unit.append((motif, count))\n",
        "\n",
        "print(f\"\\n  Single-unit motifs: {len(single_unit)}\")\n",
        "print(f\"  Multi-unit motifs: {len(multi_unit)}\")\n",
        "print(f\"  Special tokens: {len(special)}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 4: Display top motifs\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOP 50 MOTIFS BY FREQUENCY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, (motif, count) in enumerate(motif_counts.most_common(50)):\n",
        "    print(f\"  {i+1:3d}. {motif:30s} count={count:,}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 5: Save motif inventory\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nStep 5: Saving SoundScript inventory...\")\n",
        "\n",
        "inventory = {\n",
        "    \"language\": \"Sateré-Mawé\",\n",
        "    \"corpus_files\": len(lines),\n",
        "    \"total_units\": total_units,\n",
        "    \"vocab_size\": sp.get_piece_size(),\n",
        "    \"motifs\": [\n",
        "        {\"id\": i, \"symbol\": motif, \"count\": count}\n",
        "        for i, (motif, count) in enumerate(motif_counts.most_common())\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/soundscript_inventory.json\", \"w\") as f:\n",
        "    json.dump(inventory, f, indent=2)\n",
        "\n",
        "print(f\"  Saved to: {OUTPUT_DIR}/soundscript_inventory.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ SOUNDSCRIPT GENERATION COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Model: {OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Vocab: {OUTPUT_DIR}/soundscript_satere.vocab\")\n",
        "print(f\"  Inventory: {OUTPUT_DIR}/soundscript_inventory.json\")"
      ],
      "metadata": {
        "id": "q9QrIUQm36JY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1cedb665-8df7-4da7-94a1-9a170a8b7781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading all unit sequences...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/satere_units/all_units_for_bpe.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1992814877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Check file exists and count lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_units_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/satere_units/all_units_for_bpe.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore what files you have\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "print(\"Contents of satere_project:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for item in os.listdir(PROJECT_ROOT):\n",
        "    item_path = os.path.join(PROJECT_ROOT, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"📁 {item}/\")\n",
        "        # Show what's inside each folder\n",
        "        try:\n",
        "            contents = os.listdir(item_path)[:10]  # First 10 items\n",
        "            for sub in contents:\n",
        "                print(f\"    {sub}\")\n",
        "            if len(os.listdir(item_path)) > 10:\n",
        "                print(f\"    ... and {len(os.listdir(item_path)) - 10} more files\")\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        print(f\"📄 {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jopxZUgk2tva",
        "outputId": "d5f15030-37de-48ff-8591-a7063c4980dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of satere_project:\n",
            "--------------------------------------------------\n",
            "📁 soundscript_full/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep search for your tokenized files\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "print(\"Full directory tree:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for root, dirs, files in os.walk(PROJECT_ROOT):\n",
        "    level = root.replace(PROJECT_ROOT, '').count(os.sep)\n",
        "    indent = '  ' * level\n",
        "    print(f\"{indent}📁 {os.path.basename(root)}/\")\n",
        "\n",
        "    # Show files\n",
        "    sub_indent = '  ' * (level + 1)\n",
        "    for file in files[:15]:\n",
        "        print(f\"{sub_indent}📄 {file}\")\n",
        "    if len(files) > 15:\n",
        "        print(f\"{sub_indent}... and {len(files) - 15} more files\")\n",
        "\n",
        "# Also check for specific files\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Searching for key files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "key_patterns = ['units', 'bpe', 'corpus', 'json', 'model']\n",
        "found = []\n",
        "\n",
        "for root, dirs, files in os.walk(PROJECT_ROOT):\n",
        "    for file in files:\n",
        "        for pattern in key_patterns:\n",
        "            if pattern in file.lower():\n",
        "                found.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"\\nFound {len(found)} relevant files:\")\n",
        "for f in found[:30]:\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O1YomEa3Swm",
        "outputId": "f22190ac-7a3a-44ad-d27d-d1f2829a7e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full directory tree:\n",
            "============================================================\n",
            "📁 satere_project/\n",
            "  📁 soundscript_full/\n",
            "\n",
            "============================================================\n",
            "Searching for key files...\n",
            "============================================================\n",
            "\n",
            "Found 0 relevant files:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the actual folders in your Drive\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "folders_to_check = [\n",
        "    \"satere_units\",\n",
        "    \"satere_motifs\",\n",
        "    \"raw_audio\",\n",
        "    \"phase2_output\",\n",
        "    \"converted_audio\"\n",
        "]\n",
        "\n",
        "print(\"Checking your folders:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for folder in folders_to_check:\n",
        "    path = os.path.join(PROJECT_ROOT, folder)\n",
        "    if os.path.exists(path):\n",
        "        files = os.listdir(path)\n",
        "        print(f\"\\n✓ {folder}/\")\n",
        "        print(f\"  Total items: {len(files)}\")\n",
        "\n",
        "        # Show first few files\n",
        "        for f in files[:5]:\n",
        "            filepath = os.path.join(path, f)\n",
        "            if os.path.isfile(filepath):\n",
        "                size = os.path.getsize(filepath) / 1024  # KB\n",
        "                print(f\"    {f} ({size:.1f} KB)\")\n",
        "            else:\n",
        "                print(f\"    📁 {f}/\")\n",
        "\n",
        "        if len(files) > 5:\n",
        "            print(f\"    ... and {len(files) - 5} more\")\n",
        "    else:\n",
        "        print(f\"\\n✗ {folder}/ - not found\")\n",
        "\n",
        "# Specifically check for the BPE training file\n",
        "bpe_file = f\"{PROJECT_ROOT}/satere_units/all_units_for_bpe.txt\"\n",
        "if os.path.exists(bpe_file):\n",
        "    size_mb = os.path.getsize(bpe_file) / (1024 * 1024)\n",
        "    print(f\"\\n✓ Found BPE training file: {size_mb:.1f} MB\")\n",
        "else:\n",
        "    print(f\"\\n✗ BPE training file not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoU_SHjP3sMx",
        "outputId": "3114eaf5-ff43-499c-e4d1-a8d041ddbf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking your folders:\n",
            "============================================================\n",
            "\n",
            "✗ satere_units/ - not found\n",
            "\n",
            "✗ satere_motifs/ - not found\n",
            "\n",
            "✗ raw_audio/ - not found\n",
            "\n",
            "✗ phase2_output/ - not found\n",
            "\n",
            "✗ converted_audio/ - not found\n",
            "\n",
            "✗ BPE training file not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Force remount\n",
        "drive.flush_and_unmount()\n",
        "print(\"Unmounted. Now remounting...\")\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Mounted!\")\n",
        "\n",
        "# Now check again\n",
        "import os\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "print(\"\\nContents of satere_project:\")\n",
        "print(\"-\" * 50)\n",
        "for item in os.listdir(PROJECT_ROOT):\n",
        "    print(f\"  {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "oAR0oFjg4cva",
        "outputId": "d5db4924-0318-43ad-bbd1-790da683bec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Unmounted. Now remounting...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3578328574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unmounted. Now remounting...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up and remount\n",
        "import os\n",
        "\n",
        "# Remove the stale mount point\n",
        "!rm -rf /content/drive\n",
        "!mkdir -p /content/drive\n",
        "\n",
        "# Now mount fresh\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\\nChecking satere_project:\")\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "for item in os.listdir(PROJECT_ROOT):\n",
        "    print(f\"  {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRmoNU9D4qOu",
        "outputId": "e585e10e-f21c-4516-fe78-6f2b62d6cb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Checking satere_project:\n",
            "  raw_audio\n",
            "  converted_audio\n",
            "  satere_units\n",
            "  satere_motifs\n",
            "  acts12.mp3\n",
            "  phase2_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the key files\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "# Check satere_units folder\n",
        "units_path = f\"{PROJECT_ROOT}/satere_units\"\n",
        "files = os.listdir(units_path)\n",
        "print(f\"satere_units/ has {len(files)} files\")\n",
        "\n",
        "# Check for the BPE training file\n",
        "bpe_file = f\"{units_path}/all_units_for_bpe.txt\"\n",
        "if os.path.exists(bpe_file):\n",
        "    size_mb = os.path.getsize(bpe_file) / (1024 * 1024)\n",
        "    with open(bpe_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"\\n✓ BPE training file found!\")\n",
        "    print(f\"  Size: {size_mb:.1f} MB\")\n",
        "    print(f\"  Files (lines): {len(lines)}\")\n",
        "\n",
        "    # Count total units\n",
        "    total_units = sum(len(line.strip().split()) for line in lines)\n",
        "    print(f\"  Total acoustic units: {total_units:,}\")\n",
        "else:\n",
        "    print(\"\\n✗ BPE training file not found\")\n",
        "\n",
        "# Also check what's in satere_motifs (from yesterday's small test)\n",
        "motifs_path = f\"{PROJECT_ROOT}/satere_motifs\"\n",
        "if os.path.exists(motifs_path):\n",
        "    print(f\"\\nsatere_motifs/ contents:\")\n",
        "    for f in os.listdir(motifs_path):\n",
        "        print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMd0x6Na48P2",
        "outputId": "1030846c-0b66-439b-c6b3-dbaf4a3c7249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "satere_units/ has 232 files\n",
            "\n",
            "✓ BPE training file found!\n",
            "  Size: 22.4 MB\n",
            "  Files (lines): 229\n",
            "  Total acoustic units: 8,078,315\n",
            "\n",
            "satere_motifs/ contents:\n",
            "  satere_bpe.model\n",
            "  satere_bpe.vocab\n",
            "  motif_index.json\n",
            "  motif_statistics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SOUNDSCRIPT GENERATION: Full 45-hour corpus\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# The BPE training file\n",
        "bpe_file = f\"{UNITS_DIR}/all_units_for_bpe.txt\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SOUNDSCRIPT GENERATION - Full Sateré-Mawé NT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training data: 8,078,315 acoustic units from 229 files\")\n",
        "\n",
        "# Train BPE with larger vocabulary for the full corpus\n",
        "print(\"\\nTraining BPE model (10-30 minutes)...\")\n",
        "\n",
        "VOCAB_SIZE = 500  # Larger vocabulary for full corpus\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=bpe_file,\n",
        "    model_prefix=f\"{OUTPUT_DIR}/soundscript_satere\",\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    num_threads=4,\n",
        "    max_sentence_length=500000,\n",
        "    train_extremely_large_corpus=True\n",
        ")\n",
        "\n",
        "print(\"✓ BPE model trained!\")\n",
        "\n",
        "# Load the model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Vocabulary size: {sp.get_piece_size()}\")\n",
        "\n",
        "# Analyze the motifs\n",
        "print(\"\\nAnalyzing discovered motifs...\")\n",
        "\n",
        "with open(bpe_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "all_encoded = []\n",
        "for line in lines:\n",
        "    encoded = sp.encode_as_pieces(line.strip())\n",
        "    all_encoded.extend(encoded)\n",
        "\n",
        "motif_counts = Counter(all_encoded)\n",
        "\n",
        "print(f\"  Unique motifs: {len(motif_counts)}\")\n",
        "print(f\"  Total tokens: {len(all_encoded):,}\")\n",
        "\n",
        "# Show top 40 motifs\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOP 40 SOUNDSCRIPT MOTIFS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, (motif, count) in enumerate(motif_counts.most_common(40)):\n",
        "    print(f\"  {i+1:3d}. {motif:35s} count={count:,}\")\n",
        "\n",
        "# Save inventory\n",
        "inventory = {\n",
        "    \"language\": \"Sateré-Mawé\",\n",
        "    \"corpus_files\": 229,\n",
        "    \"total_units\": 8078315,\n",
        "    \"vocab_size\": sp.get_piece_size(),\n",
        "    \"motifs\": [\n",
        "        {\"id\": i, \"symbol\": motif, \"count\": count}\n",
        "        for i, (motif, count) in enumerate(motif_counts.most_common())\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/soundscript_inventory.json\", \"w\") as f:\n",
        "    json.dump(inventory, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ SOUNDSCRIPT COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Model: {OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Inventory: {OUTPUT_DIR}/soundscript_inventory.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "JpkUdLG95Kxd",
        "outputId": "03f30caf-dde0-447e-cbf6-f0f956591f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SOUNDSCRIPT GENERATION - Full Sateré-Mawé NT\n",
            "============================================================\n",
            "Training data: 8,078,315 acoustic units from 229 files\n",
            "\n",
            "Training BPE model (10-30 minutes)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (500). Please set it to a value <= 204.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2506663879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m  \u001b[0;31m# Larger vocabulary for full corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m spm.SentencePieceTrainer.train(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpe_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{OUTPUT_DIR}/soundscript_satere\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_LogStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mostream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (500). Please set it to a value <= 204."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SOUNDSCRIPT GENERATION: Full 45-hour corpus\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "bpe_file = f\"{UNITS_DIR}/all_units_for_bpe.txt\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SOUNDSCRIPT GENERATION - Full Sateré-Mawé NT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use maximum allowed vocabulary\n",
        "VOCAB_SIZE = 200\n",
        "\n",
        "print(f\"Training BPE with vocab_size={VOCAB_SIZE}...\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=bpe_file,\n",
        "    model_prefix=f\"{OUTPUT_DIR}/soundscript_satere\",\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    num_threads=4,\n",
        "    max_sentence_length=500000,\n",
        "    train_extremely_large_corpus=True\n",
        ")\n",
        "\n",
        "print(\"✓ BPE model trained!\")\n",
        "\n",
        "# Load the model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Vocabulary size: {sp.get_piece_size()}\")\n",
        "\n",
        "# Analyze the motifs\n",
        "print(\"\\nAnalyzing discovered motifs...\")\n",
        "\n",
        "with open(bpe_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "all_encoded = []\n",
        "for line in lines:\n",
        "    encoded = sp.encode_as_pieces(line.strip())\n",
        "    all_encoded.extend(encoded)\n",
        "\n",
        "motif_counts = Counter(all_encoded)\n",
        "\n",
        "print(f\"  Unique motifs: {len(motif_counts)}\")\n",
        "print(f\"  Total tokens: {len(all_encoded):,}\")\n",
        "\n",
        "# Show top 40 motifs\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOP 40 SOUNDSCRIPT MOTIFS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, (motif, count) in enumerate(motif_counts.most_common(40)):\n",
        "    print(f\"  {i+1:3d}. {motif:35s} count={count:,}\")\n",
        "\n",
        "# Save inventory\n",
        "inventory = {\n",
        "    \"language\": \"Sateré-Mawé\",\n",
        "    \"corpus_files\": 229,\n",
        "    \"total_units\": 8078315,\n",
        "    \"vocab_size\": sp.get_piece_size(),\n",
        "    \"motifs\": [\n",
        "        {\"id\": i, \"symbol\": motif, \"count\": count}\n",
        "        for i, (motif, count) in enumerate(motif_counts.most_common())\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/soundscript_inventory.json\", \"w\") as f:\n",
        "    json.dump(inventory, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ SOUNDSCRIPT COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Model: {OUTPUT_DIR}/soundscript_satere.model\")\n",
        "print(f\"  Inventory: {OUTPUT_DIR}/soundscript_inventory.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuG3Z2o-5ZGf",
        "outputId": "cf0833ad-f408-4c8b-ee3f-5d60dc100cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SOUNDSCRIPT GENERATION - Full Sateré-Mawé NT\n",
            "============================================================\n",
            "Training BPE with vocab_size=200...\n",
            "✓ BPE model trained!\n",
            "  Vocabulary size: 200\n",
            "\n",
            "Analyzing discovered motifs...\n",
            "  Unique motifs: 100\n",
            "  Total tokens: 8,078,315\n",
            "\n",
            "============================================================\n",
            "TOP 40 SOUNDSCRIPT MOTIFS\n",
            "============================================================\n",
            "    1. ▁29                                 count=266,472\n",
            "    2. ▁82                                 count=217,421\n",
            "    3. ▁26                                 count=179,759\n",
            "    4. ▁81                                 count=161,271\n",
            "    5. ▁85                                 count=158,889\n",
            "    6. ▁55                                 count=153,712\n",
            "    7. ▁18                                 count=148,241\n",
            "    8. ▁62                                 count=138,527\n",
            "    9. ▁44                                 count=131,139\n",
            "   10. ▁23                                 count=130,346\n",
            "   11. ▁19                                 count=125,666\n",
            "   12. ▁16                                 count=122,186\n",
            "   13. ▁1                                  count=119,696\n",
            "   14. ▁78                                 count=118,846\n",
            "   15. ▁39                                 count=116,044\n",
            "   16. ▁7                                  count=113,989\n",
            "   17. ▁31                                 count=110,900\n",
            "   18. ▁10                                 count=110,243\n",
            "   19. ▁84                                 count=108,609\n",
            "   20. ▁20                                 count=107,791\n",
            "   21. ▁22                                 count=107,147\n",
            "   22. ▁5                                  count=106,500\n",
            "   23. ▁15                                 count=100,602\n",
            "   24. ▁67                                 count=98,954\n",
            "   25. ▁52                                 count=98,135\n",
            "   26. ▁76                                 count=97,286\n",
            "   27. ▁4                                  count=94,742\n",
            "   28. ▁99                                 count=93,872\n",
            "   29. ▁27                                 count=92,758\n",
            "   30. ▁56                                 count=90,033\n",
            "   31. ▁46                                 count=88,857\n",
            "   32. ▁92                                 count=87,821\n",
            "   33. ▁66                                 count=86,570\n",
            "   34. ▁57                                 count=86,036\n",
            "   35. ▁8                                  count=85,965\n",
            "   36. ▁2                                  count=85,535\n",
            "   37. ▁32                                 count=84,635\n",
            "   38. ▁93                                 count=84,411\n",
            "   39. ▁53                                 count=84,202\n",
            "   40. ▁72                                 count=83,793\n",
            "\n",
            "============================================================\n",
            "✓ SOUNDSCRIPT COMPLETE\n",
            "============================================================\n",
            "  Model: /content/drive/MyDrive/satere_project/soundscript_full/soundscript_satere.model\n",
            "  Inventory: /content/drive/MyDrive/satere_project/soundscript_full/soundscript_inventory.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXTRACT AUDIO SAMPLES FOR EACH MOTIF\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "CONVERTED_DIR = f\"{PROJECT_ROOT}/converted_audio\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "SAMPLES_DIR = f\"{OUTPUT_DIR}/motif_samples\"\n",
        "\n",
        "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING AUDIO SAMPLES FOR MOTIFS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load one of the unit files to understand the format\n",
        "unit_files = [f for f in os.listdir(UNITS_DIR) if f.endswith('.units.txt')]\n",
        "print(f\"Found {len(unit_files)} unit files\")\n",
        "\n",
        "# Get list of converted audio files\n",
        "wav_files = [f for f in os.listdir(CONVERTED_DIR) if f.endswith('.wav')]\n",
        "print(f\"Found {len(wav_files)} WAV files\")\n",
        "\n",
        "# For each of the 100 motifs, find occurrences and extract samples\n",
        "SAMPLES_PER_MOTIF = 3\n",
        "motif_samples = {}\n",
        "\n",
        "# The motifs are just the unit numbers (0-99)\n",
        "motifs_to_sample = list(range(100))\n",
        "\n",
        "print(f\"\\nExtracting {SAMPLES_PER_MOTIF} samples for each of {len(motifs_to_sample)} motifs...\")\n",
        "\n",
        "for motif_id in tqdm(motifs_to_sample, desc=\"Processing motifs\"):\n",
        "    motif_samples[motif_id] = []\n",
        "\n",
        "    # Find files that contain this unit\n",
        "    occurrences = []\n",
        "\n",
        "    for unit_file in unit_files[:50]:  # Sample from first 50 files for speed\n",
        "        unit_path = os.path.join(UNITS_DIR, unit_file)\n",
        "        with open(unit_path, 'r') as f:\n",
        "            units = f.read().strip().split()\n",
        "\n",
        "        # Find positions of this motif\n",
        "        for pos, unit in enumerate(units):\n",
        "            if unit == str(motif_id):\n",
        "                # Calculate timestamp (20ms per frame)\n",
        "                start_time = pos * 0.02\n",
        "                end_time = start_time + 0.1  # 100ms window\n",
        "\n",
        "                wav_name = unit_file.replace('.units.txt', '.wav')\n",
        "                if wav_name in wav_files:\n",
        "                    occurrences.append({\n",
        "                        \"file\": wav_name,\n",
        "                        \"start\": start_time,\n",
        "                        \"end\": end_time,\n",
        "                        \"position\": pos\n",
        "                    })\n",
        "\n",
        "    # Sample up to SAMPLES_PER_MOTIF occurrences\n",
        "    if len(occurrences) >= SAMPLES_PER_MOTIF:\n",
        "        samples = random.sample(occurrences, SAMPLES_PER_MOTIF)\n",
        "    elif len(occurrences) > 0:\n",
        "        samples = occurrences\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    # Extract audio for each sample\n",
        "    for sample_idx, sample in enumerate(samples):\n",
        "        wav_path = os.path.join(CONVERTED_DIR, sample[\"file\"])\n",
        "\n",
        "        try:\n",
        "            sr, audio = wav.read(wav_path)\n",
        "            start_frame = int(sample[\"start\"] * sr)\n",
        "            end_frame = int(sample[\"end\"] * sr)\n",
        "\n",
        "            # Add buffer\n",
        "            start_frame = max(0, start_frame - int(0.05 * sr))\n",
        "            end_frame = min(len(audio), end_frame + int(0.05 * sr))\n",
        "\n",
        "            segment = audio[start_frame:end_frame]\n",
        "\n",
        "            if len(segment) > 0:\n",
        "                out_file = f\"{SAMPLES_DIR}/motif_{motif_id:02d}_sample_{sample_idx}.wav\"\n",
        "                wav.write(out_file, sr, segment)\n",
        "                motif_samples[motif_id].append(os.path.basename(out_file))\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "# Save metadata\n",
        "samples_meta = {\n",
        "    \"language\": \"Sateré-Mawé\",\n",
        "    \"total_motifs\": len(motifs_to_sample),\n",
        "    \"samples_per_motif\": SAMPLES_PER_MOTIF,\n",
        "    \"motifs\": [\n",
        "        {\"id\": mid, \"samples\": samples}\n",
        "        for mid, samples in motif_samples.items()\n",
        "        if samples\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/motif_samples_meta.json\", \"w\") as f:\n",
        "    json.dump(samples_meta, f, indent=2)\n",
        "\n",
        "# Count successful extractions\n",
        "total_samples = sum(len(s) for s in motif_samples.values())\n",
        "motifs_with_samples = len([m for m in motif_samples.values() if m])\n",
        "\n",
        "print(f\"\\n✓ EXTRACTION COMPLETE\")\n",
        "print(f\"  Motifs with samples: {motifs_with_samples}\")\n",
        "print(f\"  Total audio samples: {total_samples}\")\n",
        "print(f\"  Saved to: {SAMPLES_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVEGftVB6YKM",
        "outputId": "b4d084a5-dafa-44b0-eb41-ddae1b77f6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXTRACTING AUDIO SAMPLES FOR MOTIFS\n",
            "============================================================\n",
            "Found 229 unit files\n",
            "Found 260 WAV files\n",
            "\n",
            "Extracting 3 samples for each of 100 motifs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing motifs: 100%|██████████| 100/100 [02:22<00:00,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ EXTRACTION COMPLETE\n",
            "  Motifs with samples: 100\n",
            "  Total audio samples: 300\n",
            "  Saved to: /content/drive/MyDrive/satere_project/soundscript_full/motif_samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CREATE SOUNDSCRIPT MAPPER HTML FOR YOUR MOTIFS\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "\n",
        "# Load the motif metadata\n",
        "with open(f\"{OUTPUT_DIR}/motif_samples_meta.json\", \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "# Load the inventory for counts\n",
        "with open(f\"{OUTPUT_DIR}/soundscript_inventory.json\", \"r\") as f:\n",
        "    inventory = json.load(f)\n",
        "\n",
        "# Build motif data with counts\n",
        "motif_counts = {m[\"symbol\"].replace(\"▁\", \"\"): m[\"count\"] for m in inventory[\"motifs\"]}\n",
        "\n",
        "# Create JavaScript data\n",
        "js_data = []\n",
        "for motif in meta[\"motifs\"]:\n",
        "    motif_id = motif[\"id\"]\n",
        "    count = motif_counts.get(str(motif_id), 0)\n",
        "    js_data.append({\n",
        "        \"id\": motif_id,\n",
        "        \"symbol\": f\"M{motif_id:02d}\",\n",
        "        \"count\": count,\n",
        "        \"samples\": motif[\"samples\"]\n",
        "    })\n",
        "\n",
        "# Sort by count (most frequent first)\n",
        "js_data.sort(key=lambda x: x[\"count\"], reverse=True)\n",
        "\n",
        "print(f\"Prepared data for {len(js_data)} motifs\")\n",
        "print(f\"Total samples: {sum(len(m['samples']) for m in js_data)}\")\n",
        "\n",
        "# Save as JavaScript file for the mapper\n",
        "js_content = f\"const MOTIF_DATA = {json.dumps(js_data, indent=2)};\"\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/soundscript_data.js\", \"w\") as f:\n",
        "    f.write(js_content)\n",
        "\n",
        "print(f\"\\n✓ Saved: {OUTPUT_DIR}/soundscript_data.js\")\n",
        "\n",
        "# Show top 10 most frequent\n",
        "print(\"\\nTop 10 most frequent motifs:\")\n",
        "for m in js_data[:10]:\n",
        "    print(f\"  {m['symbol']}: {m['count']:,} occurrences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2figkVS6cB5",
        "outputId": "eba9af46-ce6f-4f42-a7ac-7d6cb3085e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared data for 100 motifs\n",
            "Total samples: 300\n",
            "\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/soundscript_full/soundscript_data.js\n",
            "\n",
            "Top 10 most frequent motifs:\n",
            "  M29: 266,472 occurrences\n",
            "  M82: 217,421 occurrences\n",
            "  M26: 179,759 occurrences\n",
            "  M81: 161,271 occurrences\n",
            "  M85: 158,889 occurrences\n",
            "  M55: 153,712 occurrences\n",
            "  M18: 148,241 occurrences\n",
            "  M62: 138,527 occurrences\n",
            "  M44: 131,139 occurrences\n",
            "  M23: 130,346 occurrences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CREATE DOWNLOADABLE ZIP FOR GITHUB DEPLOYMENT\n",
        "# =============================================================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "DEPLOY_DIR = f\"{PROJECT_ROOT}/soundscript_deploy\"\n",
        "\n",
        "# Create deployment folder\n",
        "os.makedirs(DEPLOY_DIR, exist_ok=True)\n",
        "\n",
        "# Copy motif samples\n",
        "samples_src = f\"{OUTPUT_DIR}/motif_samples\"\n",
        "samples_dst = f\"{DEPLOY_DIR}/motif_samples\"\n",
        "\n",
        "if os.path.exists(samples_dst):\n",
        "    shutil.rmtree(samples_dst)\n",
        "shutil.copytree(samples_src, samples_dst)\n",
        "\n",
        "# Copy data file\n",
        "shutil.copy(f\"{OUTPUT_DIR}/soundscript_data.js\", DEPLOY_DIR)\n",
        "shutil.copy(f\"{OUTPUT_DIR}/soundscript_inventory.json\", DEPLOY_DIR)\n",
        "\n",
        "# Count files\n",
        "sample_count = len(os.listdir(samples_dst))\n",
        "\n",
        "print(f\"✓ Deployment folder ready: {DEPLOY_DIR}\")\n",
        "print(f\"  - soundscript_data.js\")\n",
        "print(f\"  - soundscript_inventory.json\")\n",
        "print(f\"  - motif_samples/ ({sample_count} files)\")\n",
        "\n",
        "# Create zip\n",
        "zip_path = f\"{PROJECT_ROOT}/soundscript_deploy.zip\"\n",
        "shutil.make_archive(zip_path.replace('.zip', ''), 'zip', DEPLOY_DIR)\n",
        "\n",
        "print(f\"\\n✓ ZIP created: {zip_path}\")\n",
        "print(f\"\\nDownload this zip from Google Drive, then:\")\n",
        "print(\"1. Unzip it\")\n",
        "print(\"2. Create a new GitHub repository called 'soundscript-satere'\")\n",
        "print(\"3. Upload all files to the repository\")\n",
        "print(\"4. Enable GitHub Pages in Settings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zCq5Uuy79_P",
        "outputId": "92e65266-eab4-4749-95af-29aa110d8d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Deployment folder ready: /content/drive/MyDrive/satere_project/soundscript_deploy\n",
            "  - soundscript_data.js\n",
            "  - soundscript_inventory.json\n",
            "  - motif_samples/ (300 files)\n",
            "\n",
            "✓ ZIP created: /content/drive/MyDrive/satere_project/soundscript_deploy.zip\n",
            "\n",
            "Download this zip from Google Drive, then:\n",
            "1. Unzip it\n",
            "2. Create a new GitHub repository called 'soundscript-satere'\n",
            "3. Upload all files to the repository\n",
            "4. Enable GitHub Pages in Settings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CREATE THE SOUNDSCRIPT MAPPER HTML\n",
        "# =============================================================================\n",
        "\n",
        "html_content = '''<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>SoundScript Mapper | Sateré-Mawé</title>\n",
        "    <style>\n",
        "        * { margin: 0; padding: 0; box-sizing: border-box; }\n",
        "        body {\n",
        "            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n",
        "            background: #0a0a0b;\n",
        "            color: #f5f5f7;\n",
        "            min-height: 100vh;\n",
        "            line-height: 1.5;\n",
        "        }\n",
        "        .container { max-width: 1200px; margin: 0 auto; padding: 2rem; }\n",
        "        header { margin-bottom: 2rem; padding-bottom: 1.5rem; border-bottom: 1px solid #2a2a2e; }\n",
        "        h1 { font-size: 1.75rem; margin-bottom: 0.25rem; }\n",
        "        .subtitle { color: #a1a1a6; }\n",
        "\n",
        "        .stats { display: flex; gap: 2rem; margin-top: 1rem; flex-wrap: wrap; }\n",
        "        .stat { display: flex; align-items: center; gap: 0.5rem; }\n",
        "        .stat-dot { width: 10px; height: 10px; border-radius: 50%; }\n",
        "        .stat-dot.morpheme { background: #34c759; }\n",
        "        .stat-dot.syllable { background: #5e5ce6; }\n",
        "        .stat-dot.noise { background: #ff6b6b; }\n",
        "        .stat-dot.unclear { background: #ff9f0a; }\n",
        "        .stat-dot.pending { background: #6e6e73; }\n",
        "        .stat-count { font-weight: 600; font-family: monospace; }\n",
        "\n",
        "        .progress-bar { width: 100%; height: 6px; background: #1a1a1e; border-radius: 3px; margin-top: 1rem; }\n",
        "        .progress-fill { height: 100%; background: linear-gradient(90deg, #34c759, #0a84ff); border-radius: 3px; transition: width 0.3s; }\n",
        "\n",
        "        .main-grid { display: grid; grid-template-columns: 280px 1fr; gap: 1.5rem; }\n",
        "        @media (max-width: 800px) { .main-grid { grid-template-columns: 1fr; } }\n",
        "\n",
        "        .motif-list { background: #141416; border-radius: 12px; border: 1px solid #2a2a2e; max-height: 70vh; overflow: hidden; display: flex; flex-direction: column; }\n",
        "        .motif-list-header { padding: 1rem; border-bottom: 1px solid #2a2a2e; font-weight: 600; font-size: 0.875rem; color: #a1a1a6; text-transform: uppercase; }\n",
        "        .motif-list-scroll { overflow-y: auto; flex: 1; }\n",
        "        .motif-item { padding: 0.75rem 1rem; border-bottom: 1px solid #2a2a2e; cursor: pointer; display: flex; align-items: center; gap: 0.75rem; }\n",
        "        .motif-item:hover { background: #1a1a1e; }\n",
        "        .motif-item.active { background: #1a1a1e; border-left: 3px solid #0a84ff; }\n",
        "        .motif-id { font-family: monospace; font-weight: 500; min-width: 45px; }\n",
        "        .motif-status { width: 8px; height: 8px; border-radius: 50%; }\n",
        "        .motif-preview { font-size: 0.875rem; color: #a1a1a6; flex: 1; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }\n",
        "        .motif-preview.has-content { color: #f5f5f7; font-style: italic; }\n",
        "        .motif-count { font-size: 0.75rem; color: #6e6e73; font-family: monospace; }\n",
        "\n",
        "        .editor { background: #141416; border-radius: 12px; border: 1px solid #2a2a2e; }\n",
        "        .editor-header { padding: 1.25rem; border-bottom: 1px solid #2a2a2e; background: #1a1a1e; border-radius: 12px 12px 0 0; }\n",
        "        .editor-title { font-family: monospace; font-size: 1.5rem; font-weight: 600; }\n",
        "        .editor-meta { font-size: 0.875rem; color: #a1a1a6; margin-top: 0.25rem; }\n",
        "        .editor-body { padding: 1.25rem; }\n",
        "\n",
        "        .section-label { font-size: 0.75rem; font-weight: 600; color: #6e6e73; text-transform: uppercase; letter-spacing: 0.05em; margin-bottom: 0.5rem; }\n",
        "        .audio-section { margin-bottom: 1.5rem; }\n",
        "        .audio-samples { display: flex; gap: 0.5rem; flex-wrap: wrap; }\n",
        "        .audio-btn { display: flex; align-items: center; gap: 0.5rem; padding: 0.625rem 1rem; background: #1a1a1e; border: 1px solid #2a2a2e; border-radius: 8px; color: #f5f5f7; font-size: 0.875rem; cursor: pointer; }\n",
        "        .audio-btn:hover { background: #242428; border-color: #0a84ff; }\n",
        "        .audio-btn.playing { background: #0a84ff; border-color: #0a84ff; }\n",
        "\n",
        "        .form-group { margin-bottom: 1.25rem; }\n",
        "        .form-label { display: block; font-size: 0.875rem; font-weight: 500; margin-bottom: 0.375rem; color: #a1a1a6; }\n",
        "        .form-input { width: 100%; padding: 0.75rem; background: #1a1a1e; border: 1px solid #2a2a2e; border-radius: 8px; color: #f5f5f7; font-size: 1rem; }\n",
        "        .form-input:focus { outline: none; border-color: #0a84ff; }\n",
        "        textarea.form-input { min-height: 80px; resize: vertical; }\n",
        "\n",
        "        .category-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 0.5rem; }\n",
        "        .category-btn { display: flex; align-items: center; gap: 0.5rem; padding: 0.75rem; background: #1a1a1e; border: 2px solid #2a2a2e; border-radius: 8px; color: #f5f5f7; font-size: 0.875rem; cursor: pointer; text-align: left; }\n",
        "        .category-btn:hover { background: #242428; }\n",
        "        .category-btn.selected.morpheme { border-color: #34c759; background: rgba(52,199,89,0.1); }\n",
        "        .category-btn.selected.syllable { border-color: #5e5ce6; background: rgba(94,92,230,0.1); }\n",
        "        .category-btn.selected.noise { border-color: #ff6b6b; background: rgba(255,107,107,0.1); }\n",
        "        .category-btn.selected.unclear { border-color: #ff9f0a; background: rgba(255,159,10,0.1); }\n",
        "        .category-icon { width: 24px; height: 24px; border-radius: 6px; display: flex; align-items: center; justify-content: center; font-size: 0.875rem; }\n",
        "        .category-btn.morpheme .category-icon { background: rgba(52,199,89,0.2); }\n",
        "        .category-btn.syllable .category-icon { background: rgba(94,92,230,0.2); }\n",
        "        .category-btn.noise .category-icon { background: rgba(255,107,107,0.2); }\n",
        "        .category-btn.unclear .category-icon { background: rgba(255,159,10,0.2); }\n",
        "\n",
        "        .action-bar { display: flex; gap: 0.75rem; margin-top: 1.5rem; padding-top: 1rem; border-top: 1px solid #2a2a2e; flex-wrap: wrap; }\n",
        "        .btn { padding: 0.75rem 1.25rem; border-radius: 8px; font-size: 0.875rem; font-weight: 600; cursor: pointer; border: none; }\n",
        "        .btn-primary { background: #0a84ff; color: white; }\n",
        "        .btn-primary:hover { background: #0077ed; }\n",
        "        .btn-secondary { background: #1a1a1e; color: #f5f5f7; border: 1px solid #2a2a2e; }\n",
        "        .btn-secondary:hover { background: #242428; }\n",
        "\n",
        "        .export-section { margin-top: 1.5rem; padding: 1.25rem; background: #141416; border-radius: 12px; border: 1px solid #2a2a2e; }\n",
        "        .export-title { font-weight: 600; margin-bottom: 0.75rem; }\n",
        "        .export-buttons { display: flex; gap: 0.5rem; flex-wrap: wrap; }\n",
        "\n",
        "        .keyboard-hint { font-size: 0.75rem; color: #6e6e73; margin-top: 0.5rem; }\n",
        "        kbd { padding: 0.125rem 0.375rem; background: #1a1a1e; border: 1px solid #2a2a2e; border-radius: 4px; font-family: monospace; font-size: 0.6875rem; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <header>\n",
        "            <h1>🔊 SoundScript Mapper</h1>\n",
        "            <div class=\"subtitle\">Sateré-Mawé · 100 motifs from 45 hours of audio</div>\n",
        "            <div class=\"stats\">\n",
        "                <div class=\"stat\"><div class=\"stat-dot morpheme\"></div><span>Morphemes</span><span class=\"stat-count\" id=\"statMorpheme\">0</span></div>\n",
        "                <div class=\"stat\"><div class=\"stat-dot syllable\"></div><span>Syllables</span><span class=\"stat-count\" id=\"statSyllable\">0</span></div>\n",
        "                <div class=\"stat\"><div class=\"stat-dot noise\"></div><span>Noise</span><span class=\"stat-count\" id=\"statNoise\">0</span></div>\n",
        "                <div class=\"stat\"><div class=\"stat-dot unclear\"></div><span>Unclear</span><span class=\"stat-count\" id=\"statUnclear\">0</span></div>\n",
        "                <div class=\"stat\"><div class=\"stat-dot pending\"></div><span>Pending</span><span class=\"stat-count\" id=\"statPending\">0</span></div>\n",
        "            </div>\n",
        "            <div class=\"progress-bar\"><div class=\"progress-fill\" id=\"progressFill\"></div></div>\n",
        "        </header>\n",
        "\n",
        "        <div class=\"main-grid\">\n",
        "            <div class=\"motif-list\">\n",
        "                <div class=\"motif-list-header\">Motifs (by frequency)</div>\n",
        "                <div class=\"motif-list-scroll\" id=\"motifList\"></div>\n",
        "            </div>\n",
        "            <div class=\"editor\">\n",
        "                <div class=\"editor-header\">\n",
        "                    <div class=\"editor-title\" id=\"editorTitle\">Select a motif</div>\n",
        "                    <div class=\"editor-meta\" id=\"editorMeta\">Click a motif from the list to begin</div>\n",
        "                </div>\n",
        "                <div class=\"editor-body\" id=\"editorBody\">\n",
        "                    <p style=\"color:#6e6e73;text-align:center;padding:3rem;\">👈 Select a motif to start mapping</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"export-section\">\n",
        "            <div class=\"export-title\">Export Mapping Data</div>\n",
        "            <div class=\"export-buttons\">\n",
        "                <button class=\"btn btn-secondary\" onclick=\"exportJson()\">📄 JSON</button>\n",
        "                <button class=\"btn btn-secondary\" onclick=\"exportCsv()\">📊 CSV</button>\n",
        "                <button class=\"btn btn-secondary\" onclick=\"exportSummary()\">📋 Summary</button>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script src=\"soundscript_data.js\"></script>\n",
        "    <script>\n",
        "        let motifs = MOTIF_DATA;\n",
        "        let mappings = JSON.parse(localStorage.getItem('soundscript_mappings_v2') || '{}');\n",
        "        let currentIndex = -1;\n",
        "        let currentAudio = null;\n",
        "\n",
        "        function init() {\n",
        "            renderList();\n",
        "            updateStats();\n",
        "            document.addEventListener('keydown', handleKeys);\n",
        "        }\n",
        "\n",
        "        function renderList() {\n",
        "            const list = document.getElementById('motifList');\n",
        "            list.innerHTML = motifs.map((m, i) => {\n",
        "                const map = mappings[m.id] || {};\n",
        "                const status = map.category || 'pending';\n",
        "                const preview = map.transcription || (map.category === 'noise' ? '[noise]' : '');\n",
        "                return \\`<div class=\"motif-item \\${i === currentIndex ? 'active' : ''}\" onclick=\"selectMotif(\\${i})\">\n",
        "                    <div class=\"motif-status stat-dot \\${status}\"></div>\n",
        "                    <div class=\"motif-id\">\\${m.symbol}</div>\n",
        "                    <div class=\"motif-preview \\${preview ? 'has-content' : ''}\">\\${preview || 'unmapped'}</div>\n",
        "                    <div class=\"motif-count\">\\${(m.count/1000).toFixed(0)}k</div>\n",
        "                </div>\\`;\n",
        "            }).join('');\n",
        "        }\n",
        "\n",
        "        function selectMotif(i) {\n",
        "            currentIndex = i;\n",
        "            const m = motifs[i];\n",
        "            const map = mappings[m.id] || {};\n",
        "\n",
        "            document.getElementById('editorTitle').textContent = m.symbol;\n",
        "            document.getElementById('editorMeta').textContent = \\`\\${m.count.toLocaleString()} occurrences · \\${m.samples.length} samples\\`;\n",
        "\n",
        "            document.getElementById('editorBody').innerHTML = \\`\n",
        "                <div class=\"audio-section\">\n",
        "                    <div class=\"section-label\">Audio Samples</div>\n",
        "                    <div class=\"audio-samples\">\n",
        "                        \\${m.samples.map((s, j) => \\`<button class=\"audio-btn\" id=\"audio\\${j}\" onclick=\"playAudio('\\${s}', \\${j})\">▶ Sample \\${j+1}</button>\\`).join('')}\n",
        "                    </div>\n",
        "                </div>\n",
        "                <div class=\"form-group\">\n",
        "                    <label class=\"form-label\">What do you hear? (Sateré orthography)</label>\n",
        "                    <input type=\"text\" class=\"form-input\" id=\"transcription\" placeholder=\"e.g., pe, -iatu, hin\" value=\"\\${map.transcription || ''}\" onchange=\"saveField('transcription', this.value)\">\n",
        "                </div>\n",
        "                <div class=\"form-group\">\n",
        "                    <div class=\"section-label\">Category</div>\n",
        "                    <div class=\"category-grid\">\n",
        "                        <button class=\"category-btn morpheme \\${map.category==='morpheme'?'selected':''}\" onclick=\"saveField('category','morpheme')\">\n",
        "                            <div class=\"category-icon\">✓</div><div>Morpheme</div>\n",
        "                        </button>\n",
        "                        <button class=\"category-btn syllable \\${map.category==='syllable'?'selected':''}\" onclick=\"saveField('category','syllable')\">\n",
        "                            <div class=\"category-icon\">◐</div><div>Syllable</div>\n",
        "                        </button>\n",
        "                        <button class=\"category-btn noise \\${map.category==='noise'?'selected':''}\" onclick=\"saveField('category','noise')\">\n",
        "                            <div class=\"category-icon\">✗</div><div>Noise/Silence</div>\n",
        "                        </button>\n",
        "                        <button class=\"category-btn unclear \\${map.category==='unclear'?'selected':''}\" onclick=\"saveField('category','unclear')\">\n",
        "                            <div class=\"category-icon\">?</div><div>Unclear</div>\n",
        "                        </button>\n",
        "                    </div>\n",
        "                    <div class=\"keyboard-hint\"><kbd>1</kbd> Morpheme <kbd>2</kbd> Syllable <kbd>3</kbd> Noise <kbd>4</kbd> Unclear</div>\n",
        "                </div>\n",
        "                <div class=\"form-group\">\n",
        "                    <label class=\"form-label\">Gloss / Meaning</label>\n",
        "                    <input type=\"text\" class=\"form-input\" id=\"gloss\" placeholder=\"e.g., locative 'in, on, at'\" value=\"\\${map.gloss || ''}\" onchange=\"saveField('gloss', this.value)\">\n",
        "                </div>\n",
        "                <div class=\"form-group\">\n",
        "                    <label class=\"form-label\">Notes</label>\n",
        "                    <textarea class=\"form-input\" id=\"notes\" placeholder=\"Any observations...\" onchange=\"saveField('notes', this.value)\">\\${map.notes || ''}</textarea>\n",
        "                </div>\n",
        "                <div class=\"action-bar\">\n",
        "                    <button class=\"btn btn-primary\" onclick=\"saveAndNext()\">Save & Next →</button>\n",
        "                    <button class=\"btn btn-secondary\" onclick=\"navigate(-1)\" \\${i===0?'disabled':''}>← Prev</button>\n",
        "                    <button class=\"btn btn-secondary\" onclick=\"navigate(1)\" \\${i===motifs.length-1?'disabled':''}>Next →</button>\n",
        "                </div>\n",
        "                <div class=\"keyboard-hint\" style=\"text-align:center;margin-top:0.5rem;\"><kbd>←</kbd><kbd>→</kbd> Navigate <kbd>Space</kbd> Play <kbd>Enter</kbd> Save & Next</div>\n",
        "            \\`;\n",
        "            renderList();\n",
        "        }\n",
        "\n",
        "        function playAudio(file, idx) {\n",
        "            if (currentAudio) { currentAudio.pause(); document.querySelectorAll('.audio-btn').forEach(b => b.classList.remove('playing')); }\n",
        "            currentAudio = new Audio('motif_samples/' + file);\n",
        "            document.getElementById('audio' + idx).classList.add('playing');\n",
        "            currentAudio.onended = () => document.getElementById('audio' + idx).classList.remove('playing');\n",
        "            currentAudio.play();\n",
        "        }\n",
        "\n",
        "        function saveField(field, value) {\n",
        "            const m = motifs[currentIndex];\n",
        "            if (!mappings[m.id]) mappings[m.id] = {};\n",
        "            mappings[m.id][field] = value;\n",
        "            localStorage.setItem('soundscript_mappings_v2', JSON.stringify(mappings));\n",
        "            updateStats();\n",
        "            if (field === 'category') selectMotif(currentIndex);\n",
        "            else renderList();\n",
        "        }\n",
        "\n",
        "        function saveAndNext() {\n",
        "            if (currentIndex < motifs.length - 1) selectMotif(currentIndex + 1);\n",
        "        }\n",
        "\n",
        "        function navigate(dir) {\n",
        "            const next = currentIndex + dir;\n",
        "            if (next >= 0 && next < motifs.length) selectMotif(next);\n",
        "        }\n",
        "\n",
        "        function handleKeys(e) {\n",
        "            if (currentIndex < 0) return;\n",
        "            if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') {\n",
        "                if (e.key === 'Enter') { e.preventDefault(); saveAndNext(); }\n",
        "                return;\n",
        "            }\n",
        "            if (e.key === 'ArrowLeft') navigate(-1);\n",
        "            if (e.key === 'ArrowRight') navigate(1);\n",
        "            if (e.key === ' ') { e.preventDefault(); const m = motifs[currentIndex]; if (m.samples[0]) playAudio(m.samples[0], 0); }\n",
        "            if (e.key === 'Enter') saveAndNext();\n",
        "            if (e.key === '1') saveField('category', 'morpheme');\n",
        "            if (e.key === '2') saveField('category', 'syllable');\n",
        "            if (e.key === '3') saveField('category', 'noise');\n",
        "            if (e.key === '4') saveField('category', 'unclear');\n",
        "        }\n",
        "\n",
        "        function updateStats() {\n",
        "            const stats = {morpheme:0, syllable:0, noise:0, unclear:0, pending:0};\n",
        "            motifs.forEach(m => {\n",
        "                const cat = mappings[m.id]?.category || 'pending';\n",
        "                stats[cat]++;\n",
        "            });\n",
        "            document.getElementById('statMorpheme').textContent = stats.morpheme;\n",
        "            document.getElementById('statSyllable').textContent = stats.syllable;\n",
        "            document.getElementById('statNoise').textContent = stats.noise;\n",
        "            document.getElementById('statUnclear').textContent = stats.unclear;\n",
        "            document.getElementById('statPending').textContent = stats.pending;\n",
        "            document.getElementById('progressFill').style.width = ((100 - stats.pending) / 100 * 100) + '%';\n",
        "        }\n",
        "\n",
        "        function exportJson() {\n",
        "            const data = { language: 'Sateré-Mawé', exported: new Date().toISOString(), mappings: Object.entries(mappings).map(([id, m]) => ({motifId: parseInt(id), ...m})) };\n",
        "            download(JSON.stringify(data, null, 2), 'soundscript_mapping.json', 'application/json');\n",
        "        }\n",
        "\n",
        "        function exportCsv() {\n",
        "            const rows = ['motif_id,symbol,transcription,category,gloss,notes'];\n",
        "            motifs.forEach(m => {\n",
        "                const map = mappings[m.id] || {};\n",
        "                rows.push([m.id, m.symbol, \\`\"\\${map.transcription||''}\"\\`, map.category||'', \\`\"\\${map.gloss||''}\"\\`, \\`\"\\${(map.notes||'').replace(/\"/g,'\"\"')}\"\\`].join(','));\n",
        "            });\n",
        "            download(rows.join('\\\\n'), 'soundscript_mapping.csv', 'text/csv');\n",
        "        }\n",
        "\n",
        "        function exportSummary() {\n",
        "            const stats = {morpheme:[], syllable:[], noise:[], unclear:[]};\n",
        "            motifs.forEach(m => { const cat = mappings[m.id]?.category; if (cat) stats[cat].push({...m, ...mappings[m.id]}); });\n",
        "            let txt = \\`SOUNDSCRIPT MAPPING SUMMARY\\\\nLanguage: Sateré-Mawé\\\\nDate: \\${new Date().toLocaleDateString()}\\\\n\\\\n\\`;\n",
        "            txt += \\`MORPHEMES (\\${stats.morpheme.length})\\\\n\\${'='.repeat(40)}\\\\n\\${stats.morpheme.map(m => \\`\\${m.symbol}: \\${m.transcription || '?'} — \\${m.gloss || 'no gloss'}\\`).join('\\\\n')||'None'}\\\\n\\\\n\\`;\n",
        "            txt += \\`SYLLABLES (\\${stats.syllable.length})\\\\n\\${'='.repeat(40)}\\\\n\\${stats.syllable.map(m => \\`\\${m.symbol}: \\${m.transcription || '?'}\\`).join('\\\\n')||'None'}\\\\n\\\\n\\`;\n",
        "            txt += \\`NOISE (\\${stats.noise.length})\\\\n\\${'='.repeat(40)}\\\\n\\${stats.noise.map(m => m.symbol).join(', ')||'None'}\\\\n\\`;\n",
        "            download(txt, 'soundscript_summary.txt', 'text/plain');\n",
        "        }\n",
        "\n",
        "        function download(content, filename, type) {\n",
        "            const a = document.createElement('a');\n",
        "            a.href = URL.createObjectURL(new Blob([content], {type}));\n",
        "            a.download = filename;\n",
        "            a.click();\n",
        "        }\n",
        "\n",
        "        init();\n",
        "    </script>\n",
        "</body>\n",
        "</html>'''\n",
        "\n",
        "# Save to deployment folder\n",
        "DEPLOY_DIR = f\"{PROJECT_ROOT}/soundscript_deploy\"\n",
        "with open(f\"{DEPLOY_DIR}/index.html\", \"w\") as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(\"✓ Created index.html in soundscript_deploy/\")\n",
        "print(\"\\nNow update the zip file...\")\n",
        "\n",
        "# Update zip\n",
        "import shutil\n",
        "zip_path = f\"{PROJECT_ROOT}/soundscript_deploy.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "shutil.make_archive(zip_path.replace('.zip', ''), 'zip', DEPLOY_DIR)\n",
        "\n",
        "print(f\"✓ Updated: {zip_path}\")\n",
        "print(\"\\nDownload soundscript_deploy.zip from Google Drive and deploy to GitHub!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGWgbOSx8MvQ",
        "outputId": "bc0637f8-c6a8-4130-bfa3-d8cabd6d1db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:160: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:160: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3253832480.py:160: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  return \\`<div class=\"motif-item \\${i === currentIndex ? 'active' : ''}\" onclick=\"selectMotif(\\${i})\">\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created index.html in soundscript_deploy/\n",
            "\n",
            "Now update the zip file...\n",
            "✓ Updated: /content/drive/MyDrive/satere_project/soundscript_deploy.zip\n",
            "\n",
            "Download soundscript_deploy.zip from Google Drive and deploy to GitHub!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VOCODER SETUP: Convert SoundScript motifs back to audio\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q fairseq speechbrain\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "CONVERTED_DIR = f\"{PROJECT_ROOT}/converted_audio\"\n",
        "OUTPUT_DIR = f\"{PROJECT_ROOT}/soundscript_full\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "\n",
        "os.makedirs(VOCODER_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VOCODER: Concatenative Synthesis\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Building a simple vocoder from your audio corpus...\")\n",
        "\n",
        "# For each of the 100 acoustic units, extract a representative segment\n",
        "# This creates a \"codebook\" of sounds we can concatenate\n",
        "\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook\"\n",
        "os.makedirs(CODEBOOK_DIR, exist_ok=True)\n",
        "\n",
        "# Load unit files to find examples of each unit\n",
        "unit_files = [f for f in os.listdir(UNITS_DIR) if f.endswith('.units.txt')]\n",
        "wav_files = {f.replace('.wav', ''): f for f in os.listdir(CONVERTED_DIR) if f.endswith('.wav')}\n",
        "\n",
        "print(f\"Building codebook from {len(unit_files)} files...\")\n",
        "\n",
        "# For each unit (0-99), collect multiple examples\n",
        "unit_examples = {i: [] for i in range(100)}\n",
        "\n",
        "for unit_file in tqdm(unit_files[:100], desc=\"Scanning for unit examples\"):\n",
        "    base_name = unit_file.replace('.units.txt', '')\n",
        "    if base_name not in wav_files:\n",
        "        continue\n",
        "\n",
        "    # Read units\n",
        "    with open(os.path.join(UNITS_DIR, unit_file), 'r') as f:\n",
        "        units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "    # Find positions of each unit\n",
        "    for pos, unit in enumerate(units):\n",
        "        if len(unit_examples[unit]) < 10:  # Keep up to 10 examples per unit\n",
        "            unit_examples[unit].append({\n",
        "                'file': base_name,\n",
        "                'position': pos,\n",
        "                'context_before': units[max(0,pos-2):pos],\n",
        "                'context_after': units[pos+1:pos+3]\n",
        "            })\n",
        "\n",
        "# Extract audio for each unit's best example\n",
        "print(\"\\nExtracting codebook audio...\")\n",
        "\n",
        "codebook = {}\n",
        "for unit_id in tqdm(range(100), desc=\"Building codebook\"):\n",
        "    examples = unit_examples[unit_id]\n",
        "    if not examples:\n",
        "        continue\n",
        "\n",
        "    # Use the first example\n",
        "    ex = examples[0]\n",
        "    wav_path = os.path.join(CONVERTED_DIR, wav_files[ex['file']])\n",
        "\n",
        "    try:\n",
        "        sr, audio = wav.read(wav_path)\n",
        "\n",
        "        # Each unit is ~20ms, extract with small buffer\n",
        "        start_ms = ex['position'] * 20\n",
        "        end_ms = start_ms + 20\n",
        "\n",
        "        start_sample = int((start_ms / 1000) * sr)\n",
        "        end_sample = int((end_ms / 1000) * sr)\n",
        "\n",
        "        # Add tiny buffer for smoother concatenation\n",
        "        start_sample = max(0, start_sample - int(0.005 * sr))\n",
        "        end_sample = min(len(audio), end_sample + int(0.005 * sr))\n",
        "\n",
        "        segment = audio[start_sample:end_sample]\n",
        "\n",
        "        if len(segment) > 0:\n",
        "            codebook_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "            wav.write(codebook_path, sr, segment)\n",
        "            codebook[unit_id] = {\n",
        "                'path': codebook_path,\n",
        "                'duration_ms': len(segment) / sr * 1000\n",
        "            }\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✓ Codebook built: {len(codebook)} units\")\n",
        "\n",
        "# Save codebook metadata\n",
        "with open(f\"{VOCODER_DIR}/codebook_meta.json\", \"w\") as f:\n",
        "    json.dump(codebook, f, indent=2)\n",
        "\n",
        "print(f\"  Saved to: {VOCODER_DIR}/codebook_meta.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMfCB5Ni8sEt",
        "outputId": "2ff3574f-36c6-43af-dba3-256a8d9d4393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m7.9/9.6 MB\u001b[0m \u001b[31m237.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m240.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.4 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/92/b1/4f3023143436f12c98bab53f0b3db617bd18a7d223627d5030e13a7b4fc2/omegaconf-2.0.4-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.3 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.2 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/72/fe/f8d162aa059fb4f327fd75144dd69aa7e8acbb6d8d37013e4638c8490e0b/omegaconf-2.0.2-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.1 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/86/ec/605805e60abdb025b06664d107335031bb8ebdc52e0a90bdbad6a7130279/omegaconf-2.0.1-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "============================================================\n",
            "VOCODER: Concatenative Synthesis\n",
            "============================================================\n",
            "Building a simple vocoder from your audio corpus...\n",
            "Building codebook from 229 files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning for unit examples: 100%|██████████| 100/100 [00:01<00:00, 54.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting codebook audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building codebook: 100%|██████████| 100/100 [00:01<00:00, 53.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Codebook built: 100 units\n",
            "  Saved to: /content/drive/MyDrive/satere_project/vocoder_output/codebook_meta.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VOCODER: Synthesize audio from unit sequences\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from scipy import signal\n",
        "import json\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook\"\n",
        "\n",
        "# Load codebook\n",
        "with open(f\"{VOCODER_DIR}/codebook_meta.json\", \"r\") as f:\n",
        "    codebook_meta = json.load(f)\n",
        "\n",
        "# Load all codebook audio into memory\n",
        "print(\"Loading codebook into memory...\")\n",
        "codebook_audio = {}\n",
        "sample_rate = None\n",
        "\n",
        "for unit_id in range(100):\n",
        "    unit_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "    if os.path.exists(unit_path):\n",
        "        sr, audio = wav.read(unit_path)\n",
        "        sample_rate = sr\n",
        "        codebook_audio[unit_id] = audio.astype(np.float32)\n",
        "\n",
        "print(f\"✓ Loaded {len(codebook_audio)} units at {sample_rate} Hz\")\n",
        "\n",
        "def synthesize_from_units(unit_sequence, crossfade_ms=5):\n",
        "    \"\"\"\n",
        "    Convert a sequence of unit IDs to audio using concatenative synthesis.\n",
        "\n",
        "    Args:\n",
        "        unit_sequence: list of integers (0-99)\n",
        "        crossfade_ms: crossfade duration for smoothing\n",
        "\n",
        "    Returns:\n",
        "        numpy array of audio samples\n",
        "    \"\"\"\n",
        "    if not unit_sequence:\n",
        "        return np.array([])\n",
        "\n",
        "    crossfade_samples = int(crossfade_ms / 1000 * sample_rate)\n",
        "\n",
        "    # Start with first unit\n",
        "    first_unit = unit_sequence[0]\n",
        "    if first_unit not in codebook_audio:\n",
        "        output = np.zeros(int(0.02 * sample_rate))  # 20ms silence\n",
        "    else:\n",
        "        output = codebook_audio[first_unit].copy()\n",
        "\n",
        "    # Concatenate remaining units with crossfade\n",
        "    for unit_id in unit_sequence[1:]:\n",
        "        if unit_id not in codebook_audio:\n",
        "            # Insert silence for missing units\n",
        "            segment = np.zeros(int(0.02 * sample_rate))\n",
        "        else:\n",
        "            segment = codebook_audio[unit_id].copy()\n",
        "\n",
        "        if len(segment) == 0:\n",
        "            continue\n",
        "\n",
        "        # Apply crossfade if possible\n",
        "        if crossfade_samples > 0 and len(output) > crossfade_samples and len(segment) > crossfade_samples:\n",
        "            # Create fade out for end of output\n",
        "            fade_out = np.linspace(1, 0, crossfade_samples)\n",
        "            # Create fade in for start of segment\n",
        "            fade_in = np.linspace(0, 1, crossfade_samples)\n",
        "\n",
        "            # Apply fades\n",
        "            output[-crossfade_samples:] *= fade_out\n",
        "            segment[:crossfade_samples] *= fade_in\n",
        "\n",
        "            # Overlap-add\n",
        "            output[-crossfade_samples:] += segment[:crossfade_samples]\n",
        "            output = np.concatenate([output, segment[crossfade_samples:]])\n",
        "        else:\n",
        "            output = np.concatenate([output, segment])\n",
        "\n",
        "    # Normalize\n",
        "    max_val = np.max(np.abs(output))\n",
        "    if max_val > 0:\n",
        "        output = output / max_val * 0.9\n",
        "\n",
        "    return output.astype(np.int16 if output.dtype != np.int16 else output.dtype)\n",
        "\n",
        "# Test the synthesizer with a sample from your corpus\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING SYNTHESIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load a test unit sequence\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "test_files = [f for f in os.listdir(UNITS_DIR) if f.endswith('.units.txt')][:1]\n",
        "\n",
        "for test_file in test_files:\n",
        "    with open(os.path.join(UNITS_DIR, test_file), 'r') as f:\n",
        "        units = [int(u) for u in f.read().strip().split()[:500]]  # First 500 units (~10 seconds)\n",
        "\n",
        "    print(f\"\\nSynthesizing from: {test_file}\")\n",
        "    print(f\"  Units: {len(units)} (~{len(units)*20/1000:.1f} seconds)\")\n",
        "\n",
        "    # Synthesize\n",
        "    audio = synthesize_from_units(units)\n",
        "\n",
        "    # Save\n",
        "    output_path = f\"{VOCODER_DIR}/test_synthesis.wav\"\n",
        "\n",
        "    # Convert to int16 if needed\n",
        "    if audio.dtype == np.float32:\n",
        "        audio = (audio * 32767).astype(np.int16)\n",
        "\n",
        "    wav.write(output_path, sample_rate, audio)\n",
        "\n",
        "    print(f\"  ✓ Saved: {output_path}\")\n",
        "    print(f\"  Duration: {len(audio)/sample_rate:.1f} seconds\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ VOCODER READY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nYou can now synthesize audio from any unit sequence!\")\n",
        "print(\"Download 'test_synthesis.wav' from vocoder_output/ to hear the result.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hebx-PIKTCN",
        "outputId": "cc411f53-2bba-412f-d9ec-af88a50c7f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading codebook into memory...\n",
            "✓ Loaded 100 units at 16000 Hz\n",
            "\n",
            "============================================================\n",
            "TESTING SYNTHESIZER\n",
            "============================================================\n",
            "\n",
            "Synthesizing from: MAVWYIN1DA_B01_MAT_001.units.txt\n",
            "  Units: 500 (~10.0 seconds)\n",
            "  ✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/test_synthesis.wav\n",
            "  Duration: 12.2 seconds\n",
            "\n",
            "============================================================\n",
            "✓ VOCODER READY\n",
            "============================================================\n",
            "\n",
            "You can now synthesize audio from any unit sequence!\n",
            "Download 'test_synthesis.wav' from vocoder_output/ to hear the result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check what's in the codebook\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "import numpy as np\n",
        "\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook\"\n",
        "\n",
        "print(\"Checking codebook files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check a few codebook files\n",
        "for unit_id in [0, 29, 82, 55]:  # Including some frequent units\n",
        "    unit_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "    if os.path.exists(unit_path):\n",
        "        sr, audio = wav.read(unit_path)\n",
        "        duration_ms = len(audio) / sr * 1000\n",
        "        max_amplitude = np.max(np.abs(audio))\n",
        "        print(f\"Unit {unit_id:02d}: {duration_ms:.1f}ms, max amplitude: {max_amplitude}, samples: {len(audio)}\")\n",
        "    else:\n",
        "        print(f\"Unit {unit_id:02d}: FILE MISSING\")\n",
        "\n",
        "# Check the test synthesis file\n",
        "test_path = f\"{VOCODER_DIR}/test_synthesis.wav\"\n",
        "if os.path.exists(test_path):\n",
        "    sr, audio = wav.read(test_path)\n",
        "    print(f\"\\nTest synthesis file:\")\n",
        "    print(f\"  Duration: {len(audio)/sr:.1f} seconds\")\n",
        "    print(f\"  Max amplitude: {np.max(np.abs(audio))}\")\n",
        "    print(f\"  Min amplitude: {np.min(audio)}\")\n",
        "    print(f\"  Data type: {audio.dtype}\")\n",
        "    print(f\"  Non-zero samples: {np.count_nonzero(audio)}\")\n",
        "\n",
        "# Check what the original audio looks like\n",
        "CONVERTED_DIR = f\"{PROJECT_ROOT}/converted_audio\"\n",
        "sample_wav = [f for f in os.listdir(CONVERTED_DIR) if f.endswith('.wav')][0]\n",
        "sr, orig_audio = wav.read(os.path.join(CONVERTED_DIR, sample_wav))\n",
        "print(f\"\\nOriginal audio sample ({sample_wav}):\")\n",
        "print(f\"  Max amplitude: {np.max(np.abs(orig_audio))}\")\n",
        "print(f\"  Data type: {orig_audio.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gtaI8ynLx7i",
        "outputId": "a4dd2ca1-a163-42d1-abe2-a4aaae3ffcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking codebook files...\n",
            "============================================================\n",
            "Unit 00: 30.0ms, max amplitude: 15010, samples: 480\n",
            "Unit 29: 30.0ms, max amplitude: 2, samples: 480\n",
            "Unit 82: 30.0ms, max amplitude: 8, samples: 480\n",
            "Unit 55: 30.0ms, max amplitude: 10759, samples: 480\n",
            "\n",
            "Test synthesis file:\n",
            "  Duration: 12.2 seconds\n",
            "  Max amplitude: 0\n",
            "  Min amplitude: 0\n",
            "  Data type: int16\n",
            "  Non-zero samples: 0\n",
            "\n",
            "Original audio sample (MAVWYIN1DA_B01_MAT_001.wav):\n",
            "  Max amplitude: 23706\n",
            "  Data type: int16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FIXED VOCODER: Synthesize audio from unit sequences\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook\"\n",
        "\n",
        "# Reload codebook as int16 (original format)\n",
        "print(\"Reloading codebook (keeping original int16 format)...\")\n",
        "codebook_audio = {}\n",
        "sample_rate = None\n",
        "\n",
        "for unit_id in range(100):\n",
        "    unit_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "    if os.path.exists(unit_path):\n",
        "        sr, audio = wav.read(unit_path)\n",
        "        sample_rate = sr\n",
        "        codebook_audio[unit_id] = audio  # Keep as int16\n",
        "\n",
        "print(f\"✓ Loaded {len(codebook_audio)} units\")\n",
        "\n",
        "# Check amplitude distribution\n",
        "amplitudes = [(uid, np.max(np.abs(audio))) for uid, audio in codebook_audio.items()]\n",
        "amplitudes.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop 10 loudest units:\")\n",
        "for uid, amp in amplitudes[:10]:\n",
        "    print(f\"  Unit {uid:02d}: amplitude {amp}\")\n",
        "\n",
        "print(\"\\nTop 10 quietest units:\")\n",
        "for uid, amp in amplitudes[-10:]:\n",
        "    print(f\"  Unit {uid:02d}: amplitude {amp}\")\n",
        "\n",
        "def synthesize_fixed(unit_sequence):\n",
        "    \"\"\"Simple concatenation without fancy processing\"\"\"\n",
        "    if not unit_sequence:\n",
        "        return np.array([], dtype=np.int16)\n",
        "\n",
        "    segments = []\n",
        "    for unit_id in unit_sequence:\n",
        "        if unit_id in codebook_audio:\n",
        "            segments.append(codebook_audio[unit_id])\n",
        "        else:\n",
        "            # 20ms of silence for missing units\n",
        "            segments.append(np.zeros(int(0.02 * sample_rate), dtype=np.int16))\n",
        "\n",
        "    # Simple concatenation\n",
        "    output = np.concatenate(segments)\n",
        "    return output\n",
        "\n",
        "# Test with the same file\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "test_file = \"MAVWYIN1DA_B01_MAT_001.units.txt\"\n",
        "\n",
        "with open(os.path.join(UNITS_DIR, test_file), 'r') as f:\n",
        "    units = [int(u) for u in f.read().strip().split()[:500]]\n",
        "\n",
        "print(f\"\\nSynthesizing {len(units)} units...\")\n",
        "print(f\"First 20 units: {units[:20]}\")\n",
        "\n",
        "# Count how many units have good amplitude\n",
        "good_units = sum(1 for u in units if u in codebook_audio and np.max(np.abs(codebook_audio[u])) > 100)\n",
        "print(f\"Units with amplitude > 100: {good_units}/{len(units)}\")\n",
        "\n",
        "# Synthesize\n",
        "audio = synthesize_fixed(units)\n",
        "\n",
        "print(f\"\\nOutput audio:\")\n",
        "print(f\"  Samples: {len(audio)}\")\n",
        "print(f\"  Max amplitude: {np.max(np.abs(audio))}\")\n",
        "print(f\"  Duration: {len(audio)/sample_rate:.1f}s\")\n",
        "\n",
        "# Save\n",
        "output_path = f\"{VOCODER_DIR}/test_synthesis_fixed.wav\"\n",
        "wav.write(output_path, sample_rate, audio)\n",
        "print(f\"\\n✓ Saved: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlaKGw4DNIWa",
        "outputId": "fcb82d3c-ba73-4bf6-fac0-d51853f39ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading codebook (keeping original int16 format)...\n",
            "✓ Loaded 100 units\n",
            "\n",
            "Top 10 loudest units:\n",
            "  Unit 42: amplitude 19580\n",
            "  Unit 84: amplitude 19508\n",
            "  Unit 06: amplitude 18949\n",
            "  Unit 27: amplitude 18920\n",
            "  Unit 62: amplitude 18329\n",
            "  Unit 95: amplitude 18048\n",
            "  Unit 31: amplitude 17993\n",
            "  Unit 53: amplitude 17946\n",
            "  Unit 90: amplitude 17907\n",
            "  Unit 20: amplitude 17193\n",
            "\n",
            "Top 10 quietest units:\n",
            "  Unit 82: amplitude 8\n",
            "  Unit 04: amplitude 7\n",
            "  Unit 54: amplitude 3\n",
            "  Unit 13: amplitude 2\n",
            "  Unit 28: amplitude 2\n",
            "  Unit 29: amplitude 2\n",
            "  Unit 21: amplitude 1\n",
            "  Unit 26: amplitude 1\n",
            "  Unit 49: amplitude 1\n",
            "  Unit 86: amplitude 1\n",
            "\n",
            "Synthesizing 500 units...\n",
            "First 20 units: [26, 26, 26, 26, 26, 49, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "Units with amplitude > 100: 291/500\n",
            "\n",
            "Output audio:\n",
            "  Samples: 235437\n",
            "  Max amplitude: 19580\n",
            "  Duration: 14.7s\n",
            "\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/test_synthesis_fixed.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# REBUILD CODEBOOK: Find examples with actual audio content\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "CONVERTED_DIR = f\"{PROJECT_ROOT}/converted_audio\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook_v2\"\n",
        "\n",
        "os.makedirs(CODEBOOK_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"REBUILDING CODEBOOK WITH HIGH-AMPLITUDE EXAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get file lists\n",
        "unit_files = [f for f in os.listdir(UNITS_DIR) if f.endswith('.units.txt')]\n",
        "wav_files = {f.replace('.wav', ''): f for f in os.listdir(CONVERTED_DIR) if f.endswith('.wav')}\n",
        "\n",
        "# For each unit, find the LOUDEST example (not just the first one)\n",
        "print(\"Scanning for best examples of each unit...\")\n",
        "\n",
        "best_examples = {i: {'amplitude': 0, 'file': None, 'position': None} for i in range(100)}\n",
        "\n",
        "for unit_file in tqdm(unit_files[:80], desc=\"Scanning files\"):\n",
        "    base_name = unit_file.replace('.units.txt', '')\n",
        "    if base_name not in wav_files:\n",
        "        continue\n",
        "\n",
        "    # Load audio\n",
        "    wav_path = os.path.join(CONVERTED_DIR, wav_files[base_name])\n",
        "    try:\n",
        "        sr, audio = wav.read(wav_path)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # Load units\n",
        "    with open(os.path.join(UNITS_DIR, unit_file), 'r') as f:\n",
        "        units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "    # Check each unit position\n",
        "    for pos, unit_id in enumerate(units):\n",
        "        # Extract this unit's audio\n",
        "        start_sample = int(pos * 0.02 * sr)\n",
        "        end_sample = int((pos + 1) * 0.02 * sr)\n",
        "\n",
        "        if end_sample > len(audio):\n",
        "            continue\n",
        "\n",
        "        segment = audio[start_sample:end_sample]\n",
        "        amplitude = np.max(np.abs(segment))\n",
        "\n",
        "        # Keep if this is the loudest example we've found\n",
        "        if amplitude > best_examples[unit_id]['amplitude']:\n",
        "            best_examples[unit_id] = {\n",
        "                'amplitude': amplitude,\n",
        "                'file': base_name,\n",
        "                'position': pos\n",
        "            }\n",
        "\n",
        "# Now extract the best examples\n",
        "print(\"\\nExtracting best examples...\")\n",
        "\n",
        "codebook_audio = {}\n",
        "sample_rate = 16000\n",
        "\n",
        "for unit_id in tqdm(range(100), desc=\"Building codebook\"):\n",
        "    ex = best_examples[unit_id]\n",
        "    if ex['file'] is None:\n",
        "        continue\n",
        "\n",
        "    wav_path = os.path.join(CONVERTED_DIR, wav_files[ex['file']])\n",
        "    sr, audio = wav.read(wav_path)\n",
        "    sample_rate = sr\n",
        "\n",
        "    # Extract with small buffer for smoother transitions\n",
        "    start_sample = int(ex['position'] * 0.02 * sr)\n",
        "    end_sample = int((ex['position'] + 1) * 0.02 * sr)\n",
        "\n",
        "    # Add 5ms buffer on each side\n",
        "    buffer = int(0.005 * sr)\n",
        "    start_sample = max(0, start_sample - buffer)\n",
        "    end_sample = min(len(audio), end_sample + buffer)\n",
        "\n",
        "    segment = audio[start_sample:end_sample]\n",
        "\n",
        "    if len(segment) > 0:\n",
        "        codebook_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "        wav.write(codebook_path, sr, segment)\n",
        "        codebook_audio[unit_id] = segment\n",
        "\n",
        "# Report results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CODEBOOK V2 STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "amplitudes = [(uid, best_examples[uid]['amplitude']) for uid in range(100)]\n",
        "amplitudes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 units by amplitude:\")\n",
        "for uid, amp in amplitudes[:10]:\n",
        "    print(f\"  Unit {uid:02d}: {amp}\")\n",
        "\n",
        "print(\"\\nBottom 10 units by amplitude:\")\n",
        "for uid, amp in amplitudes[-10:]:\n",
        "    print(f\"  Unit {uid:02d}: {amp}\")\n",
        "\n",
        "low_amplitude = sum(1 for _, amp in amplitudes if amp < 500)\n",
        "print(f\"\\nUnits with low amplitude (<500): {low_amplitude}\")\n",
        "\n",
        "# Save metadata\n",
        "import json\n",
        "meta = {unit_id: {'amplitude': int(best_examples[unit_id]['amplitude'])} for unit_id in range(100)}\n",
        "with open(f\"{VOCODER_DIR}/codebook_v2_meta.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Codebook v2 saved to: {CODEBOOK_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlOR4jyVN4t8",
        "outputId": "8ee79ce2-dea2-4d3d-d001-6779d9131044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "REBUILDING CODEBOOK WITH HIGH-AMPLITUDE EXAMPLES\n",
            "============================================================\n",
            "Scanning for best examples of each unit...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning files: 100%|██████████| 80/80 [00:51<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting best examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building codebook: 100%|██████████| 100/100 [00:03<00:00, 31.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CODEBOOK V2 STATISTICS\n",
            "============================================================\n",
            "\n",
            "Top 10 units by amplitude:\n",
            "  Unit 46: 25361\n",
            "  Unit 20: 25218\n",
            "  Unit 10: 25194\n",
            "  Unit 44: 25156\n",
            "  Unit 90: 24929\n",
            "  Unit 28: 24815\n",
            "  Unit 00: 24801\n",
            "  Unit 81: 24728\n",
            "  Unit 76: 24624\n",
            "  Unit 55: 24613\n",
            "\n",
            "Bottom 10 units by amplitude:\n",
            "  Unit 32: 1168\n",
            "  Unit 99: 1089\n",
            "  Unit 02: 780\n",
            "  Unit 39: 391\n",
            "  Unit 56: 265\n",
            "  Unit 04: 264\n",
            "  Unit 82: 218\n",
            "  Unit 26: 214\n",
            "  Unit 29: 104\n",
            "  Unit 78: 91\n",
            "\n",
            "Units with low amplitude (<500): 7\n",
            "\n",
            "✓ Codebook v2 saved to: /content/drive/MyDrive/satere_project/vocoder_output/codebook_v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SYNTHESIZE WITH CODEBOOK V2\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook_v2\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "\n",
        "# Load codebook v2\n",
        "print(\"Loading codebook v2...\")\n",
        "codebook_audio = {}\n",
        "sample_rate = 16000\n",
        "\n",
        "for unit_id in range(100):\n",
        "    unit_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "    if os.path.exists(unit_path):\n",
        "        sr, audio = wav.read(unit_path)\n",
        "        sample_rate = sr\n",
        "        codebook_audio[unit_id] = audio\n",
        "\n",
        "print(f\"✓ Loaded {len(codebook_audio)} units\")\n",
        "\n",
        "def synthesize(unit_sequence):\n",
        "    \"\"\"Concatenate units to produce audio\"\"\"\n",
        "    segments = []\n",
        "    for unit_id in unit_sequence:\n",
        "        if unit_id in codebook_audio:\n",
        "            segments.append(codebook_audio[unit_id])\n",
        "        else:\n",
        "            segments.append(np.zeros(int(0.02 * sample_rate), dtype=np.int16))\n",
        "    return np.concatenate(segments) if segments else np.array([], dtype=np.int16)\n",
        "\n",
        "# Test 1: Synthesize from Matthew 1\n",
        "test_file = \"MAVWYIN1DA_B01_MAT_001.units.txt\"\n",
        "with open(os.path.join(UNITS_DIR, test_file), 'r') as f:\n",
        "    units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "# Take first 1000 units (~20 seconds)\n",
        "units_subset = units[:1000]\n",
        "\n",
        "print(f\"\\nSynthesizing Matthew 1 ({len(units_subset)} units, ~{len(units_subset)*0.02:.0f}s)...\")\n",
        "audio = synthesize(units_subset)\n",
        "\n",
        "output_path = f\"{VOCODER_DIR}/matthew1_synthesis.wav\"\n",
        "wav.write(output_path, sample_rate, audio)\n",
        "print(f\"✓ Saved: {output_path}\")\n",
        "print(f\"  Duration: {len(audio)/sample_rate:.1f}s\")\n",
        "print(f\"  Max amplitude: {np.max(np.abs(audio))}\")\n",
        "\n",
        "# Test 2: Also synthesize from a different book for comparison\n",
        "test_file2 = [f for f in os.listdir(UNITS_DIR) if 'JHN' in f or 'MRK' in f or 'LUK' in f]\n",
        "if test_file2:\n",
        "    with open(os.path.join(UNITS_DIR, test_file2[0]), 'r') as f:\n",
        "        units2 = [int(u) for u in f.read().strip().split()[:1000]]\n",
        "\n",
        "    print(f\"\\nSynthesizing {test_file2[0][:20]}... ({len(units2)} units)\")\n",
        "    audio2 = synthesize(units2)\n",
        "\n",
        "    output_path2 = f\"{VOCODER_DIR}/other_book_synthesis.wav\"\n",
        "    wav.write(output_path2, sample_rate, audio2)\n",
        "    print(f\"✓ Saved: {output_path2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Download these files from vocoder_output/ and listen:\")\n",
        "print(\"  - matthew1_synthesis.wav\")\n",
        "print(\"  - other_book_synthesis.wav (if created)\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKJ2P7jLOkzx",
        "outputId": "6ce88e26-1b0c-4618-b5de-1c47ef6f0835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading codebook v2...\n",
            "✓ Loaded 100 units\n",
            "\n",
            "Synthesizing Matthew 1 (1000 units, ~20s)...\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/matthew1_synthesis.wav\n",
            "  Duration: 30.0s\n",
            "  Max amplitude: 25361\n",
            "\n",
            "Synthesizing MAVWYIN1DA_B02_MRK_0... (1000 units)\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/other_book_synthesis.wav\n",
            "\n",
            "============================================================\n",
            "Download these files from vocoder_output/ and listen:\n",
            "  - matthew1_synthesis.wav\n",
            "  - other_book_synthesis.wav (if created)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# IMPROVED SYNTHESIS WITH CROSSFADE AND FILTERING\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from scipy import signal\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "CODEBOOK_DIR = f\"{VOCODER_DIR}/codebook_v2\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "\n",
        "# Load codebook v2\n",
        "print(\"Loading codebook v2...\")\n",
        "codebook_audio = {}\n",
        "sample_rate = 16000\n",
        "\n",
        "for unit_id in range(100):\n",
        "    unit_path = f\"{CODEBOOK_DIR}/unit_{unit_id:02d}.wav\"\n",
        "    if os.path.exists(unit_path):\n",
        "        sr, audio = wav.read(unit_path)\n",
        "        sample_rate = sr\n",
        "        # Convert to float for processing\n",
        "        codebook_audio[unit_id] = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "print(f\"✓ Loaded {len(codebook_audio)} units\")\n",
        "\n",
        "def synthesize_smooth(unit_sequence, crossfade_ms=10):\n",
        "    \"\"\"\n",
        "    Concatenate units with crossfade to reduce metallic artifacts.\n",
        "    \"\"\"\n",
        "    if not unit_sequence:\n",
        "        return np.array([], dtype=np.int16)\n",
        "\n",
        "    crossfade_samples = int(crossfade_ms / 1000 * sample_rate)\n",
        "\n",
        "    # Get first segment\n",
        "    if unit_sequence[0] in codebook_audio:\n",
        "        output = codebook_audio[unit_sequence[0]].copy()\n",
        "    else:\n",
        "        output = np.zeros(int(0.02 * sample_rate), dtype=np.float32)\n",
        "\n",
        "    # Concatenate with crossfade\n",
        "    for unit_id in unit_sequence[1:]:\n",
        "        if unit_id in codebook_audio:\n",
        "            segment = codebook_audio[unit_id].copy()\n",
        "        else:\n",
        "            segment = np.zeros(int(0.02 * sample_rate), dtype=np.float32)\n",
        "\n",
        "        if len(segment) < crossfade_samples or len(output) < crossfade_samples:\n",
        "            output = np.concatenate([output, segment])\n",
        "            continue\n",
        "\n",
        "        # Create crossfade\n",
        "        fade_out = np.linspace(1.0, 0.0, crossfade_samples)\n",
        "        fade_in = np.linspace(0.0, 1.0, crossfade_samples)\n",
        "\n",
        "        # Apply fades\n",
        "        output[-crossfade_samples:] *= fade_out\n",
        "        segment[:crossfade_samples] *= fade_in\n",
        "\n",
        "        # Overlap-add\n",
        "        output[-crossfade_samples:] += segment[:crossfade_samples]\n",
        "        output = np.concatenate([output, segment[crossfade_samples:]])\n",
        "\n",
        "    # Apply gentle low-pass filter to reduce harshness\n",
        "    nyquist = sample_rate / 2\n",
        "    cutoff = 7000  # Hz - preserve speech but reduce high-frequency artifacts\n",
        "    b, a = signal.butter(4, cutoff / nyquist, btype='low')\n",
        "    output = signal.filtfilt(b, a, output)\n",
        "\n",
        "    # Normalize\n",
        "    max_val = np.max(np.abs(output))\n",
        "    if max_val > 0:\n",
        "        output = output / max_val * 0.85\n",
        "\n",
        "    # Convert back to int16\n",
        "    return (output * 32767).astype(np.int16)\n",
        "\n",
        "# Synthesize Matthew 1 with smoothing\n",
        "test_file = \"MAVWYIN1DA_B01_MAT_001.units.txt\"\n",
        "with open(os.path.join(UNITS_DIR, test_file), 'r') as f:\n",
        "    units = [int(u) for u in f.read().strip().split()[:1000]]\n",
        "\n",
        "print(f\"\\nSynthesizing with crossfade smoothing...\")\n",
        "audio_smooth = synthesize_smooth(units, crossfade_ms=10)\n",
        "\n",
        "output_path = f\"{VOCODER_DIR}/matthew1_smooth.wav\"\n",
        "wav.write(output_path, sample_rate, audio_smooth)\n",
        "print(f\"✓ Saved: {output_path}\")\n",
        "print(f\"  Duration: {len(audio_smooth)/sample_rate:.1f}s\")\n",
        "\n",
        "# Also try with longer crossfade\n",
        "print(f\"\\nSynthesizing with longer crossfade (15ms)...\")\n",
        "audio_smoother = synthesize_smooth(units, crossfade_ms=15)\n",
        "\n",
        "output_path2 = f\"{VOCODER_DIR}/matthew1_smoother.wav\"\n",
        "wav.write(output_path2, sample_rate, audio_smoother)\n",
        "print(f\"✓ Saved: {output_path2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Download and compare:\")\n",
        "print(\"  - matthew1_smooth.wav (10ms crossfade)\")\n",
        "print(\"  - matthew1_smoother.wav (15ms crossfade)\")\n",
        "print(\"These should have less metallic noise.\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtyTZygLPGI5",
        "outputId": "f713f702-b1ed-4a90-abdf-412af02d92a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading codebook v2...\n",
            "✓ Loaded 100 units\n",
            "\n",
            "Synthesizing with crossfade smoothing...\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/matthew1_smooth.wav\n",
            "  Duration: 20.0s\n",
            "\n",
            "Synthesizing with longer crossfade (15ms)...\n",
            "✓ Saved: /content/drive/MyDrive/satere_project/vocoder_output/matthew1_smoother.wav\n",
            "\n",
            "============================================================\n",
            "Download and compare:\n",
            "  - matthew1_smooth.wav (10ms crossfade)\n",
            "  - matthew1_smoother.wav (15ms crossfade)\n",
            "These should have less metallic noise.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to MP3 for WhatsApp\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "VOCODER_DIR = f\"{PROJECT_ROOT}/vocoder_output\"\n",
        "\n",
        "# Convert the smoothest version to MP3\n",
        "input_file = f\"{VOCODER_DIR}/matthew1_smoother.wav\"\n",
        "output_file = f\"{VOCODER_DIR}/matthew1_whatsapp.mp3\"\n",
        "\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-i\", input_file,\n",
        "    \"-codec:a\", \"libmp3lame\", \"-qscale:a\", \"2\",\n",
        "    output_file\n",
        "], capture_output=True)\n",
        "\n",
        "print(f\"✓ Created: {output_file}\")\n",
        "\n",
        "# Also make a shorter version (first 10 seconds) in case they want a quick sample\n",
        "input_file2 = f\"{VOCODER_DIR}/matthew1_smoother.wav\"\n",
        "output_file2 = f\"{VOCODER_DIR}/matthew1_short_whatsapp.mp3\"\n",
        "\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-i\", input_file2,\n",
        "    \"-t\", \"10\",  # First 10 seconds only\n",
        "    \"-codec:a\", \"libmp3lame\", \"-qscale:a\", \"2\",\n",
        "    output_file2\n",
        "], capture_output=True)\n",
        "\n",
        "print(f\"✓ Created: {output_file2} (10 seconds)\")\n",
        "\n",
        "print(\"\\nDownload from vocoder_output/:\")\n",
        "print(\"  - matthew1_whatsapp.mp3 (full ~20 seconds)\")\n",
        "print(\"  - matthew1_short_whatsapp.mp3 (10 seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CuhesFgQOqF",
        "outputId": "4419c9a7-50a1-4b80-a987-d3d286ea4208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created: /content/drive/MyDrive/satere_project/vocoder_output/matthew1_whatsapp.mp3\n",
            "✓ Created: /content/drive/MyDrive/satere_project/vocoder_output/matthew1_short_whatsapp.mp3 (10 seconds)\n",
            "\n",
            "Download from vocoder_output/:\n",
            "  - matthew1_whatsapp.mp3 (full ~20 seconds)\n",
            "  - matthew1_short_whatsapp.mp3 (10 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# NEURAL VOCODER TRAINING: HiFi-GAN for Sateré-Mawé\n",
        "# =============================================================================\n",
        "# This will take 4-8 hours to train. Start it and let it run overnight.\n",
        "\n",
        "# Step 1: Install dependencies\n",
        "!pip install -q torch torchaudio tensorboard\n",
        "\n",
        "# Clone HiFi-GAN repository\n",
        "import os\n",
        "if not os.path.exists('/content/hifi-gan'):\n",
        "    !git clone https://github.com/jik876/hifi-gan.git /content/hifi-gan\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcf0AbFKT40f",
        "outputId": "b43d0dad-2dce-415f-85bf-faf283f895b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/hifi-gan'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Total 48 (delta 0), reused 0 (delta 0), pack-reused 48 (from 1)\u001b[K\n",
            "Receiving objects: 100% (48/48), 620.94 KiB | 22.18 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "✓ Dependencies installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 2: PREPARE TRAINING DATA FOR HiFi-GAN\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "UNITS_DIR = f\"{PROJECT_ROOT}/satere_units\"\n",
        "CONVERTED_DIR = f\"{PROJECT_ROOT}/converted_audio\"\n",
        "HIFIGAN_DATA = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "os.makedirs(f\"{HIFIGAN_DATA}/wavs\", exist_ok=True)\n",
        "os.makedirs(f\"{HIFIGAN_DATA}/units\", exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARING HiFi-GAN TRAINING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get all files\n",
        "unit_files = sorted([f for f in os.listdir(UNITS_DIR) if f.endswith('.units.txt')])\n",
        "print(f\"Found {len(unit_files)} unit files\")\n",
        "\n",
        "# We'll create segments of ~3-10 seconds for training\n",
        "# This gives the model enough context to learn prosody\n",
        "\n",
        "SEGMENT_LENGTH_SEC = 5  # 5 second segments\n",
        "SAMPLE_RATE = 16000\n",
        "SEGMENT_SAMPLES = SEGMENT_LENGTH_SEC * SAMPLE_RATE\n",
        "UNITS_PER_SECOND = 50  # 20ms per unit = 50 units/second\n",
        "SEGMENT_UNITS = SEGMENT_LENGTH_SEC * UNITS_PER_SECOND\n",
        "\n",
        "training_pairs = []\n",
        "segment_id = 0\n",
        "\n",
        "print(f\"\\nCreating {SEGMENT_LENGTH_SEC}-second segments...\")\n",
        "\n",
        "for unit_file in tqdm(unit_files[:150], desc=\"Processing files\"):  # Use 150 files for training\n",
        "    base_name = unit_file.replace('.units.txt', '')\n",
        "    wav_path = os.path.join(CONVERTED_DIR, base_name + '.wav')\n",
        "\n",
        "    if not os.path.exists(wav_path):\n",
        "        continue\n",
        "\n",
        "    # Load audio\n",
        "    try:\n",
        "        sr, audio = wav.read(wav_path)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # Load units\n",
        "    with open(os.path.join(UNITS_DIR, unit_file), 'r') as f:\n",
        "        units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "    # Create segments\n",
        "    num_segments = len(audio) // SEGMENT_SAMPLES\n",
        "\n",
        "    for seg_idx in range(num_segments):\n",
        "        # Audio segment\n",
        "        start_sample = seg_idx * SEGMENT_SAMPLES\n",
        "        end_sample = start_sample + SEGMENT_SAMPLES\n",
        "        audio_segment = audio[start_sample:end_sample]\n",
        "\n",
        "        # Corresponding units\n",
        "        start_unit = seg_idx * SEGMENT_UNITS\n",
        "        end_unit = start_unit + SEGMENT_UNITS\n",
        "\n",
        "        if end_unit > len(units):\n",
        "            continue\n",
        "\n",
        "        unit_segment = units[start_unit:end_unit]\n",
        "\n",
        "        # Skip silent segments\n",
        "        if np.max(np.abs(audio_segment)) < 500:\n",
        "            continue\n",
        "\n",
        "        # Save audio segment\n",
        "        seg_name = f\"seg_{segment_id:05d}\"\n",
        "        wav_out = f\"{HIFIGAN_DATA}/wavs/{seg_name}.wav\"\n",
        "        wav.write(wav_out, sr, audio_segment)\n",
        "\n",
        "        # Save unit sequence\n",
        "        unit_out = f\"{HIFIGAN_DATA}/units/{seg_name}.txt\"\n",
        "        with open(unit_out, 'w') as f:\n",
        "            f.write(' '.join(map(str, unit_segment)))\n",
        "\n",
        "        training_pairs.append({\n",
        "            'id': seg_name,\n",
        "            'source_file': base_name,\n",
        "            'duration_sec': SEGMENT_LENGTH_SEC\n",
        "        })\n",
        "\n",
        "        segment_id += 1\n",
        "\n",
        "print(f\"\\n✓ Created {len(training_pairs)} training segments\")\n",
        "print(f\"  Total duration: {len(training_pairs) * SEGMENT_LENGTH_SEC / 60:.1f} minutes\")\n",
        "\n",
        "# Split into train/validation\n",
        "random.shuffle(training_pairs)\n",
        "val_size = min(100, len(training_pairs) // 10)\n",
        "train_pairs = training_pairs[val_size:]\n",
        "val_pairs = training_pairs[:val_size]\n",
        "\n",
        "# Save file lists\n",
        "with open(f\"{HIFIGAN_DATA}/train_files.txt\", 'w') as f:\n",
        "    for p in train_pairs:\n",
        "        f.write(p['id'] + '\\n')\n",
        "\n",
        "with open(f\"{HIFIGAN_DATA}/val_files.txt\", 'w') as f:\n",
        "    for p in val_pairs:\n",
        "        f.write(p['id'] + '\\n')\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'num_train': len(train_pairs),\n",
        "    'num_val': len(val_pairs),\n",
        "    'segment_length_sec': SEGMENT_LENGTH_SEC,\n",
        "    'sample_rate': SAMPLE_RATE,\n",
        "    'num_units': 100\n",
        "}\n",
        "\n",
        "with open(f\"{HIFIGAN_DATA}/metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Training set: {len(train_pairs)} segments\")\n",
        "print(f\"✓ Validation set: {len(val_pairs)} segments\")\n",
        "print(f\"✓ Data saved to: {HIFIGAN_DATA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBPPATN1cQYm",
        "outputId": "73bfd260-7fcd-4c1c-e70b-d00578114f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PREPARING HiFi-GAN TRAINING DATA\n",
            "============================================================\n",
            "Found 229 unit files\n",
            "\n",
            "Creating 5-second segments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 150/150 [05:09<00:00,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Created 22934 training segments\n",
            "  Total duration: 1911.2 minutes\n",
            "\n",
            "✓ Training set: 22834 segments\n",
            "✓ Validation set: 100 segments\n",
            "✓ Data saved to: /content/drive/MyDrive/satere_project/hifigan_training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 3: CREATE UNIT-BASED HiFi-GAN MODEL\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/hifi-gan')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "HIFIGAN_DATA = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_list, segment_length=16000*5):\n",
        "        self.data_dir = data_dir\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "        with open(file_list, 'r') as f:\n",
        "            self.files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        print(f\"  Loaded {len(self.files)} files\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "\n",
        "        # Load audio\n",
        "        wav_path = f\"{self.data_dir}/wavs/{name}.wav\"\n",
        "        sr, audio = wav.read(wav_path)\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Load units\n",
        "        unit_path = f\"{self.data_dir}/units/{name}.txt\"\n",
        "        with open(unit_path, 'r') as f:\n",
        "            units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "        return torch.FloatTensor(audio), torch.LongTensor(units)\n",
        "\n",
        "# =============================================================================\n",
        "# Simple but effective Unit-to-Audio Generator\n",
        "# =============================================================================\n",
        "\n",
        "class UnitVocoder(nn.Module):\n",
        "    def __init__(self, num_units=100, unit_embed_dim=256, upsample_rates=[5, 5, 4, 4, 2]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Unit embedding\n",
        "        self.unit_embed = nn.Embedding(num_units, unit_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.pre_conv = nn.Conv1d(unit_embed_dim, 512, kernel_size=7, padding=3)\n",
        "\n",
        "        # Upsampling layers (20ms units -> 16kHz audio)\n",
        "        # 50 units/sec -> 16000 samples/sec = 320x upsampling\n",
        "        # 5 * 5 * 4 * 4 * 2 = 800, we'll trim\n",
        "        self.upsample_rates = upsample_rates\n",
        "        self.upsamples = nn.ModuleList()\n",
        "\n",
        "        channels = 512\n",
        "        for i, rate in enumerate(upsample_rates):\n",
        "            out_channels = channels // 2 if i < len(upsample_rates) - 1 else 64\n",
        "            self.upsamples.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose1d(channels, out_channels,\n",
        "                                       kernel_size=rate*2, stride=rate, padding=rate//2),\n",
        "                    nn.LeakyReLU(0.1),\n",
        "                    nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3),\n",
        "                    nn.LeakyReLU(0.1),\n",
        "                )\n",
        "            )\n",
        "            channels = out_channels\n",
        "\n",
        "        # Output layer\n",
        "        self.post_conv = nn.Conv1d(64, 1, kernel_size=7, padding=3)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, units):\n",
        "        # units: (batch, seq_len)\n",
        "        x = self.unit_embed(units)  # (batch, seq_len, embed_dim)\n",
        "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len)\n",
        "\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = self.tanh(x)\n",
        "\n",
        "        return x.squeeze(1)  # (batch, audio_len)\n",
        "\n",
        "# =============================================================================\n",
        "# Multi-Scale Discriminator\n",
        "# =============================================================================\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(1),\n",
        "            self._make_discriminator(2),\n",
        "            self._make_discriminator(4),\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(2, 2),\n",
        "            nn.AvgPool1d(4, 4),\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self, scale):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, kernel_size=41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, kernel_size=41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, kernel_size=5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# =============================================================================\n",
        "# Initialize models\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nInitializing models...\")\n",
        "generator = UnitVocoder(num_units=100).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "num_params_g = sum(p.numel() for p in generator.parameters())\n",
        "num_params_d = sum(p.numel() for p in discriminator.parameters())\n",
        "print(f\"  Generator parameters: {num_params_g:,}\")\n",
        "print(f\"  Discriminator parameters: {num_params_d:,}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Create data loaders\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "train_dataset = UnitAudioDataset(HIFIGAN_DATA, f\"{HIFIGAN_DATA}/train_files.txt\")\n",
        "val_dataset = UnitAudioDataset(HIFIGAN_DATA, f\"{HIFIGAN_DATA}/val_files.txt\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "\n",
        "print(\"\\n✓ Models and data ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY7EsrXMcjTJ",
        "outputId": "d930bef3-f5b6-4aa7-c9ca-eb5f8674676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initializing models...\n",
            "  Generator parameters: 3,311,617\n",
            "  Discriminator parameters: 5,455,491\n",
            "\n",
            "Loading datasets...\n",
            "  Loaded 22834 files\n",
            "  Loaded 100 files\n",
            "  Train batches: 1428\n",
            "  Val batches: 25\n",
            "\n",
            "✓ Models and data ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 4: TRAIN THE VOCODER\n",
        "# =============================================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# Training settings\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.0002\n",
        "SAVE_EVERY = 5  # Save checkpoint every N epochs\n",
        "\n",
        "# Optimizers\n",
        "optimizer_g = optim.AdamW(generator.parameters(), lr=LEARNING_RATE, betas=(0.8, 0.99))\n",
        "optimizer_d = optim.AdamW(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.8, 0.99))\n",
        "\n",
        "# Learning rate schedulers\n",
        "scheduler_g = optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=0.99)\n",
        "scheduler_d = optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=0.99)\n",
        "\n",
        "# Loss functions\n",
        "l1_loss = nn.L1Loss()\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(f\"{MODEL_DIR}/logs\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING UNIT VOCODER\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Batch size: 16\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    total_loss_g = 0\n",
        "    total_loss_d = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for batch_idx, (audio, units) in enumerate(progress_bar):\n",
        "        audio = audio.to(device)\n",
        "        units = units.to(device)\n",
        "\n",
        "        # Generate audio\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # ===================\n",
        "        # Train Discriminator\n",
        "        # ===================\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Real audio\n",
        "        real_out = discriminator(audio.unsqueeze(1))\n",
        "        # Fake audio\n",
        "        fake_out = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        loss_d = 0\n",
        "        for real, fake in zip(real_out, fake_out):\n",
        "            loss_d += mse_loss(real, torch.ones_like(real))\n",
        "            loss_d += mse_loss(fake, torch.zeros_like(fake))\n",
        "\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # ===================\n",
        "        # Train Generator\n",
        "        # ===================\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # Adversarial loss\n",
        "        fake_out = discriminator(audio_fake.unsqueeze(1))\n",
        "        loss_adv = 0\n",
        "        for fake in fake_out:\n",
        "            loss_adv += mse_loss(fake, torch.ones_like(fake))\n",
        "\n",
        "        # Reconstruction loss (L1)\n",
        "        loss_recon = l1_loss(audio_fake, audio) * 45  # Weight for L1 loss\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_g = loss_adv + loss_recon\n",
        "\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        total_loss_g += loss_g.item()\n",
        "        total_loss_d += loss_d.item()\n",
        "        num_batches += 1\n",
        "        global_step += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'G_loss': f'{loss_g.item():.3f}',\n",
        "            'D_loss': f'{loss_d.item():.3f}'\n",
        "        })\n",
        "\n",
        "        # Log to tensorboard\n",
        "        if global_step % 100 == 0:\n",
        "            writer.add_scalar('Loss/Generator', loss_g.item(), global_step)\n",
        "            writer.add_scalar('Loss/Discriminator', loss_d.item(), global_step)\n",
        "\n",
        "    # Update learning rates\n",
        "    scheduler_g.step()\n",
        "    scheduler_d.step()\n",
        "\n",
        "    # Epoch summary\n",
        "    avg_loss_g = total_loss_g / num_batches\n",
        "    avg_loss_d = total_loss_d / num_batches\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    print(f\"\\n  Epoch {epoch+1} complete in {epoch_time:.1f}s\")\n",
        "    print(f\"  Avg G Loss: {avg_loss_g:.4f}, Avg D Loss: {avg_loss_d:.4f}\")\n",
        "\n",
        "    # ===================\n",
        "    # Validation\n",
        "    # ===================\n",
        "    if (epoch + 1) % 2 == 0:  # Validate every 2 epochs\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for audio, units in val_loader:\n",
        "                audio = audio.to(device)\n",
        "                units = units.to(device)\n",
        "\n",
        "                audio_fake = generator(units)\n",
        "                min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "                val_loss += l1_loss(audio_fake[:, :min_len], audio[:, :min_len]).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"  Validation L1 Loss: {val_loss:.4f}\")\n",
        "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'generator_state_dict': generator.state_dict(),\n",
        "                'discriminator_state_dict': discriminator.state_dict(),\n",
        "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "            }, f\"{MODEL_DIR}/best_model.pt\")\n",
        "            print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % SAVE_EVERY == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch+1}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Generate sample audio every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_audio, sample_units = val_dataset[0]\n",
        "            sample_units = sample_units.unsqueeze(0).to(device)\n",
        "            generated = generator(sample_units).squeeze().cpu().numpy()\n",
        "\n",
        "            # Save sample\n",
        "            generated_int16 = (generated * 32767).astype(np.int16)\n",
        "            sample_path = f\"{MODEL_DIR}/sample_epoch{epoch+1}.wav\"\n",
        "            wav.write(sample_path, 16000, generated_int16)\n",
        "            print(f\"  ✓ Saved sample audio: {sample_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ TRAINING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {MODEL_DIR}\")\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRO3zCAgENC",
        "outputId": "a532af73-0561-468d-dced-306cb51b431d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING UNIT VOCODER\n",
            "============================================================\n",
            "  Epochs: 50\n",
            "  Batch size: 16\n",
            "  Learning rate: 0.0002\n",
            "  Device: cuda\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 1428/1428 [40:09<00:00,  1.69s/it, G_loss=7.432, D_loss=0.247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 1 complete in 2409.8s\n",
            "  Avg G Loss: 6.2638, Avg D Loss: 0.5480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 1428/1428 [40:07<00:00,  1.69s/it, G_loss=6.476, D_loss=0.449]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 2 complete in 2407.1s\n",
            "  Avg G Loss: 6.4641, Avg D Loss: 0.4347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0916\n",
            "  ✓ Saved best model (val_loss: 0.0916)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 1428/1428 [40:08<00:00,  1.69s/it, G_loss=7.151, D_loss=0.392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 3 complete in 2408.5s\n",
            "  Avg G Loss: 6.4193, Avg D Loss: 0.4168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 1428/1428 [40:14<00:00,  1.69s/it, G_loss=6.001, D_loss=0.870]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 4 complete in 2414.1s\n",
            "  Avg G Loss: 6.1879, Avg D Loss: 0.6943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 1428/1428 [40:15<00:00,  1.69s/it, G_loss=6.767, D_loss=0.558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 5 complete in 2415.0s\n",
            "  Avg G Loss: 6.2297, Avg D Loss: 0.6593\n",
            "  ✓ Saved checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 1428/1428 [40:11<00:00,  1.69s/it, G_loss=5.346, D_loss=0.798]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 6 complete in 2411.9s\n",
            "  Avg G Loss: 6.2239, Avg D Loss: 0.6581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 1428/1428 [40:12<00:00,  1.69s/it, G_loss=6.523, D_loss=0.576]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 7 complete in 2412.2s\n",
            "  Avg G Loss: 6.2421, Avg D Loss: 0.6541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 1428/1428 [40:13<00:00,  1.69s/it, G_loss=7.782, D_loss=0.497]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 8 complete in 2413.4s\n",
            "  Avg G Loss: 6.2512, Avg D Loss: 0.6502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 1428/1428 [40:13<00:00,  1.69s/it, G_loss=7.112, D_loss=0.572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 9 complete in 2413.0s\n",
            "  Avg G Loss: 6.3734, Avg D Loss: 0.6357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 1428/1428 [40:14<00:00,  1.69s/it, G_loss=5.246, D_loss=0.948]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 10 complete in 2415.0s\n",
            "  Avg G Loss: 6.3499, Avg D Loss: 0.6462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0981\n",
            "  ✓ Saved checkpoint\n",
            "  ✓ Saved sample audio: /content/drive/MyDrive/satere_project/hifigan_model/sample_epoch10.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 1428/1428 [40:15<00:00,  1.69s/it, G_loss=5.311, D_loss=0.739]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 11 complete in 2415.7s\n",
            "  Avg G Loss: 6.3602, Avg D Loss: 0.6355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 1428/1428 [40:12<00:00,  1.69s/it, G_loss=7.143, D_loss=0.562]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 12 complete in 2412.5s\n",
            "  Avg G Loss: 6.3228, Avg D Loss: 0.6362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 1428/1428 [40:14<00:00,  1.69s/it, G_loss=6.430, D_loss=0.780]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 13 complete in 2414.2s\n",
            "  Avg G Loss: 6.4224, Avg D Loss: 0.6193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 1428/1428 [40:15<00:00,  1.69s/it, G_loss=6.454, D_loss=0.535]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 14 complete in 2415.2s\n",
            "  Avg G Loss: 6.4475, Avg D Loss: 0.6059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 1428/1428 [40:17<00:00,  1.69s/it, G_loss=6.418, D_loss=0.655]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 15 complete in 2417.1s\n",
            "  Avg G Loss: 6.4663, Avg D Loss: 0.5943\n",
            "  ✓ Saved checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 1428/1428 [40:18<00:00,  1.69s/it, G_loss=7.285, D_loss=0.565]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 16 complete in 2418.6s\n",
            "  Avg G Loss: 6.5010, Avg D Loss: 0.5799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 1428/1428 [40:17<00:00,  1.69s/it, G_loss=5.465, D_loss=0.692]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 17 complete in 2417.5s\n",
            "  Avg G Loss: 6.4730, Avg D Loss: 0.5859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 1428/1428 [40:17<00:00,  1.69s/it, G_loss=6.164, D_loss=0.669]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 18 complete in 2417.1s\n",
            "  Avg G Loss: 6.5268, Avg D Loss: 0.5709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 1428/1428 [40:16<00:00,  1.69s/it, G_loss=6.723, D_loss=0.584]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 19 complete in 2416.8s\n",
            "  Avg G Loss: 6.5942, Avg D Loss: 0.5276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 1428/1428 [40:19<00:00,  1.69s/it, G_loss=6.692, D_loss=0.557]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 20 complete in 2419.0s\n",
            "  Avg G Loss: 6.6849, Avg D Loss: 0.4644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0965\n",
            "  ✓ Saved checkpoint\n",
            "  ✓ Saved sample audio: /content/drive/MyDrive/satere_project/hifigan_model/sample_epoch20.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 1428/1428 [40:17<00:00,  1.69s/it, G_loss=6.741, D_loss=0.236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 21 complete in 2417.9s\n",
            "  Avg G Loss: 6.7695, Avg D Loss: 0.4220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 1428/1428 [40:19<00:00,  1.69s/it, G_loss=7.123, D_loss=0.284]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 22 complete in 2419.1s\n",
            "  Avg G Loss: 6.8773, Avg D Loss: 0.3488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 1428/1428 [40:19<00:00,  1.69s/it, G_loss=7.362, D_loss=0.423]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 23 complete in 2419.4s\n",
            "  Avg G Loss: 6.8554, Avg D Loss: 0.3593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 1428/1428 [40:19<00:00,  1.69s/it, G_loss=6.539, D_loss=0.164]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 24 complete in 2419.5s\n",
            "  Avg G Loss: 6.9048, Avg D Loss: 0.3414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation L1 Loss: 0.0990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 1428/1428 [40:19<00:00,  1.69s/it, G_loss=6.589, D_loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 25 complete in 2419.8s\n",
            "  Avg G Loss: 6.8876, Avg D Loss: 0.3214\n",
            "  ✓ Saved checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50:  29%|██▊       | 408/1428 [11:31<28:49,  1.70s/it, G_loss=6.759, D_loss=0.319]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "# Check for existing codebook files\n",
        "possible_files = [\n",
        "    \"satere_soundscript_codebook_v1.npy\",\n",
        "    \"kmeans_model.pkl\",\n",
        "    \"kmeans_model.joblib\",\n",
        "    \"codebook.npy\",\n",
        "    \"cluster_centers.npy\"\n",
        "]\n",
        "\n",
        "print(\"Searching for codebook files...\\n\")\n",
        "\n",
        "for root, dirs, files in os.walk(PROJECT_ROOT):\n",
        "    for f in files:\n",
        "        if \"kmeans\" in f.lower() or \"codebook\" in f.lower() or \"cluster\" in f.lower():\n",
        "            path = os.path.join(root, f)\n",
        "            print(f\"Found: {path}\")\n",
        "\n",
        "# Also check satere_units folder\n",
        "units_dir = f\"{PROJECT_ROOT}/satere_units\"\n",
        "if os.path.exists(units_dir):\n",
        "    print(f\"\\nFiles in satere_units/: {os.listdir(units_dir)[:10]}...\")"
      ],
      "metadata": {
        "id": "UjKuW341ghwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/satere_project\"))"
      ],
      "metadata": {
        "id": "luga7L-yNACj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "MODEL_DIR = \"/content/drive/MyDrive/satere_project/hifigan_model\"\n",
        "print(\"Saved checkpoints:\")\n",
        "for f in sorted(os.listdir(MODEL_DIR)):\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6MEnhIReLF4Y",
        "outputId": "9f38757d-5b99-4fa3-ac96-f547e57c755d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoints:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1451939815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODEL_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/satere_project/hifigan_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved checkpoints:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr2411g5rBGx",
        "outputId": "07ea8954-e780-4ac9-b151-d2a9c8bdb080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "MODEL_DIR = \"/content/drive/MyDrive/satere_project/hifigan_model\"\n",
        "print(\"Saved checkpoints:\")\n",
        "for f in sorted(os.listdir(MODEL_DIR)):\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqUiuP4MrOnF",
        "outputId": "123ce388-1f95-4926-84d7-c72bfb173006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoints:\n",
            "  best_model.pt\n",
            "  checkpoint_epoch10.pt\n",
            "  checkpoint_epoch15.pt\n",
            "  checkpoint_epoch20.pt\n",
            "  checkpoint_epoch25.pt\n",
            "  checkpoint_epoch5.pt\n",
            "  logs\n",
            "  sample_epoch10.wav\n",
            "  sample_epoch20.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load training config\n",
        "with open(f\"{TRAINING_DIR}/training_manifest.json\", \"r\") as f:\n",
        "    manifest = json.load(f)\n",
        "\n",
        "print(f\"Training samples: {len(manifest['files'])}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS ============\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, dilations):\n",
        "        super().__init__()\n",
        "        self.convs1 = nn.ModuleList()\n",
        "        self.convs2 = nn.ModuleList()\n",
        "        for d in dilations:\n",
        "            self.convs1.append(nn.Conv1d(channels, channels, kernel_size, dilation=d, padding=(kernel_size*d-d)//2))\n",
        "            self.convs2.append(nn.Conv1d(channels, channels, kernel_size, dilation=1, padding=(kernel_size-1)//2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.1)\n",
        "            xt = F.leaky_relu(c1(xt), 0.1)\n",
        "            xt = c2(xt)\n",
        "            x = x + xt\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, unit_embed_dim=256, upsample_rates=[8,8,4,2],\n",
        "                 upsample_kernels=[16,16,8,4], resblock_kernels=[3,7,11], resblock_dilations=[[1,3,5],[1,3,5],[1,3,5]]):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_units, unit_embed_dim)\n",
        "        self.conv_pre = nn.Conv1d(unit_embed_dim, 256, 7, padding=3)\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.resblocks = nn.ModuleList()\n",
        "\n",
        "        ch = 256\n",
        "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernels)):\n",
        "            self.ups.append(nn.ConvTranspose1d(ch, ch//2, k, u, padding=(k-u)//2))\n",
        "            ch = ch//2\n",
        "            for j, (rk, rd) in enumerate(zip(resblock_kernels, resblock_dilations)):\n",
        "                self.resblocks.append(ResBlock(ch, rk, rd))\n",
        "\n",
        "        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)\n",
        "        self.num_upsamples = len(upsample_rates)\n",
        "        self.num_kernels = len(resblock_kernels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).transpose(1, 2)\n",
        "        x = self.conv_pre(x)\n",
        "        for i, up in enumerate(self.ups):\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            x = up(x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(1, 16, 15, padding=7),\n",
        "            nn.Conv1d(16, 64, 41, stride=4, padding=20, groups=4),\n",
        "            nn.Conv1d(64, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.Conv1d(256, 1024, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(1024, 1024, 41, stride=4, padding=20, groups=256),\n",
        "            nn.Conv1d(1024, 1024, 5, padding=2),\n",
        "        ])\n",
        "        self.conv_post = nn.Conv1d(1024, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmaps = []\n",
        "        for conv in self.convs:\n",
        "            x = F.leaky_relu(conv(x), 0.1)\n",
        "            fmaps.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmaps.append(x)\n",
        "        return x.flatten(1, -1), fmaps\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, manifest, segment_length=16384):\n",
        "        self.files = manifest['files']\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.files[idx]\n",
        "        units = np.load(item['units_path'])\n",
        "        audio = np.load(item['audio_path'])\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch25.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER ============\n",
        "dataset = UnitAudioDataset(manifest)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Validation set\n",
        "val_dataset = UnitAudioDataset(manifest)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        # Generate\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_out, _ = discriminator(audio.unsqueeze(1))\n",
        "        fake_out, _ = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "        d_loss = torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_out, fake_fmaps = discriminator(audio_fake.unsqueeze(1))\n",
        "        _, real_fmaps = discriminator(audio.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = torch.mean((fake_out - 1)**2)\n",
        "        g_loss_fm = sum(torch.mean(torch.abs(rf - ff)) for rf, ff in zip(real_fmaps, fake_fmaps))\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_fm * 2 + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete in {pbar.format_dict['elapsed']:.1f}s\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Save sample every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            import scipy.io.wavfile as wav\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "-q2hGtf0rZ7r",
        "outputId": "6544fe67-c2bd-4555-e1e8-c134f166a36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_training/training_manifest.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2587235659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load training config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TRAINING_DIR}/training_manifest.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmanifest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_training/training_manifest.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "\n",
        "# Check hifigan_training folder\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "if os.path.exists(TRAINING_DIR):\n",
        "    print(\"Files in hifigan_training:\")\n",
        "    for f in sorted(os.listdir(TRAINING_DIR))[:20]:\n",
        "        print(f\"  {f}\")\n",
        "else:\n",
        "    print(\"hifigan_training folder not found\")\n",
        "\n",
        "# Check what folders exist\n",
        "print(\"\\nMain project folders:\")\n",
        "for f in sorted(os.listdir(PROJECT_ROOT)):\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ciZrLvrt9Y",
        "outputId": "efc506a8-386e-48f0-a17f-9590057e5563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in hifigan_training:\n",
            "  metadata.json\n",
            "  train_files.txt\n",
            "  units\n",
            "  val_files.txt\n",
            "  wavs\n",
            "\n",
            "Main project folders:\n",
            "  acts12.mp3\n",
            "  converted_audio\n",
            "  hifigan_model\n",
            "  hifigan_training\n",
            "  phase2_output\n",
            "  raw_audio\n",
            "  satere_motifs\n",
            "  satere_units\n",
            "  soundscript_deploy\n",
            "  soundscript_deploy.zip\n",
            "  soundscript_full\n",
            "  vocoder_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(f\"{PROJECT_ROOT}/hifigan_training/metadata.json\", \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(\"Keys:\", metadata.keys())\n",
        "print(\"\\nFirst few entries:\")\n",
        "if isinstance(metadata, dict):\n",
        "    for k, v in list(metadata.items())[:3]:\n",
        "        print(f\"  {k}: {v}\")\n",
        "elif isinstance(metadata, list):\n",
        "    for item in metadata[:3]:\n",
        "        print(f\"  {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lLH-98pr-Pp",
        "outputId": "7f239355-3f1c-4f0b-cb30-dd9af1dffa94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['num_train', 'num_val', 'segment_length_sec', 'sample_rate', 'num_units'])\n",
            "\n",
            "First few entries:\n",
            "  num_train: 22834\n",
            "  num_val: 100\n",
            "  segment_length_sec: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check train_files.txt format\n",
        "with open(f\"{PROJECT_ROOT}/hifigan_training/train_files.txt\", \"r\") as f:\n",
        "    lines = f.readlines()[:5]\n",
        "\n",
        "print(\"First 5 lines of train_files.txt:\")\n",
        "for line in lines:\n",
        "    print(f\"  {line.strip()}\")\n",
        "\n",
        "# Check the units and wavs folders\n",
        "print(\"\\nSample files in units/:\")\n",
        "units_dir = f\"{PROJECT_ROOT}/hifigan_training/units\"\n",
        "for f in sorted(os.listdir(units_dir))[:3]:\n",
        "    print(f\"  {f}\")\n",
        "\n",
        "print(\"\\nSample files in wavs/:\")\n",
        "wavs_dir = f\"{PROJECT_ROOT}/hifigan_training/wavs\"\n",
        "for f in sorted(os.listdir(wavs_dir))[:3]:\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cZRAK6BsJQv",
        "outputId": "c7fac9bc-3932-4a21-9709-77b418099d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 lines of train_files.txt:\n",
            "  seg_18430\n",
            "  seg_02412\n",
            "  seg_17923\n",
            "  seg_01742\n",
            "  seg_17771\n",
            "\n",
            "Sample files in units/:\n",
            "  seg_00000.txt\n",
            "  seg_00001.txt\n",
            "  seg_00002.txt\n",
            "\n",
            "Sample files in wavs/:\n",
            "  seg_00000.wav\n",
            "  seg_00001.wav\n",
            "  seg_00002.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unit file format\n",
        "with open(f\"{PROJECT_ROOT}/hifigan_training/units/seg_00000.txt\", \"r\") as f:\n",
        "    content = f.read().strip()\n",
        "print(f\"Unit file content (first 200 chars):\\n{content[:200]}\")\n",
        "print(f\"\\nTotal length: {len(content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Swjdu-sYxs",
        "outputId": "8edbfe2b-454c-4747-e460-e0c0042a4743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit file content (first 200 chars):\n",
            "26 26 26 26 26 49 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 49 30 98 98 88 88 88 91 91 91 69 46 46 46 66 8 72 23 58 49 99 56 93 18 16 16 10 10 10 51 90 6 6 6 6 6 84 27 62 62 62 62 62 62 52 52\n",
            "\n",
            "Total length: 734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{TRAINING_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{TRAINING_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS ============\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, dilations):\n",
        "        super().__init__()\n",
        "        self.convs1 = nn.ModuleList()\n",
        "        self.convs2 = nn.ModuleList()\n",
        "        for d in dilations:\n",
        "            self.convs1.append(nn.Conv1d(channels, channels, kernel_size, dilation=d, padding=(kernel_size*d-d)//2))\n",
        "            self.convs2.append(nn.Conv1d(channels, channels, kernel_size, dilation=1, padding=(kernel_size-1)//2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.1)\n",
        "            xt = F.leaky_relu(c1(xt), 0.1)\n",
        "            xt = c2(xt)\n",
        "            x = x + xt\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, unit_embed_dim=256, upsample_rates=[8,8,4,2],\n",
        "                 upsample_kernels=[16,16,8,4], resblock_kernels=[3,7,11], resblock_dilations=[[1,3,5],[1,3,5],[1,3,5]]):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_units, unit_embed_dim)\n",
        "        self.conv_pre = nn.Conv1d(unit_embed_dim, 256, 7, padding=3)\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.resblocks = nn.ModuleList()\n",
        "\n",
        "        ch = 256\n",
        "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernels)):\n",
        "            self.ups.append(nn.ConvTranspose1d(ch, ch//2, k, u, padding=(k-u)//2))\n",
        "            ch = ch//2\n",
        "            for j, (rk, rd) in enumerate(zip(resblock_kernels, resblock_dilations)):\n",
        "                self.resblocks.append(ResBlock(ch, rk, rd))\n",
        "\n",
        "        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)\n",
        "        self.num_upsamples = len(upsample_rates)\n",
        "        self.num_kernels = len(resblock_kernels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).transpose(1, 2)\n",
        "        x = self.conv_pre(x)\n",
        "        for i, up in enumerate(self.ups):\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            x = up(x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(1, 16, 15, padding=7),\n",
        "            nn.Conv1d(16, 64, 41, stride=4, padding=20, groups=4),\n",
        "            nn.Conv1d(64, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.Conv1d(256, 1024, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(1024, 1024, 41, stride=4, padding=20, groups=256),\n",
        "            nn.Conv1d(1024, 1024, 5, padding=2),\n",
        "        ])\n",
        "        self.conv_post = nn.Conv1d(1024, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmaps = []\n",
        "        for conv in self.convs:\n",
        "            x = F.leaky_relu(conv(x), 0.1)\n",
        "            fmaps.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmaps.append(x)\n",
        "        return x.flatten(1, -1), fmaps\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "        self.sample_rate = 16000\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_list[idx]\n",
        "\n",
        "        # Load units from text file\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "\n",
        "        # Load audio from wav file\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Segment\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch25.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"\\n✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER ============\n",
        "dataset = UnitAudioDataset(train_files, TRAINING_DIR)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, TRAINING_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        # Generate\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_out, _ = discriminator(audio.unsqueeze(1))\n",
        "        fake_out, _ = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "        d_loss = torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_out, fake_fmaps = discriminator(audio_fake.unsqueeze(1))\n",
        "        _, real_fmaps = discriminator(audio.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = torch.mean((fake_out - 1)**2)\n",
        "        g_loss_fm = sum(torch.mean(torch.abs(rf - ff)) for rf, ff in zip(real_fmaps, fake_fmaps))\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_fm * 2 + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Save sample every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "IYbdUMVCsjLj",
        "outputId": "b2166197-ecca-430b-8f85-2661fcab5794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"conv_pre.weight\", \"conv_pre.bias\", \"ups.0.weight\", \"ups.0.bias\", \"ups.1.weight\", \"ups.1.bias\", \"ups.2.weight\", \"ups.2.bias\", \"ups.3.weight\", \"ups.3.bias\", \"resblocks.0.convs1.0.weight\", \"resblocks.0.convs1.0.bias\", \"resblocks.0.convs1.1.weight\", \"resblocks.0.convs1.1.bias\", \"resblocks.0.convs1.2.weight\", \"resblocks.0.convs1.2.bias\", \"resblocks.0.convs2.0.weight\", \"resblocks.0.convs2.0.bias\", \"resblocks.0.convs2.1.weight\", \"resblocks.0.convs2.1.bias\", \"resblocks.0.convs2.2.weight\", \"resblocks.0.convs2.2.bias\", \"resblocks.1.convs1.0.weight\", \"resblocks.1.convs1.0.bias\", \"resblocks.1.convs1.1.weight\", \"resblocks.1.convs1.1.bias\", \"resblocks.1.convs1.2.weight\", \"resblocks.1.convs1.2.bias\", \"resblocks.1.convs2.0.weight\", \"resblocks.1.convs2.0.bias\", \"resblocks.1.convs2.1.weight\", \"resblocks.1.convs2.1.bias\", \"resblocks.1.convs2.2.weight\", \"resblocks.1.convs2.2.bias\", \"resblocks.2.convs1.0.weight\", \"resblocks.2.convs1.0.bias\", \"resblocks.2.convs1.1.weight\", \"resblocks.2.convs1.1.bias\", \"resblocks.2.convs1.2.weight\", \"resblocks.2.convs1.2.bias\", \"resblocks.2.convs2.0.weight\", \"resblocks.2.convs2.0.bias\", \"resblocks.2.convs2.1.weight\", \"resblocks.2.convs2.1.bias\", \"resblocks.2.convs2.2.weight\", \"resblocks.2.convs2.2.bias\", \"resblocks.3.convs1.0.weight\", \"resblocks.3.convs1.0.bias\", \"resblocks.3.convs1.1.weight\", \"resblocks.3.convs1.1.bias\", \"resblocks.3.convs1.2.weight\", \"resblocks.3.convs1.2.bias\", \"resblocks.3.convs2.0.weight\", \"resblocks.3.convs2.0.bias\", \"resblocks.3.convs2.1.weight\", \"resblocks.3.convs2.1.bias\", \"resblocks.3.convs2.2.weight\", \"resblocks.3.convs2.2.bias\", \"resblocks.4.convs1.0.weight\", \"resblocks.4.convs1.0.bias\", \"resblocks.4.convs1.1.weight\", \"resblocks.4.convs1.1.bias\", \"resblocks.4.convs1.2.weight\", \"resblocks.4.convs1.2.bias\", \"resblocks.4.convs2.0.weight\", \"resblocks.4.convs2.0.bias\", \"resblocks.4.convs2.1.weight\", \"resblocks.4.convs2.1.bias\", \"resblocks.4.convs2.2.weight\", \"resblocks.4.convs2.2.bias\", \"resblocks.5.convs1.0.weight\", \"resblocks.5.convs1.0.bias\", \"resblocks.5.convs1.1.weight\", \"resblocks.5.convs1.1.bias\", \"resblocks.5.convs1.2.weight\", \"resblocks.5.convs1.2.bias\", \"resblocks.5.convs2.0.weight\", \"resblocks.5.convs2.0.bias\", \"resblocks.5.convs2.1.weight\", \"resblocks.5.convs2.1.bias\", \"resblocks.5.convs2.2.weight\", \"resblocks.5.convs2.2.bias\", \"resblocks.6.convs1.0.weight\", \"resblocks.6.convs1.0.bias\", \"resblocks.6.convs1.1.weight\", \"resblocks.6.convs1.1.bias\", \"resblocks.6.convs1.2.weight\", \"resblocks.6.convs1.2.bias\", \"resblocks.6.convs2.0.weight\", \"resblocks.6.convs2.0.bias\", \"resblocks.6.convs2.1.weight\", \"resblocks.6.convs2.1.bias\", \"resblocks.6.convs2.2.weight\", \"resblocks.6.convs2.2.bias\", \"resblocks.7.convs1.0.weight\", \"resblocks.7.convs1.0.bias\", \"resblocks.7.convs1.1.weight\", \"resblocks.7.convs1.1.bias\", \"resblocks.7.convs1.2.weight\", \"resblocks.7.convs1.2.bias\", \"resblocks.7.convs2.0.weight\", \"resblocks.7.convs2.0.bias\", \"resblocks.7.convs2.1.weight\", \"resblocks.7.convs2.1.bias\", \"resblocks.7.convs2.2.weight\", \"resblocks.7.convs2.2.bias\", \"resblocks.8.convs1.0.weight\", \"resblocks.8.convs1.0.bias\", \"resblocks.8.convs1.1.weight\", \"resblocks.8.convs1.1.bias\", \"resblocks.8.convs1.2.weight\", \"resblocks.8.convs1.2.bias\", \"resblocks.8.convs2.0.weight\", \"resblocks.8.convs2.0.bias\", \"resblocks.8.convs2.1.weight\", \"resblocks.8.convs2.1.bias\", \"resblocks.8.convs2.2.weight\", \"resblocks.8.convs2.2.bias\", \"resblocks.9.convs1.0.weight\", \"resblocks.9.convs1.0.bias\", \"resblocks.9.convs1.1.weight\", \"resblocks.9.convs1.1.bias\", \"resblocks.9.convs1.2.weight\", \"resblocks.9.convs1.2.bias\", \"resblocks.9.convs2.0.weight\", \"resblocks.9.convs2.0.bias\", \"resblocks.9.convs2.1.weight\", \"resblocks.9.convs2.1.bias\", \"resblocks.9.convs2.2.weight\", \"resblocks.9.convs2.2.bias\", \"resblocks.10.convs1.0.weight\", \"resblocks.10.convs1.0.bias\", \"resblocks.10.convs1.1.weight\", \"resblocks.10.convs1.1.bias\", \"resblocks.10.convs1.2.weight\", \"resblocks.10.convs1.2.bias\", \"resblocks.10.convs2.0.weight\", \"resblocks.10.convs2.0.bias\", \"resblocks.10.convs2.1.weight\", \"resblocks.10.convs2.1.bias\", \"resblocks.10.convs2.2.weight\", \"resblocks.10.convs2.2.bias\", \"resblocks.11.convs1.0.weight\", \"resblocks.11.convs1.0.bias\", \"resblocks.11.convs1.1.weight\", \"resblocks.11.convs1.1.bias\", \"resblocks.11.convs1.2.weight\", \"resblocks.11.convs1.2.bias\", \"resblocks.11.convs2.0.weight\", \"resblocks.11.convs2.0.bias\", \"resblocks.11.convs2.1.weight\", \"resblocks.11.convs2.1.bias\", \"resblocks.11.convs2.2.weight\", \"resblocks.11.convs2.2.bias\", \"conv_post.weight\", \"conv_post.bias\". \n\tUnexpected key(s) in state_dict: \"unit_embed.weight\", \"pre_conv.weight\", \"pre_conv.bias\", \"upsamples.0.0.weight\", \"upsamples.0.0.bias\", \"upsamples.0.2.weight\", \"upsamples.0.2.bias\", \"upsamples.1.0.weight\", \"upsamples.1.0.bias\", \"upsamples.1.2.weight\", \"upsamples.1.2.bias\", \"upsamples.2.0.weight\", \"upsamples.2.0.bias\", \"upsamples.2.2.weight\", \"upsamples.2.2.bias\", \"upsamples.3.0.weight\", \"upsamples.3.0.bias\", \"upsamples.3.2.weight\", \"upsamples.3.2.bias\", \"upsamples.4.0.weight\", \"upsamples.4.0.bias\", \"upsamples.4.2.weight\", \"upsamples.4.2.bias\", \"post_conv.weight\", \"post_conv.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3458207211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'discriminator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_g_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"conv_pre.weight\", \"conv_pre.bias\", \"ups.0.weight\", \"ups.0.bias\", \"ups.1.weight\", \"ups.1.bias\", \"ups.2.weight\", \"ups.2.bias\", \"ups.3.weight\", \"ups.3.bias\", \"resblocks.0.convs1.0.weight\", \"resblocks.0.convs1.0.bias\", \"resblocks.0.convs1.1.weight\", \"resblocks.0.convs1.1.bias\", \"resblocks.0.convs1.2.weight\", \"resblocks.0.convs1.2.bias\", \"resblocks.0.convs2.0.weight\", \"resblocks.0.convs2.0.bias\", \"resblocks.0.convs2.1.weight\", \"resblocks.0.convs2.1.bias\", \"resblocks.0.convs2.2.weight\", \"resblocks.0.convs2.2.bias\", \"resblocks.1.convs1.0.weight\", \"resblocks.1.convs1.0.bias\", \"resblocks.1.convs1.1.weight\", \"resblocks.1.convs1.1.bias\", \"resblocks.1.convs1.2.weight\", \"resblocks.1.convs1.2.bias\", \"resblocks.1.convs2.0.weight\", \"resblocks.1.convs2.0.bias\", \"resblocks.1.convs2.1.weight\", \"resblocks.1.convs2.1.bias\", \"resblocks.1.convs2.2.weight\", \"resblocks.1.convs2.2.bias\", \"resblocks.2.convs1.0.weight\", \"resblocks.2.convs1.0.bias\", \"resblocks.2.convs1.1.weight\", \"resblocks.2.convs1.1.bias\", \"resblocks.2.convs1.2.weight\", \"resblocks.2.convs1.2.bias\", \"resblocks.2.convs2.0.weight\", \"resblocks.2.convs2.0.bias\", \"resblocks.2.convs2.1.weight\", \"resblocks.2.convs2.1.bias\", \"resblocks.2.convs2.2.weight\", \"resblocks.2.convs2.2.bias\", \"resblocks.3.convs1.0.weight\", \"resblocks.3.convs1.0.bias\", \"resblocks.3.convs1.1.weight\", \"resblocks.3.convs1.1.bias\", \"resblocks.3.convs1.2.weight\", \"resblocks.3.convs1.2.bias\", \"resblocks.3.convs2.0.weight\",...\n\tUnexpected key(s) in state_dict: \"unit_embed.weight\", \"pre_conv.weight\", \"pre_conv.bias\", \"upsamples.0.0.weight\", \"upsamples.0.0.bias\", \"upsamples.0.2.weight\", \"upsamples.0.2.bias\", \"upsamples.1.0.weight\", \"upsamples.1.0.bias\", \"upsamples.1.2.weight\", \"upsamples.1.2.bias\", \"upsamples.2.0.weight\", \"upsamples.2.0.bias\", \"upsamples.2.2.weight\", \"upsamples.2.2.bias\", \"upsamples.3.0.weight\", \"upsamples.3.0.bias\", \"upsamples.3.2.weight\", \"upsamples.3.2.bias\", \"upsamples.4.0.weight\", \"upsamples.4.0.bias\", \"upsamples.4.2.weight\", \"upsamples.4.2.bias\", \"post_conv.weight\", \"post_conv.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the checkpoint structure\n",
        "checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch25.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "print(\"Checkpoint keys:\")\n",
        "for key in checkpoint.keys():\n",
        "    print(f\"  {key}\")\n",
        "\n",
        "print(\"\\nGenerator state_dict keys (first 20):\")\n",
        "gen_keys = list(checkpoint['generator_state_dict'].keys())\n",
        "for key in gen_keys[:20]:\n",
        "    print(f\"  {key}\")\n",
        "\n",
        "print(f\"\\n... total {len(gen_keys)} keys\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fneYk26es0HB",
        "outputId": "c755dc25-fa02-4089-c8db-a4278e7d2628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint keys:\n",
            "  epoch\n",
            "  generator_state_dict\n",
            "  discriminator_state_dict\n",
            "  optimizer_g_state_dict\n",
            "  optimizer_d_state_dict\n",
            "\n",
            "Generator state_dict keys (first 20):\n",
            "  unit_embed.weight\n",
            "  pre_conv.weight\n",
            "  pre_conv.bias\n",
            "  upsamples.0.0.weight\n",
            "  upsamples.0.0.bias\n",
            "  upsamples.0.2.weight\n",
            "  upsamples.0.2.bias\n",
            "  upsamples.1.0.weight\n",
            "  upsamples.1.0.bias\n",
            "  upsamples.1.2.weight\n",
            "  upsamples.1.2.bias\n",
            "  upsamples.2.0.weight\n",
            "  upsamples.2.0.bias\n",
            "  upsamples.2.2.weight\n",
            "  upsamples.2.2.bias\n",
            "  upsamples.3.0.weight\n",
            "  upsamples.3.0.bias\n",
            "  upsamples.3.2.weight\n",
            "  upsamples.3.2.bias\n",
            "  upsamples.4.0.weight\n",
            "\n",
            "... total 25 keys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check tensor shapes to understand the architecture\n",
        "print(\"Generator weights shapes:\")\n",
        "for key, value in checkpoint['generator_state_dict'].items():\n",
        "    print(f\"  {key}: {value.shape}\")\n",
        "\n",
        "print(\"\\nDiscriminator weights shapes:\")\n",
        "for key, value in checkpoint['discriminator_state_dict'].items():\n",
        "    print(f\"  {key}: {value.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkQj5WZatH6H",
        "outputId": "32f56b64-3e34-4394-938c-eb75a828c28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator weights shapes:\n",
            "  unit_embed.weight: torch.Size([100, 256])\n",
            "  pre_conv.weight: torch.Size([512, 256, 7])\n",
            "  pre_conv.bias: torch.Size([512])\n",
            "  upsamples.0.0.weight: torch.Size([512, 256, 10])\n",
            "  upsamples.0.0.bias: torch.Size([256])\n",
            "  upsamples.0.2.weight: torch.Size([256, 256, 7])\n",
            "  upsamples.0.2.bias: torch.Size([256])\n",
            "  upsamples.1.0.weight: torch.Size([256, 128, 10])\n",
            "  upsamples.1.0.bias: torch.Size([128])\n",
            "  upsamples.1.2.weight: torch.Size([128, 128, 7])\n",
            "  upsamples.1.2.bias: torch.Size([128])\n",
            "  upsamples.2.0.weight: torch.Size([128, 64, 8])\n",
            "  upsamples.2.0.bias: torch.Size([64])\n",
            "  upsamples.2.2.weight: torch.Size([64, 64, 7])\n",
            "  upsamples.2.2.bias: torch.Size([64])\n",
            "  upsamples.3.0.weight: torch.Size([64, 32, 8])\n",
            "  upsamples.3.0.bias: torch.Size([32])\n",
            "  upsamples.3.2.weight: torch.Size([32, 32, 7])\n",
            "  upsamples.3.2.bias: torch.Size([32])\n",
            "  upsamples.4.0.weight: torch.Size([32, 64, 4])\n",
            "  upsamples.4.0.bias: torch.Size([64])\n",
            "  upsamples.4.2.weight: torch.Size([64, 64, 7])\n",
            "  upsamples.4.2.bias: torch.Size([64])\n",
            "  post_conv.weight: torch.Size([1, 64, 7])\n",
            "  post_conv.bias: torch.Size([1])\n",
            "\n",
            "Discriminator weights shapes:\n",
            "  discriminators.0.0.weight: torch.Size([64, 1, 15])\n",
            "  discriminators.0.0.bias: torch.Size([64])\n",
            "  discriminators.0.2.weight: torch.Size([128, 16, 41])\n",
            "  discriminators.0.2.bias: torch.Size([128])\n",
            "  discriminators.0.4.weight: torch.Size([256, 8, 41])\n",
            "  discriminators.0.4.bias: torch.Size([256])\n",
            "  discriminators.0.6.weight: torch.Size([512, 16, 41])\n",
            "  discriminators.0.6.bias: torch.Size([512])\n",
            "  discriminators.0.8.weight: torch.Size([512, 512, 5])\n",
            "  discriminators.0.8.bias: torch.Size([512])\n",
            "  discriminators.0.10.weight: torch.Size([1, 512, 3])\n",
            "  discriminators.0.10.bias: torch.Size([1])\n",
            "  discriminators.1.0.weight: torch.Size([64, 1, 15])\n",
            "  discriminators.1.0.bias: torch.Size([64])\n",
            "  discriminators.1.2.weight: torch.Size([128, 16, 41])\n",
            "  discriminators.1.2.bias: torch.Size([128])\n",
            "  discriminators.1.4.weight: torch.Size([256, 8, 41])\n",
            "  discriminators.1.4.bias: torch.Size([256])\n",
            "  discriminators.1.6.weight: torch.Size([512, 16, 41])\n",
            "  discriminators.1.6.bias: torch.Size([512])\n",
            "  discriminators.1.8.weight: torch.Size([512, 512, 5])\n",
            "  discriminators.1.8.bias: torch.Size([512])\n",
            "  discriminators.1.10.weight: torch.Size([1, 512, 3])\n",
            "  discriminators.1.10.bias: torch.Size([1])\n",
            "  discriminators.2.0.weight: torch.Size([64, 1, 15])\n",
            "  discriminators.2.0.bias: torch.Size([64])\n",
            "  discriminators.2.2.weight: torch.Size([128, 16, 41])\n",
            "  discriminators.2.2.bias: torch.Size([128])\n",
            "  discriminators.2.4.weight: torch.Size([256, 8, 41])\n",
            "  discriminators.2.4.bias: torch.Size([256])\n",
            "  discriminators.2.6.weight: torch.Size([512, 16, 41])\n",
            "  discriminators.2.6.bias: torch.Size([512])\n",
            "  discriminators.2.8.weight: torch.Size([512, 512, 5])\n",
            "  discriminators.2.8.bias: torch.Size([512])\n",
            "  discriminators.2.10.weight: torch.Size([1, 512, 3])\n",
            "  discriminators.2.10.bias: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{TRAINING_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{TRAINING_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS (matching checkpoint) ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        # 5 upsample blocks: each has ConvTranspose + LeakyReLU + Conv + LeakyReLU\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)  # (B, embed_dim, T)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 3 sub-discriminators\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator()\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(4, 2, padding=2),\n",
        "            nn.AvgPool1d(16, 8, padding=8)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, 41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, 41, stride=4, padding=20, groups=8),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, 5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_list[idx]\n",
        "\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch25.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"\\n✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER ============\n",
        "dataset = UnitAudioDataset(train_files, TRAINING_DIR)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, TRAINING_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        # Generate\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator update\n",
        "        optimizer_d.zero_grad()\n",
        "        real_outs = discriminator(audio.unsqueeze(1))\n",
        "        fake_outs = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        d_loss = 0\n",
        "        for real_out, fake_out in zip(real_outs, fake_outs):\n",
        "            d_loss += torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator update\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outs = discriminator(audio_fake.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = 0\n",
        "        for fake_out in fake_outs:\n",
        "            g_loss_adv += torch.mean((fake_out - 1)**2)\n",
        "\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Save sample every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "Z49_X1ndtP0o",
        "outputId": "f1b83a7e-9f0e-461c-f779-11200718088d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Discriminator:\n\tsize mismatch for discriminators.0.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41]).\n\tsize mismatch for discriminators.1.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41]).\n\tsize mismatch for discriminators.2.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3547005736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'discriminator_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_g_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_d_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Discriminator:\n\tsize mismatch for discriminators.0.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41]).\n\tsize mismatch for discriminators.1.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41]).\n\tsize mismatch for discriminators.2.4.weight: copying a param with shape torch.Size([256, 8, 41]) from checkpoint, the shape in current model is torch.Size([256, 16, 41])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{TRAINING_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{TRAINING_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS (matching checkpoint) ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 3 sub-discriminators with correct group sizes from checkpoint\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator()\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(4, 2, padding=2),\n",
        "            nn.AvgPool1d(16, 8, padding=8)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self):\n",
        "        # Groups: 64->128 (groups=4, 64/4=16 in, 128/4=32 out per group)\n",
        "        # But checkpoint shows: [128, 16, 41] meaning in_ch=16*groups\n",
        "        # 64 in, 128 out, groups=4 means each group: 16 in -> 32 out\n",
        "        # [256, 8, 41] means 8 input channels per group\n",
        "        # 128 in, groups where 128/groups=8, so groups=16\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, 41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, 41, stride=4, padding=20, groups=16),  # Fixed: groups=16\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, 5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_list[idx]\n",
        "\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch25.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "print(f\"\\n✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER ============\n",
        "dataset = UnitAudioDataset(train_files, TRAINING_DIR)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, TRAINING_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_outs = discriminator(audio.unsqueeze(1))\n",
        "        fake_outs = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        d_loss = 0\n",
        "        for real_out, fake_out in zip(real_outs, fake_outs):\n",
        "            d_loss += torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outs = discriminator(audio_fake.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = 0\n",
        "        for fake_out in fake_outs:\n",
        "            g_loss_adv += torch.mean((fake_out - 1)**2)\n",
        "\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "E4l24c2atiUn",
        "outputId": "f20bcb7e-3bae-499c-e879-9738f16bdee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n",
            "\n",
            "✓ Resumed from epoch 24\n",
            "  Starting at epoch 25\n",
            "✓ DataLoader ready: 1428 batches\n",
            "\n",
            "============================================================\n",
            "RESUMING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50:  54%|█████▍    | 768/1428 [2:07:24<1:49:29,  9.95s/it, G_loss=5.567, D_loss=0.089]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1560527426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Y7ReM9w3t1WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4VWX1SfKRiL",
        "outputId": "8bfcd55c-1cba-4616-a6c9-4e495308e414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 19 16:52:39 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0             32W /   70W |    2188MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{TRAINING_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{TRAINING_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator()\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(4, 2, padding=2),\n",
        "            nn.AvgPool1d(16, 8, padding=8)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, 41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, 5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# ============ DATASET WITH CACHING ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384, cache_in_memory=False):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "        self.cache = {}\n",
        "        self.cache_in_memory = cache_in_memory\n",
        "\n",
        "        # Pre-load a subset to warm up\n",
        "        if cache_in_memory:\n",
        "            print(\"Pre-loading data into memory...\")\n",
        "            for i, file_id in enumerate(tqdm(file_list[:5000], desc=\"Caching\")):\n",
        "                self._load_and_cache(i, file_id)\n",
        "\n",
        "    def _load_and_cache(self, idx, file_id):\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "        self.cache[idx] = (units, audio)\n",
        "        return units, audio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx in self.cache:\n",
        "            units, audio = self.cache[idx]\n",
        "        else:\n",
        "            file_id = self.file_list[idx]\n",
        "            with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "                units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "            sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Find best checkpoint\n",
        "checkpoints = [f for f in os.listdir(MODEL_DIR) if f.startswith('checkpoint_epoch')]\n",
        "if checkpoints:\n",
        "    epochs = [int(f.split('epoch')[1].split('.')[0]) for f in checkpoints]\n",
        "    best_epoch = max(epochs)\n",
        "    checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch{best_epoch}.pt\"\n",
        "    print(f\"Found checkpoint: epoch {best_epoch}\")\n",
        "else:\n",
        "    checkpoint_path = None\n",
        "    print(\"No checkpoint found, starting fresh\")\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    start_epoch = 1\n",
        "\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER - OPTIMIZED ============\n",
        "dataset = UnitAudioDataset(train_files, TRAINING_DIR, cache_in_memory=False)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Avoid multiprocessing overhead with Drive\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, TRAINING_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# Warm up GPU\n",
        "print(\"Warming up GPU...\")\n",
        "dummy = torch.randn(16, 32).long().to(device)\n",
        "_ = generator(dummy)\n",
        "print(\"✓ GPU ready\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device, non_blocking=True)\n",
        "        audio = audio.to(device, non_blocking=True)\n",
        "\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_outs = discriminator(audio.unsqueeze(1))\n",
        "        fake_outs = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        d_loss = 0\n",
        "        for real_out, fake_out in zip(real_outs, fake_outs):\n",
        "            d_loss += torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outs = discriminator(audio_fake.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = 0\n",
        "        for fake_out in fake_outs:\n",
        "            g_loss_adv += torch.mean((fake_out - 1)**2)\n",
        "\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "Bp_TPWUzLD4Z",
        "outputId": "bed2e106-7175-4b42-aba6-8f905282667b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n",
            "Found checkpoint: epoch 25\n",
            "✓ Resumed from epoch 24\n",
            "  Starting at epoch 25\n",
            "✓ DataLoader ready: 1428 batches\n",
            "Warming up GPU...\n",
            "✓ GPU ready\n",
            "\n",
            "============================================================\n",
            "RESUMING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50:   0%|          | 0/1428 [00:10<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-644435023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Clear any previous CUDA errors\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"\n",
        "TRAINING_DIR = f\"{PROJECT_ROOT}/hifigan_training\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{TRAINING_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{TRAINING_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator()\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(4, 2, padding=2),\n",
        "            nn.AvgPool1d(16, 8, padding=8)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, 41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, 5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_list[idx]\n",
        "\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Find best checkpoint\n",
        "checkpoints = [f for f in os.listdir(MODEL_DIR) if f.startswith('checkpoint_epoch')]\n",
        "if checkpoints:\n",
        "    epochs = [int(f.split('epoch')[1].split('.')[0]) for f in checkpoints]\n",
        "    best_epoch = max(epochs)\n",
        "    checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch{best_epoch}.pt\"\n",
        "    print(f\"Found checkpoint: epoch {best_epoch}\")\n",
        "else:\n",
        "    checkpoint_path = None\n",
        "    print(\"No checkpoint found\")\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    start_epoch = 1\n",
        "\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER ============\n",
        "dataset = UnitAudioDataset(train_files, TRAINING_DIR)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, TRAINING_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_outs = discriminator(audio.unsqueeze(1))\n",
        "        fake_outs = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        d_loss = 0\n",
        "        for real_out, fake_out in zip(real_outs, fake_outs):\n",
        "            d_loss += torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outs = discriminator(audio_fake.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = 0\n",
        "        for fake_out in fake_outs:\n",
        "            g_loss_adv += torch.mean((fake_out - 1)**2)\n",
        "\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio: {MODEL_DIR}/sample_epoch{epoch}.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "YvGMuGKHLZBa",
        "outputId": "a798ea52-fa9a-45d8-8cf6-44307ded1cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n",
            "Found checkpoint: epoch 25\n",
            "✓ Resumed from epoch 24\n",
            "  Starting at epoch 25\n",
            "✓ DataLoader ready: 1428 batches\n",
            "\n",
            "============================================================\n",
            "RESUMING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50:  23%|██▎       | 328/1428 [50:08<2:48:09,  9.17s/it, G_loss=4.624, D_loss=0.063]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1105363556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1105363556.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.training_dir}/wavs/{file_id}.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32768.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_big_endian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_rf64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf64_chunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0mfmt_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0mdata_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mstr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'RIFF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mis_rf64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy training data to local Colab storage (much faster)\n",
        "LOCAL_DIR = \"/content/hifigan_local\"\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/satere_project/hifigan_training\"\n",
        "\n",
        "if not os.path.exists(LOCAL_DIR):\n",
        "    print(\"Copying training data to local storage...\")\n",
        "    print(\"This will take 5-10 minutes but training will be MUCH faster.\")\n",
        "    shutil.copytree(DRIVE_DIR, LOCAL_DIR)\n",
        "    print(f\"✓ Copied to {LOCAL_DIR}\")\n",
        "else:\n",
        "    print(f\"✓ Local data already exists at {LOCAL_DIR}\")\n",
        "\n",
        "print(f\"\\nLocal files: {len(os.listdir(f'{LOCAL_DIR}/units'))} unit files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "BvaFLhG5L4Ri",
        "outputId": "f8ef3f4c-4ec0-40fe-ea41-31f1f4968175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying training data to local storage...\n",
            "This will take 5-10 minutes but training will be MUCH faster.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_training'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3471820584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Copying training data to local storage...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This will take 5-10 minutes but training will be MUCH faster.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDRIVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOCAL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ Copied to {LOCAL_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutil.copytree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/satere_project/hifigan_training'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "RgeDQO-qCpDD",
        "outputId": "6f65a4a2-bda7-4f19-f86e-127fab827621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths - LOCAL storage for training data, DRIVE for saving checkpoints\n",
        "LOCAL_DIR = \"/content/hifigan_local\"  # Fast local storage\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model\"  # Save checkpoints to Drive\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists from local\n",
        "with open(f\"{LOCAL_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{LOCAL_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ MODEL DEFINITIONS ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1, output_padding=0),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator(),\n",
        "            self._make_discriminator()\n",
        "        ])\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.Identity(),\n",
        "            nn.AvgPool1d(4, 2, padding=2),\n",
        "            nn.AvgPool1d(16, 8, padding=8)\n",
        "        ])\n",
        "\n",
        "    def _make_discriminator(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 15, padding=7),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(64, 128, 41, stride=4, padding=20, groups=4),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(128, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 512, 5, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(512, 1, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for pool, disc in zip(self.pools, self.discriminators):\n",
        "            x_pooled = pool(x)\n",
        "            outputs.append(disc(x_pooled))\n",
        "        return outputs\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, file_list, training_dir, segment_length=16384):\n",
        "        self.file_list = file_list\n",
        "        self.training_dir = training_dir\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_list[idx]\n",
        "\n",
        "        with open(f\"{self.training_dir}/units/{file_id}.txt\", \"r\") as f:\n",
        "            units = np.array([int(x) for x in f.read().strip().split()], dtype=np.int64)\n",
        "\n",
        "        sr, audio = wav.read(f\"{self.training_dir}/wavs/{file_id}.wav\")\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        unit_len = self.segment_length // 512\n",
        "        if len(units) > unit_len:\n",
        "            start = np.random.randint(0, len(units) - unit_len)\n",
        "            units = units[start:start+unit_len]\n",
        "            audio_start = start * 512\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "\n",
        "        if len(audio) < self.segment_length:\n",
        "            audio = np.pad(audio, (0, self.segment_length - len(audio)))\n",
        "        if len(units) < unit_len:\n",
        "            units = np.pad(units, (0, unit_len - len(units)))\n",
        "\n",
        "        return torch.LongTensor(units), torch.FloatTensor(audio)\n",
        "\n",
        "# ============ LOAD MODELS AND RESUME ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "\n",
        "# Find best checkpoint from Drive\n",
        "checkpoints = [f for f in os.listdir(MODEL_DIR) if f.startswith('checkpoint_epoch')]\n",
        "if checkpoints:\n",
        "    epochs = [int(f.split('epoch')[1].split('.')[0]) for f in checkpoints]\n",
        "    best_epoch = max(epochs)\n",
        "    checkpoint_path = f\"{MODEL_DIR}/checkpoint_epoch{best_epoch}.pt\"\n",
        "    print(f\"Found checkpoint: epoch {best_epoch}\")\n",
        "else:\n",
        "    checkpoint_path = None\n",
        "    print(\"No checkpoint found\")\n",
        "\n",
        "if checkpoint_path:\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"✓ Resumed from epoch {checkpoint['epoch']}\")\n",
        "else:\n",
        "    start_epoch = 1\n",
        "\n",
        "print(f\"  Starting at epoch {start_epoch}\")\n",
        "\n",
        "# ============ DATALOADER - using LOCAL storage ============\n",
        "dataset = UnitAudioDataset(train_files, LOCAL_DIR)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataset = UnitAudioDataset(val_files, LOCAL_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(dataloader)} batches\")\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMING TRAINING (using local storage)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for units, audio in pbar:\n",
        "        units = units.to(device)\n",
        "        audio = audio.to(device)\n",
        "\n",
        "        audio_fake = generator(units)\n",
        "\n",
        "        min_len = min(audio.shape[1], audio_fake.shape[1])\n",
        "        audio = audio[:, :min_len]\n",
        "        audio_fake = audio_fake[:, :min_len]\n",
        "\n",
        "        # Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_outs = discriminator(audio.unsqueeze(1))\n",
        "        fake_outs = discriminator(audio_fake.detach().unsqueeze(1))\n",
        "\n",
        "        d_loss = 0\n",
        "        for real_out, fake_out in zip(real_outs, fake_outs):\n",
        "            d_loss += torch.mean((real_out - 1)**2) + torch.mean(fake_out**2)\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outs = discriminator(audio_fake.unsqueeze(1))\n",
        "\n",
        "        g_loss_adv = 0\n",
        "        for fake_out in fake_outs:\n",
        "            g_loss_adv += torch.mean((fake_out - 1)**2)\n",
        "\n",
        "        g_loss_l1 = F.l1_loss(audio_fake, audio) * 45\n",
        "        g_loss = g_loss_adv + g_loss_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        pbar.set_postfix({'G_loss': f'{g_loss.item():.3f}', 'D_loss': f'{d_loss.item():.3f}'})\n",
        "\n",
        "    avg_g = epoch_g_loss / len(dataloader)\n",
        "    avg_d = epoch_d_loss / len(dataloader)\n",
        "    print(f\"\\n  Epoch {epoch} complete\")\n",
        "    print(f\"  Avg G Loss: {avg_g:.4f}, Avg D Loss: {avg_d:.4f}\")\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for units, audio in val_loader:\n",
        "                units, audio = units.to(device), audio.to(device)\n",
        "                fake = generator(units)\n",
        "                min_len = min(audio.shape[1], fake.shape[1])\n",
        "                val_loss += F.l1_loss(fake[:,:min_len], audio[:,:min_len]).item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"\\n  Validation L1 Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint to DRIVE every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint to Drive\")\n",
        "\n",
        "    # Save sample to DRIVE every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_units = torch.LongTensor(dataset[0][0]).unsqueeze(0).to(device)\n",
        "            sample_audio = generator(sample_units).squeeze().cpu().numpy()\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, (sample_audio * 32767).astype(np.int16))\n",
        "            print(f\"  ✓ Saved sample audio to Drive\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUY895wNXcQZ",
        "outputId": "3a50d39e-c212-4f5f-bc98-b06bba21358b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training files: 22834\n",
            "Validation files: 100\n",
            "Found checkpoint: epoch 25\n",
            "✓ Resumed from epoch 24\n",
            "  Starting at epoch 25\n",
            "✓ DataLoader ready: 1428 batches\n",
            "\n",
            "============================================================\n",
            "RESUMING TRAINING (using local storage)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=7.787, D_loss=0.012]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 25 complete\n",
            "  Avg G Loss: 5.5681, Avg D Loss: 0.1430\n",
            "  ✓ Saved checkpoint to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=7.085, D_loss=0.036]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 26 complete\n",
            "  Avg G Loss: 5.6244, Avg D Loss: 0.0634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.12it/s, G_loss=8.098, D_loss=0.019]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 27 complete\n",
            "  Avg G Loss: 5.6361, Avg D Loss: 0.0563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=5.139, D_loss=0.059]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 28 complete\n",
            "  Avg G Loss: 5.5948, Avg D Loss: 0.0505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=4.438, D_loss=0.020]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 29 complete\n",
            "  Avg G Loss: 5.5855, Avg D Loss: 0.0866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 1428/1428 [07:35<00:00,  3.13it/s, G_loss=6.537, D_loss=0.011]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 30 complete\n",
            "  Avg G Loss: 5.6172, Avg D Loss: 0.0471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0735\n",
            "  ✓ Saved checkpoint to Drive\n",
            "  ✓ Saved sample audio to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 1428/1428 [07:35<00:00,  3.14it/s, G_loss=5.447, D_loss=0.015]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 31 complete\n",
            "  Avg G Loss: 5.6211, Avg D Loss: 0.0393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=8.033, D_loss=0.027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 32 complete\n",
            "  Avg G Loss: 5.6402, Avg D Loss: 0.0363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=7.720, D_loss=0.004]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 33 complete\n",
            "  Avg G Loss: 5.6443, Avg D Loss: 0.0238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=2.747, D_loss=0.143]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 34 complete\n",
            "  Avg G Loss: 5.6419, Avg D Loss: 0.0231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=4.742, D_loss=0.003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 35 complete\n",
            "  Avg G Loss: 5.6765, Avg D Loss: 0.0233\n",
            "  ✓ Saved checkpoint to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=6.640, D_loss=0.001]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 36 complete\n",
            "  Avg G Loss: 5.6775, Avg D Loss: 0.0222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=5.126, D_loss=0.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 37 complete\n",
            "  Avg G Loss: 5.6520, Avg D Loss: 0.0206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=8.141, D_loss=0.009]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 38 complete\n",
            "  Avg G Loss: 5.6752, Avg D Loss: 0.0188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=6.552, D_loss=0.006]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 39 complete\n",
            "  Avg G Loss: 5.6421, Avg D Loss: 0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=6.919, D_loss=0.066]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 40 complete\n",
            "  Avg G Loss: 5.6288, Avg D Loss: 0.0295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0684\n",
            "  ✓ Saved checkpoint to Drive\n",
            "  ✓ Saved sample audio to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.12it/s, G_loss=7.316, D_loss=0.022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 41 complete\n",
            "  Avg G Loss: 5.6646, Avg D Loss: 0.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=5.362, D_loss=0.001]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 42 complete\n",
            "  Avg G Loss: 5.6936, Avg D Loss: 0.0141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=6.630, D_loss=0.006]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 43 complete\n",
            "  Avg G Loss: 5.6573, Avg D Loss: 0.0132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=6.683, D_loss=0.017]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 44 complete\n",
            "  Avg G Loss: 5.6638, Avg D Loss: 0.0203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=4.240, D_loss=0.002]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 45 complete\n",
            "  Avg G Loss: 5.6578, Avg D Loss: 0.0158\n",
            "  ✓ Saved checkpoint to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 1428/1428 [07:37<00:00,  3.12it/s, G_loss=5.284, D_loss=0.001]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 46 complete\n",
            "  Avg G Loss: 5.6778, Avg D Loss: 0.0174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=2.965, D_loss=0.032]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 47 complete\n",
            "  Avg G Loss: 5.6775, Avg D Loss: 0.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=7.646, D_loss=0.021]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 48 complete\n",
            "  Avg G Loss: 5.6587, Avg D Loss: 0.0151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.12it/s, G_loss=4.180, D_loss=0.003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 49 complete\n",
            "  Avg G Loss: 5.6572, Avg D Loss: 0.0159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 1428/1428 [07:36<00:00,  3.13it/s, G_loss=7.558, D_loss=0.001]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Epoch 50 complete\n",
            "  Avg G Loss: 5.6738, Avg D Loss: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Validation L1 Loss: 0.0649\n",
            "  ✓ Saved checkpoint to Drive\n",
            "  ✓ Saved sample audio to Drive\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th1lO5q6eiLa",
        "outputId": "d4343038-9f2b-4586-ba59-8c771db6e087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Paths\n",
        "LOCAL_DIR = \"/content/hifigan_local\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model_v2\"  # New folder for v2\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{LOCAL_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{LOCAL_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_list):\n",
        "        self.data_dir = data_dir\n",
        "        self.files = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "\n",
        "        # Load audio\n",
        "        wav_path = f\"{self.data_dir}/wavs/{name}.wav\"\n",
        "        sr, audio = wav.read(wav_path)\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Load units\n",
        "        unit_path = f\"{self.data_dir}/units/{name}.txt\"\n",
        "        with open(unit_path, 'r') as f:\n",
        "            units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "        return torch.FloatTensor(audio), torch.LongTensor(units)\n",
        "\n",
        "# ============ GENERATOR (same architecture) ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "# ============ DISCRIMINATOR (simplified) ============\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(1, 16, 15, stride=1, padding=7),\n",
        "            nn.Conv1d(16, 64, 41, stride=4, padding=20, groups=4),\n",
        "            nn.Conv1d(64, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(512, 512, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(512, 512, 5, stride=1, padding=2),\n",
        "        ])\n",
        "        self.conv_post = nn.Conv1d(512, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        fmaps = []\n",
        "        for conv in self.convs:\n",
        "            x = conv(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            fmaps.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        return x, fmaps\n",
        "\n",
        "# ============ TRAINING SETUP ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# KEY FIX 1: Lower discriminator learning rate\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.00005, betas=(0.8, 0.99))  # 4x lower\n",
        "\n",
        "# Learning rate schedulers\n",
        "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=0.999)\n",
        "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=0.999)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = UnitAudioDataset(LOCAL_DIR, train_files)\n",
        "val_dataset = UnitAudioDataset(LOCAL_DIR, val_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(train_loader)} batches\")\n",
        "\n",
        "# ============ LOSS FUNCTIONS ============\n",
        "def feature_loss(real_fmaps, fake_fmaps):\n",
        "    loss = 0\n",
        "    for real_fm, fake_fm in zip(real_fmaps, fake_fmaps):\n",
        "        loss += F.l1_loss(fake_fm, real_fm.detach())\n",
        "    return loss\n",
        "\n",
        "def discriminator_loss(real_out, fake_out):\n",
        "    real_loss = torch.mean((1 - real_out) ** 2)\n",
        "    fake_loss = torch.mean(fake_out ** 2)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_adv_loss(fake_out):\n",
        "    return torch.mean((1 - fake_out) ** 2)\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 100  # More epochs\n",
        "SAVE_EVERY = 10\n",
        "SAMPLE_EVERY = 10\n",
        "\n",
        "# KEY FIX 2: L1 loss weight (high at start, decreases over time)\n",
        "L1_WEIGHT_START = 45.0\n",
        "L1_WEIGHT_END = 10.0\n",
        "FM_WEIGHT = 2.0\n",
        "\n",
        "# KEY FIX 3: Discriminator warmup - only L1 loss for first N epochs\n",
        "D_WARMUP_EPOCHS = 10\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING (v2 - Fixed Hyperparameters)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Discriminator warmup: {D_WARMUP_EPOCHS} epochs\")\n",
        "print(f\"  L1 weight: {L1_WEIGHT_START} -> {L1_WEIGHT_END}\")\n",
        "print(f\"  D learning rate: 0.00005 (4x lower than G)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # Compute L1 weight for this epoch (linear decay)\n",
        "    l1_weight = L1_WEIGHT_START - (L1_WEIGHT_START - L1_WEIGHT_END) * (epoch - 1) / (NUM_EPOCHS - 1)\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "    epoch_l1_loss = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for batch_idx, (real_audio, units) in enumerate(pbar):\n",
        "        real_audio = real_audio.to(device)\n",
        "        units = units.to(device)\n",
        "\n",
        "        # Generate fake audio\n",
        "        fake_audio = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(real_audio.shape[1], fake_audio.shape[1])\n",
        "        real_audio = real_audio[:, :min_len]\n",
        "        fake_audio = fake_audio[:, :min_len]\n",
        "\n",
        "        # L1 loss (always computed)\n",
        "        l1_loss = F.l1_loss(fake_audio, real_audio)\n",
        "\n",
        "        # ============ DISCRIMINATOR UPDATE ============\n",
        "        if epoch > D_WARMUP_EPOCHS:\n",
        "            optimizer_d.zero_grad()\n",
        "\n",
        "            real_out, real_fmaps = discriminator(real_audio)\n",
        "            fake_out, _ = discriminator(fake_audio.detach())\n",
        "\n",
        "            d_loss = discriminator_loss(real_out, fake_out)\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            epoch_d_loss += d_loss.item()\n",
        "        else:\n",
        "            d_loss = torch.tensor(0.0)\n",
        "\n",
        "        # ============ GENERATOR UPDATE ============\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        if epoch > D_WARMUP_EPOCHS:\n",
        "            fake_out, fake_fmaps = discriminator(fake_audio)\n",
        "            _, real_fmaps = discriminator(real_audio)\n",
        "\n",
        "            g_adv_loss = generator_adv_loss(fake_out)\n",
        "            fm_loss = feature_loss(real_fmaps, fake_fmaps)\n",
        "\n",
        "            g_loss = g_adv_loss + FM_WEIGHT * fm_loss + l1_weight * l1_loss\n",
        "        else:\n",
        "            # Warmup: only L1 loss\n",
        "            g_loss = l1_weight * l1_loss\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_l1_loss += l1_loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'G': f'{g_loss.item():.3f}',\n",
        "            'D': f'{d_loss.item():.3f}',\n",
        "            'L1': f'{l1_loss.item():.4f}'\n",
        "        })\n",
        "\n",
        "    # Update learning rates\n",
        "    scheduler_g.step()\n",
        "    scheduler_d.step()\n",
        "\n",
        "    # Epoch stats\n",
        "    avg_g = epoch_g_loss / len(train_loader)\n",
        "    avg_d = epoch_d_loss / len(train_loader) if epoch > D_WARMUP_EPOCHS else 0\n",
        "    avg_l1 = epoch_l1_loss / len(train_loader)\n",
        "\n",
        "    print(f\"\\n  Epoch {epoch}: G={avg_g:.4f}, D={avg_d:.4f}, L1={avg_l1:.4f}, L1_weight={l1_weight:.1f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_l1 = 0\n",
        "        with torch.no_grad():\n",
        "            for real_audio, units in val_loader:\n",
        "                real_audio = real_audio.to(device)\n",
        "                units = units.to(device)\n",
        "                fake_audio = generator(units)\n",
        "                min_len = min(real_audio.shape[1], fake_audio.shape[1])\n",
        "                val_l1 += F.l1_loss(fake_audio[:, :min_len], real_audio[:, :min_len]).item()\n",
        "        val_l1 /= len(val_loader)\n",
        "        print(f\"  Validation L1: {val_l1:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % SAVE_EVERY == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Save sample audio\n",
        "    if epoch % SAMPLE_EVERY == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_audio, sample_units = val_dataset[0]\n",
        "            sample_units = sample_units.unsqueeze(0).to(device)\n",
        "            generated = generator(sample_units).squeeze().cpu().numpy()\n",
        "            generated = (generated * 32767).astype(np.int16)\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, generated)\n",
        "        print(f\"  ✓ Saved sample audio\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE (v2)\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "xaTFQgCaXNFz",
        "outputId": "fc9dbed9-9a0b-4360-cd4b-1726471daa6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hifigan_local/train_files.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1633618392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load file lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{LOCAL_DIR}/train_files.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hifigan_local/train_files.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if drive is mounted\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"Drive is mounted\")\n",
        "\n",
        "    # Check satere_project folder\n",
        "    satere_path = '/content/drive/MyDrive/satere_project'\n",
        "    if os.path.exists(satere_path):\n",
        "        print(f\"\\nContents of {satere_path}:\")\n",
        "        for item in os.listdir(satere_path):\n",
        "            print(f\"  {item}\")\n",
        "    else:\n",
        "        print(f\"\\n{satere_path} does not exist\")\n",
        "        print(\"\\nContents of MyDrive:\")\n",
        "        for item in os.listdir('/content/drive/MyDrive')[:20]:\n",
        "            print(f\"  {item}\")\n",
        "else:\n",
        "    print(\"Drive is NOT mounted. Run: drive.mount('/content/drive')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvHSqnQoBoPF",
        "outputId": "5139cafa-bce3-472d-c710-19f77479c5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive is mounted\n",
            "\n",
            "Contents of /content/drive/MyDrive/satere_project:\n",
            "  hifigan_model_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Search for hifigan_training\n",
        "for root, dirs, files in os.walk('/content/drive/MyDrive'):\n",
        "    if 'hifigan_training' in dirs:\n",
        "        full_path = os.path.join(root, 'hifigan_training')\n",
        "        print(f\"Found: {full_path}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "-K-wboI6DS2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List everything in satere_project\n",
        "path = '/content/drive/MyDrive/satere_project'\n",
        "print(\"Contents of satere_project:\")\n",
        "for item in os.listdir(path):\n",
        "    print(f\"  {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B36dcfqD6de",
        "outputId": "1edd0d88-a281-4216-c4c5-2597e842e634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of satere_project:\n",
            "  hifigan_model_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "BcD8TG8QEn1E",
        "outputId": "d17344be-f4c8-4e64-8357-54ba0efb6316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3361420860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_and_unmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "gmWG7M6VEzyD",
        "outputId": "d7443fd1-9641-4b8c-b1e9-6f344ebfe633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3329394316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "fFBZhHe6FFhY",
        "outputId": "68383803-b3f4-4434-d790-b8f5b6368345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove the existing mount point\n",
        "if os.path.exists('/content/drive'):\n",
        "    shutil.rmtree('/content/drive')\n",
        "\n",
        "# Now mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y857v-_Ftfc",
        "outputId": "c184c600-8c4c-47a4-b180-e9433f65b423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/satere_project'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTY8CfnlGBar",
        "outputId": "e7878b85-7355-4f6d-c7e3-c03df63ad2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['raw_audio', 'converted_audio', 'satere_units', 'satere_motifs', 'acts12.mp3', 'phase2_output', 'soundscript_full', 'soundscript_deploy', 'soundscript_deploy.zip', 'vocoder_output', 'hifigan_training', 'hifigan_model']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "LOCAL_DIR = \"/content/hifigan_local\"\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/satere_project/hifigan_training\"\n",
        "\n",
        "if os.path.exists(LOCAL_DIR):\n",
        "    shutil.rmtree(LOCAL_DIR)\n",
        "\n",
        "print(\"Copying training data to local storage...\")\n",
        "shutil.copytree(DRIVE_DIR, LOCAL_DIR)\n",
        "print(f\"✓ Copied to {LOCAL_DIR}\")\n",
        "\n",
        "print(f\"\\nLocal files: {len(os.listdir(f'{LOCAL_DIR}/units'))} unit files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "52aBXH5GGN-c",
        "outputId": "cc88aeb8-1178-4e66-b16c-64e5f1b53a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying training data to local storage...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-271758559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Copying training data to local storage...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDRIVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOCAL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ Copied to {LOCAL_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    534\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[1;32m    537\u001b[0m                          ignore_dangling_symlinks, dirs_exist_ok)\n\u001b[1;32m    538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;31m# Will raise a SpecialFileError for unsupported file types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                 \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;31m# catch the Error from the recursive copytree so that we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# continue with other files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "LOCAL_DIR = \"/content/hifigan_local\"\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/satere_project/hifigan_training\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(f\"{LOCAL_DIR}/wavs\", exist_ok=True)\n",
        "os.makedirs(f\"{LOCAL_DIR}/units\", exist_ok=True)\n",
        "\n",
        "# Copy text files first\n",
        "shutil.copy(f\"{DRIVE_DIR}/train_files.txt\", LOCAL_DIR)\n",
        "shutil.copy(f\"{DRIVE_DIR}/val_files.txt\", LOCAL_DIR)\n",
        "print(\"✓ Copied file lists\")\n",
        "\n",
        "# Get file lists\n",
        "wav_files = os.listdir(f\"{DRIVE_DIR}/wavs\")\n",
        "unit_files = os.listdir(f\"{DRIVE_DIR}/units\")\n",
        "print(f\"Files to copy: {len(wav_files)} wavs, {len(unit_files)} units\")\n",
        "\n",
        "# Copy function\n",
        "def copy_file(args):\n",
        "    src, dst = args\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# Copy wavs in parallel\n",
        "print(\"Copying wavs...\")\n",
        "wav_args = [(f\"{DRIVE_DIR}/wavs/{f}\", f\"{LOCAL_DIR}/wavs/{f}\") for f in wav_files]\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    list(executor.map(copy_file, wav_args))\n",
        "print(f\"✓ Copied {len(wav_files)} wav files\")\n",
        "\n",
        "# Copy units in parallel\n",
        "print(\"Copying units...\")\n",
        "unit_args = [(f\"{DRIVE_DIR}/units/{f}\", f\"{LOCAL_DIR}/units/{f}\") for f in unit_files]\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    list(executor.map(copy_file, unit_args))\n",
        "print(f\"✓ Copied {len(unit_files)} unit files\")\n",
        "\n",
        "print(\"\\n✓ Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch8G8PO3GYv4",
        "outputId": "fe01e6c3-4c9c-41ae-a91c-6367e6488aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Copied file lists\n",
            "Files to copy: 22934 wavs, 22934 units\n",
            "Copying wavs...\n",
            "✓ Copied 22934 wav files\n",
            "Copying units...\n",
            "✓ Copied 22934 unit files\n",
            "\n",
            "✓ Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io.wavfile as wav\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Paths\n",
        "LOCAL_DIR = \"/content/hifigan_local\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/satere_project\"\n",
        "MODEL_DIR = f\"{PROJECT_ROOT}/hifigan_model_v2\"  # New folder for v2\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load file lists\n",
        "with open(f\"{LOCAL_DIR}/train_files.txt\", \"r\") as f:\n",
        "    train_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open(f\"{LOCAL_DIR}/val_files.txt\", \"r\") as f:\n",
        "    val_files = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Validation files: {len(val_files)}\")\n",
        "\n",
        "# ============ DATASET ============\n",
        "class UnitAudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, file_list):\n",
        "        self.data_dir = data_dir\n",
        "        self.files = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "\n",
        "        # Load audio\n",
        "        wav_path = f\"{self.data_dir}/wavs/{name}.wav\"\n",
        "        sr, audio = wav.read(wav_path)\n",
        "        audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Load units\n",
        "        unit_path = f\"{self.data_dir}/units/{name}.txt\"\n",
        "        with open(unit_path, 'r') as f:\n",
        "            units = [int(u) for u in f.read().strip().split()]\n",
        "\n",
        "        return torch.FloatTensor(audio), torch.LongTensor(units)\n",
        "\n",
        "# ============ GENERATOR (same architecture) ============\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_units=100, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.unit_embed = nn.Embedding(num_units, embed_dim)\n",
        "        self.pre_conv = nn.Conv1d(embed_dim, 512, 7, padding=3)\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(512, 256, 10, stride=5, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(256, 256, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(256, 128, 10, stride=5, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(128, 128, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(128, 64, 8, stride=4, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(64, 32, 8, stride=4, padding=2),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(32, 32, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose1d(32, 64, 4, stride=2, padding=1),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv1d(64, 64, 7, padding=3),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.post_conv = nn.Conv1d(64, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unit_embed(x).transpose(1, 2)\n",
        "        x = self.pre_conv(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        for upsample in self.upsamples:\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.post_conv(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "# ============ DISCRIMINATOR (simplified) ============\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(1, 16, 15, stride=1, padding=7),\n",
        "            nn.Conv1d(16, 64, 41, stride=4, padding=20, groups=4),\n",
        "            nn.Conv1d(64, 256, 41, stride=4, padding=20, groups=16),\n",
        "            nn.Conv1d(256, 512, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(512, 512, 41, stride=4, padding=20, groups=64),\n",
        "            nn.Conv1d(512, 512, 5, stride=1, padding=2),\n",
        "        ])\n",
        "        self.conv_post = nn.Conv1d(512, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        fmaps = []\n",
        "        for conv in self.convs:\n",
        "            x = conv(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            fmaps.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        return x, fmaps\n",
        "\n",
        "# ============ TRAINING SETUP ============\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# KEY FIX 1: Lower discriminator learning rate\n",
        "optimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
        "optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.00005, betas=(0.8, 0.99))  # 4x lower\n",
        "\n",
        "# Learning rate schedulers\n",
        "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=0.999)\n",
        "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=0.999)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = UnitAudioDataset(LOCAL_DIR, train_files)\n",
        "val_dataset = UnitAudioDataset(LOCAL_DIR, val_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ DataLoader ready: {len(train_loader)} batches\")\n",
        "\n",
        "# ============ LOSS FUNCTIONS ============\n",
        "def feature_loss(real_fmaps, fake_fmaps):\n",
        "    loss = 0\n",
        "    for real_fm, fake_fm in zip(real_fmaps, fake_fmaps):\n",
        "        loss += F.l1_loss(fake_fm, real_fm.detach())\n",
        "    return loss\n",
        "\n",
        "def discriminator_loss(real_out, fake_out):\n",
        "    real_loss = torch.mean((1 - real_out) ** 2)\n",
        "    fake_loss = torch.mean(fake_out ** 2)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_adv_loss(fake_out):\n",
        "    return torch.mean((1 - fake_out) ** 2)\n",
        "\n",
        "# ============ TRAINING LOOP ============\n",
        "NUM_EPOCHS = 100  # More epochs\n",
        "SAVE_EVERY = 10\n",
        "SAMPLE_EVERY = 10\n",
        "\n",
        "# KEY FIX 2: L1 loss weight (high at start, decreases over time)\n",
        "L1_WEIGHT_START = 45.0\n",
        "L1_WEIGHT_END = 10.0\n",
        "FM_WEIGHT = 2.0\n",
        "\n",
        "# KEY FIX 3: Discriminator warmup - only L1 loss for first N epochs\n",
        "D_WARMUP_EPOCHS = 10\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING (v2 - Fixed Hyperparameters)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Discriminator warmup: {D_WARMUP_EPOCHS} epochs\")\n",
        "print(f\"  L1 weight: {L1_WEIGHT_START} -> {L1_WEIGHT_END}\")\n",
        "print(f\"  D learning rate: 0.00005 (4x lower than G)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # Compute L1 weight for this epoch (linear decay)\n",
        "    l1_weight = L1_WEIGHT_START - (L1_WEIGHT_START - L1_WEIGHT_END) * (epoch - 1) / (NUM_EPOCHS - 1)\n",
        "\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "    epoch_l1_loss = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for batch_idx, (real_audio, units) in enumerate(pbar):\n",
        "        real_audio = real_audio.to(device)\n",
        "        units = units.to(device)\n",
        "\n",
        "        # Generate fake audio\n",
        "        fake_audio = generator(units)\n",
        "\n",
        "        # Match lengths\n",
        "        min_len = min(real_audio.shape[1], fake_audio.shape[1])\n",
        "        real_audio = real_audio[:, :min_len]\n",
        "        fake_audio = fake_audio[:, :min_len]\n",
        "\n",
        "        # L1 loss (always computed)\n",
        "        l1_loss = F.l1_loss(fake_audio, real_audio)\n",
        "\n",
        "        # ============ DISCRIMINATOR UPDATE ============\n",
        "        if epoch > D_WARMUP_EPOCHS:\n",
        "            optimizer_d.zero_grad()\n",
        "\n",
        "            real_out, real_fmaps = discriminator(real_audio)\n",
        "            fake_out, _ = discriminator(fake_audio.detach())\n",
        "\n",
        "            d_loss = discriminator_loss(real_out, fake_out)\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            epoch_d_loss += d_loss.item()\n",
        "        else:\n",
        "            d_loss = torch.tensor(0.0)\n",
        "\n",
        "        # ============ GENERATOR UPDATE ============\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        if epoch > D_WARMUP_EPOCHS:\n",
        "            fake_out, fake_fmaps = discriminator(fake_audio)\n",
        "            _, real_fmaps = discriminator(real_audio)\n",
        "\n",
        "            g_adv_loss = generator_adv_loss(fake_out)\n",
        "            fm_loss = feature_loss(real_fmaps, fake_fmaps)\n",
        "\n",
        "            g_loss = g_adv_loss + FM_WEIGHT * fm_loss + l1_weight * l1_loss\n",
        "        else:\n",
        "            # Warmup: only L1 loss\n",
        "            g_loss = l1_weight * l1_loss\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_l1_loss += l1_loss.item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'G': f'{g_loss.item():.3f}',\n",
        "            'D': f'{d_loss.item():.3f}',\n",
        "            'L1': f'{l1_loss.item():.4f}'\n",
        "        })\n",
        "\n",
        "    # Update learning rates\n",
        "    scheduler_g.step()\n",
        "    scheduler_d.step()\n",
        "\n",
        "    # Epoch stats\n",
        "    avg_g = epoch_g_loss / len(train_loader)\n",
        "    avg_d = epoch_d_loss / len(train_loader) if epoch > D_WARMUP_EPOCHS else 0\n",
        "    avg_l1 = epoch_l1_loss / len(train_loader)\n",
        "\n",
        "    print(f\"\\n  Epoch {epoch}: G={avg_g:.4f}, D={avg_d:.4f}, L1={avg_l1:.4f}, L1_weight={l1_weight:.1f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 2 == 0:\n",
        "        generator.eval()\n",
        "        val_l1 = 0\n",
        "        with torch.no_grad():\n",
        "            for real_audio, units in val_loader:\n",
        "                real_audio = real_audio.to(device)\n",
        "                units = units.to(device)\n",
        "                fake_audio = generator(units)\n",
        "                min_len = min(real_audio.shape[1], fake_audio.shape[1])\n",
        "                val_l1 += F.l1_loss(fake_audio[:, :min_len], real_audio[:, :min_len]).item()\n",
        "        val_l1 /= len(val_loader)\n",
        "        print(f\"  Validation L1: {val_l1:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % SAVE_EVERY == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
        "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, f\"{MODEL_DIR}/checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  ✓ Saved checkpoint\")\n",
        "\n",
        "    # Save sample audio\n",
        "    if epoch % SAMPLE_EVERY == 0:\n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_audio, sample_units = val_dataset[0]\n",
        "            sample_units = sample_units.unsqueeze(0).to(device)\n",
        "            generated = generator(sample_units).squeeze().cpu().numpy()\n",
        "            generated = (generated * 32767).astype(np.int16)\n",
        "            wav.write(f\"{MODEL_DIR}/sample_epoch{epoch}.wav\", 16000, generated)\n",
        "        print(f\"  ✓ Saved sample audio\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE (v2)\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "rCP0UJnWbDn0",
        "outputId": "c4d31f81-c903-4238-9499-266139b1f69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hifigan_local/train_files.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1633618392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load file lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{LOCAL_DIR}/train_files.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hifigan_local/train_files.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check what's in the local folder\n",
        "print(os.listdir('/content/hifigan_local'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "R0gcV7K64Z2l",
        "outputId": "a033ff9c-ed35-41bc-8028-a266fd348fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hifigan_local'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2691141591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check what's in the local folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/hifigan_local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hifigan_local'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUn_6UMJ4sVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}