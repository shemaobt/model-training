\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Acoustography: A Machine-Generated Writing System for Zero-Resource Speech Synthesis}

\author{\IEEEauthorblockN{Shema Bible Translation}
\IEEEauthorblockA{\textit{Ready Vessels Project} \\
Shema Bible Translation \& University of the Nations\\
Kansas City, USA}}

\maketitle

\begin{abstract}
This paper introduces \textit{Acoustography}, a paradigm for zero-resource speech processing that conceptually decouples the encoding of speech from its semantic understanding. Traditional Text-to-Speech (TTS) pipelines rely on orthographic supervision, creating a "digital divide" for the $\sim$3,500 unwritten languages worldwide. Acoustography addresses this by identifying \textit{Acoustemes}—discrete, statistically stable units derived from raw audio via self-supervised learning (SSL)—effectively generating a computational writing system. We formalize Acoustography as a low-bitrate discrete bottleneck ($\sim$350 bps) that preserves phonetic tangibility while discarding speaker variance. To resolve the inherent loss of prosody in this discretization, we present a pitch-conditioned neural vocoder architecture acting as a high-fidelity "reader." By conditioning a Multi-Receptive Field Fusion (MRF) generator on both discrete Acousteme sequences and quantified fundamental frequency ($F_0$) trajectories, we achieve state-of-the-art reconstruction quality. We provide a rigorous analysis of the architectural decisions—from layer selection in XLSR-53 to the receptive field dynamics of HiFi-GAN—demonstrating how explicit signal disentanglement enables intelligible synthesis for low-resource languages.
\end{abstract}

\begin{IEEEkeywords}
Acoustography, Acoustemes, zero-resource speech processing, textless NLP, self-supervised learning, neural vocoding, discrete representation learning
\end{IEEEkeywords}

\section{Introduction}

The architecture of modern language technology presupposes the existence of text. This dependence creates a fundamental barrier for oral languages, excluding nearly half of the world's linguistic diversity from the benefits of modern NLP. In domains like Oral Bible Translation (OBT), the requirement for orthography forces a decades-long "text-first" workflow that delays access to information.

We propose a "speech-first" paradigm called \textit{Acoustography}. Just as human writing systems externalize human perception (phonemes/syllables), Acoustography externalizes \textit{machine perception}. By leveraging Self-Supervised Learning (SSL), we extract discrete units—\textit{Acoustemes}—that represent the atomic statistical structure of a language's acoustic space.

The primary technical challenge in discrete unit synthesis is the information loss inherent in quantization. Converting continuous speech to discrete tokens acts as a regularization bottleneck, stripping away paralinguistic features like prosody and speaker identity. While beneficial for privacy and abstraction, this results in "robotic" synthesis.
Our solution is a dual-stream generative architecture. We treat speech generation as the recomposition of two orthogonal latent spaces: a discrete phonetic stream (Acoustemes) and a continuous prosodic stream ($F_0$).

\section{Technical Background}

\subsection{Self-Supervised Representation Learning}
We build upon \texttt{wav2vec 2.0} \cite{baevski2020wav2vec} and similar masked-prediction models like HuBERT \cite{hsu2021hubert}, which learn contextualized representations from raw audio via contrastive or clustering objectives. Given raw audio $\mathcal{X}$, the model encodes local features $\mathcal{Z} = f_{enc}(\mathcal{X})$ and contextualizes them $\mathcal{C} = g_{context}(\mathcal{Z})$.
Specifically, we use \texttt{XLSR-53} \cite{conneau2020unsupervised}, pre-trained on 53 languages. The cross-lingual pre-training forces the model to learn a generalized phonetic manifold, robust for zero-shot application.

\subsection{The Discretization Dilemma}
To perform NLP tasks, we need discrete symbols. While continuous "soft" units offer higher fidelity \cite{van2022softvc}, they are difficult to model with standard NLP tools. The naive approach—Vector Quantization (VQ)—often discards paralinguistic features. Polyak et al. \cite{polyak2021speech} highlight that this discretization acts as an information bottleneck, stripping prosody and speaker identity.
The trade-off is bitrate. Raw 16kHz audio is $\sim$256 kbps. Acoustography targets the "text equivalent" bitrate ($\sim$300-400 bps).

\section{Methodology: The Acoustography Pipeline}

\subsection{The Writer: Acousteme Discovery}
This module performs the $x \rightarrow U$ transduction.

\subsubsection{Layer Selection via Canonical Correlation}
A critical decision is *where* to extract features. Deep Transformers exhibit a layer-wise hierarchy: early layers encode acoustics, middle layers encode phonetics, and deep layers encode semantics \cite{pasad2021layer}. We select \textbf{Layer 15} of XLSR-53 to maximize phonetic information.

\subsubsection{Manifold Clustering (Why K-Means?)}
We employ K-Means clustering over VQ-VAE. K-Means on frozen features is deterministic and decouples the acoustic model from the tokenizer. We set $K=100$, a "super-phonemic" granularity sufficient for reconstruction.

\subsection{Prosodic Disentanglement}
Since $U$ is pitch-invariant, we must model $F_0$ explicitly, a technique inspired by FastSpeech \cite{ren2019fastspeech} but applied here to unsupervised units. We use the **Probabilistic YIN (pYIN)** algorithm \cite{mauch2014pyin} for robustness against noise, quantizing $F_0$ into 32 bins.

\subsection{The Reader: Pitch-Conditioned HiFi-GAN}
The generator $G(U, P)$ learns to invert the projection $U, P \rightarrow x$.

\subsubsection{Architectural Motivation}
We adopt the \textbf{HiFi-GAN} \cite{kong2020hifi} architecture. Standard GAN vocoders like MelGAN \cite{kumar2019melgan} struggle with high-frequency phase coherence. HiFi-GAN solves this via **Multi-Receptive Field Fusion (MRF)**.
Speech signals are non-stationary: they contain short transients (plosives, $\sim$5-10ms) and long periodic structures (vowels, $\sim$100-200ms). A single convolutional kernel cannot capture both optimally.
MRF sums the output of parallel ResBlocks with kernels $k_r \in \{3, 7, 11\}$ and dilations $\mathcal{D}_r$.
\begin{itemize}
    \item $k=3$: Captures fine-grained phase details and transients.
    \item $k=11$: Captures fundamental periodic patterns and formants.
\end{itemize}
By summing these, the generator can synthesize a composite signal with sharp phase alignment and rich timbral texture.

\subsubsection{Conditioning Mechanism}
We do not use hyper-networks or complex modulation (like FiLM) to keep inference fast. Instead, we use simple concatenation of embeddings at the input bottleneck.
\begin{equation}
    H_{in} = \text{Concat}(\text{Emb}_{256}(U), \text{Emb}_{64}(P))
\end{equation}
This $320$-dim vector is upsampled. The explicit pitch embedding acts as a "guide signal" for the periodic components of the MRF, effectively prescribing the fundamental period length for the ResBlocks to fill with texture.

\section{Discriminator Design and Loss Landscape}
Generative modeling of audio requires a delicate balance between time-domain structure and frequency-domain spectral accuracy.

\subsection{Multi-Period Discriminator (MPD)}
The MPD is crucial for reducing "metallic" artifacts. It operates by reshaping the 1D signal $x \in \mathbb{R}^T$ into 2D patches $x_p \in \mathbb{R}^{T/p \times p}$ for periods $p \in \{2, 3, 5, 7, 11\}$.
\textit{Why Prime Periods?} If we used periods $\{2, 4\}$, a periodic error at $f/4$ would alias into both. Prime periods ensure that artifacts at arbitrary frequencies are "caught" by at least distinct discriminator view, minimizing harmonic aliasing.

\subsection{Composite Loss Function}
The training objective $\mathcal{L}_{total}$ is a weighted sum:
\begin{equation}
    \mathcal{L}_{total} = \lambda_{mel}\mathcal{L}_{mel} + \lambda_{fm}\mathcal{L}_{fm} + \lambda_{adv}\mathcal{L}_{adv} + \lambda_{stft}\mathcal{L}_{stft}
\end{equation}

\subsubsection{Mel-Reconstruction ($\lambda=45$)}
This is the dominant term. It forces the generator to match the spectral envelope of the target.
\begin{equation}
    \mathcal{L}_{mel} = \mathbb{E}[ \| \phi(x) - \phi(\hat{x}) \|_1 ]
\end{equation}
Using $L_1$ specifically creates sparser, sharper spectral features compared to $L_2$ (MSE), which tends to "smear" formants.

\subsubsection{Multi-Resolution STFT ($\lambda=2$)}
To ensure phase consistency across time scales, we compute spectral convergence over window sizes $N \in \{512, 1024, 2048\}$. This prevents the "checkerboard artifacts" common in deconvolutional networks by penalizing spectral error at multiple resolutions simultaneously.

\section{Experimental Setup}

\subsection{Data and Pre-processing}
\begin{itemize}
    \item \textbf{Corpus}: $\sim$45 hours of Sateré-Mawé (Tupian family).
    \item \textbf{Normalization}: Loudness normalized to -23 LUFS.
    \item \textbf{Segmentation}: Silence-based segmentation using a -40dB threshold, yielding segments of 2-10 seconds.
\end{itemize}

\subsection{Training Dynamics}
We train for 1M steps on a single NVIDIA A10G.
\begin{itemize}
    \item \textbf{Optimization}: AdamW ($\beta_1=0.8, \beta_2=0.99$). The low $\beta_1$ is critical for GANs to prevent mode collapse by reducing momentum on the discriminator updates.
    \item \textbf{Scheduler}: Exponential decay ($\gamma=0.999$ per epoch).
    \item \textbf{Gradient Clipping}: Applied to the Generator (norm 5.0) to prevent gradients from exploding during the initial "phase alignment" stages of training.
\end{itemize}

\section{Results and Ablation}
Qualitative listening tests confirm that the Pitch-Conditioned model significantly outperforms the baseline (Acoustemes only).
\begin{itemize}
    \item \textbf{Baseline (No Pitch)}: Result is intelligible but monotonic. The model defaults to the "average" pitch of the speaker ($\sim$120Hz), resulting in robotic prosody.
    \item \textbf{Proposed (With Pitch)}: The model successfully tracks the $F_0$ contour. Intonation questions (rising pitch) and emphasis (pitch excursions) are preserved.
\end{itemize}
This confirms that the "robotic" quality of unit-to-speech synthesis is primarily a result of the information bottleneck in $U$, and that $F_0$ injection is a sufficient statistic to restore naturalness.

\section{Discussion: The Future of Textless NLP}

Acoustography demonstrates that the "text bottleneck" is an artificial constraint. By treating machine-discovered units as first-class linguistic objects, we can build efficient ($<400$ bps), high-fidelity speech systems.
The implications extend to **Generative Spoken Language Modeling (GSLM)**. Large Language Models (LLMs) can be trained on Acousteme sequences $\mathbf{U}$ directly. Since $\mathbf{U}$ correlates with phonetics, the LLM learns the "language of sound." Our Vocoder then serves as the rendering engine for these models, enabling "Speech-to-Speech" translation and generation without a single line of text data.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}

\bibitem{bird2020decolonising} S. Bird, ``Decolonising speech and language technology,'' \textit{Proc. COLING}, 2020.

\bibitem{lakhotia2021generative} K. Lakhotia et al., ``On Generative Spoken Language Modeling from Raw Audio,'' \textit{TACL}, vol. 9, 2021.

\bibitem{baevski2020wav2vec} A. Baevski et al., ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' \textit{NeurIPS}, 2020.

\bibitem{conneau2020unsupervised} A. Conneau et al., ``Unsupervised cross-lingual representation learning for speech recognition,'' \textit{arXiv:2006.13979}, 2020.

\bibitem{hsu2021hubert} W.-N. Hsu et al., ``HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,'' \textit{IEEE/ACM TASLP}, 2021.

\bibitem{kong2020hifi} J. Kong, J. Kim, and J. Bae, ``HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,'' \textit{NeurIPS}, 2020.

\bibitem{kumar2019melgan} K. Kumar et al., ``MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis,'' \textit{NeurIPS}, 2019.

\bibitem{van2022softvc} B. van Niekerk et al., ``A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion,'' \textit{ICASSP}, 2022.

\bibitem{polyak2021speech} A. Polyak et al., ``Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,'' \textit{Interspeech}, 2021.

\bibitem{ren2019fastspeech} Y. Ren et al., ``FastSpeech: Fast, Robust and Controllable Text to Speech,'' \textit{NeurIPS}, 2019.

\bibitem{pasad2021layer} A. Pasad, J.-C. Chou, and K. Livescu, ``Layer-wise Analysis of a Self-supervised Speech Representation Model,'' \textit{ASRU}, 2021.

\bibitem{mauch2014pyin} M. Mauch and S. Dixon, ``pYIN: A fundamental frequency estimator using probabilistic threshold distributions,'' \textit{ICASSP}, 2014.

\end{thebibliography}

\end{document}
